[
  {
    "objectID": "index.html#learning-goals",
    "href": "index.html#learning-goals",
    "title": "EDS 217 - Python for Environmental Data Science",
    "section": "Learning Goals",
    "text": "Learning Goals\n(what you will be able to do) - Setup a python environment for data science using conda.\n\nConduct reproducible analyses within interactive jupyter notebook environments\nUse the VSCode IDE to write and execute python notebooks as well as scripts.\nRead and write basic-to-intermediate scripts and programs in the Python programming language\nPerform analyses on structured data using numpy\nLoad, explore, aggregate, analyze, and display data using pandas.\nLearn to visualize data using matplotlib and friends.\nApply all of these tools to analyze environmental datasets\nDevelop a short tutorial on how to use a python data science library for environmental analysis"
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "EDS 217 - Python for Environmental Data Science",
    "section": "Lectures",
    "text": "Lectures\nLectures will be used to introduce general concepts and principles of the python language. We won‚Äôt have too many of these, but they will be used to introduce new concepts and provide context for the interactive sessions."
  },
  {
    "objectID": "index.html#interactive-sessions",
    "href": "index.html#interactive-sessions",
    "title": "EDS 217 - Python for Environmental Data Science",
    "section": "Interactive Sessions",
    "text": "Interactive Sessions\nInteractive sessions will be used to demonstrate the use of python syntax, libraries, and other tools essential to the environmental data science workflow. These sessions will generally be conducted in jupyter notebooks, and will be available for you to download and use as a reference. We will also use these sessions to work through example code and computations related to the major course topics."
  },
  {
    "objectID": "index.html#practice-sessions",
    "href": "index.html#practice-sessions",
    "title": "EDS 217 - Python for Environmental Data Science",
    "section": "Practice Sessions",
    "text": "Practice Sessions\nPractice sessions will be used to provide opportunities to using the concepts, tools, and libraries presented during interactive sessions to solve more open-ended programming problems. While the goal is to develop independent confidence in python programming, we will work through these problems in paired/collaborative coding. Example solutions to these practice problems will always be available."
  },
  {
    "objectID": "index.html#trypy-sessions",
    "href": "index.html#trypy-sessions",
    "title": "EDS 217 - Python for Environmental Data Science",
    "section": "TryPy Sessions",
    "text": "TryPy Sessions\nTryPy sessions are designed to allow you to develop your own reproducible workflows based on core data science principles. We will often structure these sessions in a similar way to prior activities from EDS221. This will allow you to get a better understanding of the differences between python and R as well as the strengths and weaknesses of each."
  },
  {
    "objectID": "index.html#group-work",
    "href": "index.html#group-work",
    "title": "EDS 217 - Python for Environmental Data Science",
    "section": "Group Work",
    "text": "Group Work\nYour group project is a ‚ÄúData Science Show-and-Tell‚Äù that will extend the skills you develop during the first week of the course through the creation of collaborative presentations focused on datasets and libraries of your choosing. Working in teams, you will develop a short, reproducible tutorial on how to use a python data science library with examples using datasets relevant to environmental analysis. These tutorials will be presented to the class during the final week of the course and shared with the class via a github repository."
  },
  {
    "objectID": "index.html#course-outline",
    "href": "index.html#course-outline",
    "title": "EDS 217 - Python for Environmental Data Science",
    "section": "Course Outline",
    "text": "Course Outline\n\nTuesday, September 5\n\nDay[0] - Ready, Set, Python!\nThe materials for our first day are designed to introduce the basics of working with Python and getting your local machines setup for the course.\n\n\nMorning Session\nIntro to Python Data Science\nInteractive Session 0-1 - Ready, Set, Python!\n\n\nAfternoon Session\nGetting Help\nInteractive Session 0-2 - Hello, Python Data Science\n\n\n\nWednesday, September 6\n\nDay[1] - Do you speak Python?\nToday we will explore variables and learn the basic syntax of the python programming language.\n\n\nMorning Session\nInteractive Session 1-1 - Variables & Operators\nPractice Session 1-1 - Variables & Operators\nInteractive Session 1-2 - Lists and Indexing\n\n\nAfternoon Session\nPractice Session 1-2 - Lists and Indexing\nTryPy 00 - St.¬†Louis Lead Data\n\n\n\nThursday, September 7\n\nDay[2] - Going with the flow\nToday we complete our quick tour of the core python language as we learn the fundamentals of controlling the flow of programs in python. We will also prepare ourselves for the data science packages to come by learning how to use python to work with structured data.\n\n\nMorning Session\nInteractive Session 2-1 - Ifs or Elses\nPractice Session 2-1 - Ifs or Elses\nInteractive Session 2-2 - Structured Data\n\n\nAfternoon Session\nThe Zen of Python\nPractice Session 2-2 - Structured Data\n[TryPy2 w/ Real Data only]\n\n\n\nFriday, September 8\n\nDay[3] - Numpy üßÆ (Hooray for Arrays!)\n\n\nMorning Session\nSession 3-1 - NumPy\n\n\nAfternoon Session\nDebugging (Anna) [COMING SOON]\nPractice Session 3-1 - NumPy\n\n\n\nMonday, September 11\n\nDay[4] - Pandas üêº\nOur journey into Python‚Äôs Data Science toolkit begins with NumPy, a library designed to perform advanced calculations on matrices.\nWe end our first week with arguably the most important library in the Python data science ecosystem: pandas.\nNow that we‚Äôve learned how to import, manage, and analyze data using pandas, it‚Äôs time to make some graphs!\n\n\nMorning Session\nSession 4-1 - Pandas\n\n\nAfternoon Session\nCo-Pilot [COMING SOON]\nPractice Session 4-1 - Pandas\n\n\n\nTuesday, September 12\n\nDay[5] - Matplotlib üìà\nMatplotlib is the primary libary used for plotting data in Python (although there are some great alternatives), so we will start there.\n\n\nMorning Session\nSession 5-1 - Matplotlib\n\n\nAfternoon Session\nPractice Session 5-1 - Matplotlib [COMING SOON]\n\n\n\nWednesday September 13 - Friday, September 15\n\nDay[6:] - Group Project Work ‚úèÔ∏è\nOur final activity will be a group project in which you work with a team of 3-4 of your classmates to create a brief tutorial introducting one of the many other libraries available to conduct environmental data science in Python.\nGroup Project\nYou will develop your tutorial using the same Jupyter Notebook structures that we‚Äôve been using throughout the class and by incorporating examples using a dataset of your choosing.\nThe goal of this excercise is to collaboratively develop a set of data, notebooks, and visualizations that are entirely reproducible, shared on github, and used by others to learn how to use the library you‚Äôve chosen.\nOn our last day (Day[-1]), we‚Äôll spend the afternoon conducting a Python Data Science Show and Tell"
  },
  {
    "objectID": "lectures/lectures.html",
    "href": "lectures/lectures.html",
    "title": "EDS 217 Lectures",
    "section": "",
    "text": "Return to Course Home Page\n\n\n\n\nThis page contains links to lecture materials for EDS 217.\n\nLecture 0: Introduction to Python Data Science\n\n\nLecture 1: The Zen of Python\n\n\nLecture 2: Getting Help"
  },
  {
    "objectID": "lectures/99_dry_vs_wet.html#dry-vs.-wet",
    "href": "lectures/99_dry_vs_wet.html#dry-vs.-wet",
    "title": "EDS 217, Lecture 4: DRY üèú vs.¬†WET üåä",
    "section": "DRY vs.¬†WET",
    "text": "DRY vs.¬†WET\nIf DRY means ‚ÄúDon‚Äôt Repeat Yourself‚Äù‚Ä¶ then WET means ‚ÄúWrite Every Time‚Äù, or ‚ÄúWe Enjoy Typing‚Äù\nDon‚Äôt write WET code!\n\nHow to DRY out your code\nWe write DRY code - or we DRY out WET code - through a combination of abstraction and normalization."
  },
  {
    "objectID": "lectures/99_dry_vs_wet.html#abstraction",
    "href": "lectures/99_dry_vs_wet.html#abstraction",
    "title": "EDS 217, Lecture 4: DRY üèú vs.¬†WET üåä",
    "section": "Abstraction",
    "text": "Abstraction\nThe ‚Äúprinciple of abstraction‚Äù aims to reduce duplication of information (usually code) in a program whenever it is practical to do so:\n‚ÄúEach significant piece of functionality in a program should be implemented in just one place in the source code. Where similar functions are carried out by distinct pieces of code, it is generally beneficial to combine them into one by abstracting out the varying parts.‚Äù\nBenjamin C. Pierce - Types and Programming Languages\n\nAbstraction Example\nThe easiest way to understand abstraction is to see it in action. Here‚Äôs an example that you are already familiar with; determining the energy emitted by an object as a function of its temperature:\nQ = \\epsilon \\sigma T^4\nwhere \\epsilon is an object‚Äôs emmissivity, \\sigma is the Stefan-Boltzmann constant, and T is temperature in degrees Kelvin.\n\n\nAbstraction Example\nWe might write the following code to determine Q:\n\n\nCode\n# How much energy is emitted by an object at a certain temperature?\nŒµ = 1      # emissivity [-]\nœÉ = 5.67e-8  # stefan-boltzmann constant [W/T^4]\nT_C = 40         # temperature [deg-C]\n\nQ = Œµ * œÉ * (T_C+273.15)**4\nprint(Q)\n\n\n\n\nAbstraction Example\nBut this code is going to get very WET very fast.\n\n\nCode\n# How much energy is emitted by an object at a certain temperature?\nŒµ = 1      # emissivity [-]\nœÉ = 5.67e-8  # stefan-boltzmann constant [W/m^2/K^4]\nT_C = 40         # temperature [deg-C]\n\nQ = Œµ * œÉ * (T_C+273.15)**4\n\n# New T value? Different epsilon? What about a bunch of T values?\nT_2 = 30\n\nQ2 = Œµ * œÉ * (T_2+273.15)**4\n\n\n\n\n\nAbstraction Example\nHere‚Äôs a DRY version obtained using abstraction:\n\n\nCode\n# energy.py contains a function to calculate Q from T \nfrom energy import Q \n\nT = 40 # deg-C\nE = Q(T, unit='C')\n\n\n\n\nAbstraction Summary, Part 1\n\nWe keep our code DRY by using abstraction. In addition to functions, python also provides Classes as another important way to create abstractions.\nFunctions and Classes are the subject of this tomorrow‚Äôs exercise.\n\n\n\nAbstraction Summary, Part 2\n\nIn general, the process of keeping code DRY through successive layers of abstraction is known as re-factoring.\nThe ‚ÄúRule of Three‚Äù states that you should probably consider refactoring (i.e.¬†adding abstraction) whenever you find your code doing the same thing three times or more."
  },
  {
    "objectID": "lectures/99_dry_vs_wet.html#normalization",
    "href": "lectures/99_dry_vs_wet.html#normalization",
    "title": "EDS 217, Lecture 4: DRY üèú vs.¬†WET üåä",
    "section": "Normalization",
    "text": "Normalization\nNormalization is the process of structuring data in order to reduce redundancy and improve integrity."
  },
  {
    "objectID": "lectures/99_dry_vs_wet.html#normalization-1",
    "href": "lectures/99_dry_vs_wet.html#normalization-1",
    "title": "EDS 217, Lecture 4: DRY üèú vs.¬†WET üåä",
    "section": "Normalization",
    "text": "Normalization\nSome of the key principles of Normalization include:\n\nAll data have a Primary Key, which uniquely identifies a record. Usually, in python, this key is called an Index.\nAtomic columns, meaning entries contain a single value. This means no collections should appear as elements within a data table. (i.e.¬†‚Äúcells‚Äù in structured data should not contain lists!)\nNo transitive dependencies. This means that there should not be implicit associations between columns within data tables.\n\n\nPrimary Keys\nThis form of normalization is easy to obtain, as the idea of an Index is embedded in almost any Python data structure, and a core component of data structures witin pandas, which is the most popular data science library in python (coming next week!).\n\n\nPrimary Keys\n\n\nCode\n# All DataFrames in pandas are created with an index (i.e unique primary key)\nimport pandas as pd\naverage_high_temps = [18.3, 18.3, 18.9, 20.6, 21.1, 21.7,\n                      23.9, 24.4, 23.9, 22.8, 20.6, 18.3]\nsb_high_temp = pd.DataFrame(\n    average_high_temps, # This list will become a single column of values\n    columns=['Average_High_Temperature'] # This is the name of the column\n) # NOTE: use sb_high_temp.head() py-&gt;month_list\n#sb_high_temp.index = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\nsb_high_temp.head()\n\n\n\n\nAtomic Columns\nThe idea of atomic columns is that each element in a data structure should contain a unique value. This requirement is harder to obtain and you will sometimes violate it.\n\n\nCode\n# import pandas as pd\naverage_high_temps = [18.3, 18.3, 18.9, 20.6, 21.1, 21.7, 23.9, 24.4, 23.9, 22.8, 20.6, 18.3]\naverage_rainfall = [110.7, 119.1, 74.2, 31.5, 8.4, 2.3, 0.5, 1.3, 3.6, 22.9, 45.5, 77.2]\n\n# THIS DATAFRAME IS NOT ATOMIC. EACH ELEMENT IN THE COLUMN IS A LIST.\nsb_climate = pd.DataFrame([\n    [average_high_temps, # The first column will contain a list.\n     average_rainfall]], # The second column will also contain a list.\n    columns=['Monthly Average Temp', 'Monthly Average Rainfall'] # Column names\n)\nsb_climate.head()\n\n\n\n\nAtomic Columns\nThe idea of atomic columns is that each element in a data structure should contain a unique value. This requirement is harder to obtain and you will sometimes violate it.\n\n\nCode\nimport pandas as pd\naverage_high_temps = [18.3, 18.3, 18.9, 20.6, 21.1, 21.7, 23.9, 24.4, 23.9, 22.8, 20.6, 18.3]\naverage_rainfall = [110.7, 119.1, 74.2, 31.5, 8.4, 2.3, 0.5, 1.3, 3.6, 22.9, 45.5, 77.2]\n\n# THIS DATAFRAME IS ATOMIC. EACH ELEMENT IN THE COLUMN IS A SINGLE VALUE.\nsb_climate = pd.DataFrame({ # Using a dict to create the data frame.\n    'Average_High_Temperature':average_high_temps, # This is the first column\n    'Average_Rainfall':average_rainfall # This is the second column\n})\nsb_climate.index = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\nsb_climate.head()\n\n\n\n\nTransitive Dependencies\nThe idea of transitive dependencies is the inclusion of multiple associated attributes within the same data structure.\n\nTransitive dependencies make updating data very difficult, but they can be helpful in analyzying data.\nSo we should only introduce them in data that we will not be editing.\n\nUsually environmental data, and especially timeseries, are rarely modified after creation. So we don‚Äôt need to worry as much about these dependencies.\nFor example, contrast a data record of ‚Äútemperatures through time‚Äù to a data record of ‚Äúuser contacts in a social network‚Äù.\n\n\nTransitive Dependencies\nThe idea of transitive dependencies is the inclusion of multiple associated attributes within the same data structure.\n\n\nCode\nimport pandas as pd\naverage_high_temps = [18.3, 18.3, 18.9, 20.6, 21.1, 21.7, 23.9, 24.4, 23.9, 22.8, 20.6, 18.3]\naverage_rainfall = [110.7, 119.1, 74.2, 31.5, 8.4, 2.3, 0.5, 1.3, 3.6, 22.9, 45.5, 77.2]\n\n# TRANSITIVE ASSOCIATIONS EXIST BETWEEN MONTHS AND SEASONS IN THIS DATAFRAME:\nmonth = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\nseason = ['Winter', 'Winter', 'Spring', 'Spring', 'Spring', 'Summer', 'Summer', 'Summer', 'Fall', 'Fall', 'Fall', 'Winter']\nsb_climate = pd.DataFrame({ # Using a dict to create the data frame.\n    'Month': month,         # Adding month as the first column of the data frame\n    'Season': season,       # Adding the season for each month (this is a transitive dependency)\n    'Avg_High_Temp':average_high_temps, # This is the third column\n    'Avg_Rain':average_rainfall         # This is the fourth column\n})\nsb_climate.head()\n\n\n\n\nNormalization Summary\nIn general, for data analysis, basic normalization is handled for you.\n\nFor read only data with fixed associations, a lack of normalization is manageable.\nHowever, many analyses are easier if you structure your data in ways that are as normalized as possible.\nIf you are collecting data then it is important to develop an organization structure that is normalized."
  },
  {
    "objectID": "lectures/99_dry_vs_wet.html#the-end",
    "href": "lectures/99_dry_vs_wet.html#the-end",
    "title": "EDS 217, Lecture 4: DRY üèú vs.¬†WET üåä",
    "section": "The End",
    "text": "The End"
  },
  {
    "objectID": "lectures/02_helpGPT.html#finding-help",
    "href": "lectures/02_helpGPT.html#finding-help",
    "title": "EDS 217, Lecture 3: Getting Help",
    "section": "Finding Help",
    "text": "Finding Help\nWhen you get an error, or an unexpected result, or you are not sure what to do‚Ä¶\n\nOptions:\n\nFinding help inside Python\nFinding help outside Python\n\n\n\nFinding Help Inside Python\nHow do we interrogate the data (and other objects) we encounter while coding?\n\nmy_var = 'some_unknown_thing'\nWhat is it?\n\n\nCode\nmy_var = 'some_unknown_thing'\ntype(my_var)\n\n\nstr\n\n\nThe type() command tells you what sort of thing an object is.\n\n\nFinding Help Inside Python\nHow do we interrogate the data (and other objects) we encounter while coding?\n\nmy_var = 'some_unknown_thing'\nWhat can I do with it?\n\n\nCode\nmy_var = ['my', 'list', 'of', 'things']\nmy_var = my_var + ['a', 'nother', 'list']\nmy_var\n\n\n['my', 'list', 'of', 'things', 'a', 'nother', 'list']\n\n\nThe dir() command tells you what attributes an object has.\n\n\nUnderstanding object attributes\ndir(my_var)\n\n['__add__',\n '__class__',\n '__contains__',\n '__delattr__',\n '__delitem__',\n '__dir__',...\n]\n\n\nWhat‚Äôs with all these __attributes__ ?\n__attributes__ are internal (or private) attributes associated with all python objects.\nThese are called ‚Äúmagic‚Äù or ‚Äúdunder‚Äù methods.\ndunder ‚Üí ‚Äúdouble under‚Äù ‚Üí __\n\n\nUnderstanding object attributes‚Ä¶ ‚Äúdunder‚Äù the hood üòí\nEverything in Python is an object, and every operation corresponds to a method.\n\n\nCode\n# __add__ and __mul__. __len__. (None). 2 Wrongs.\n\n3 + 4\n\n\n7\n\n\n\n\nUnderstanding object attributes‚Ä¶ ‚Äúdunder‚Äù the hood üòí\nGenerally, you will not have to worry about dunder methods.\nHere‚Äôs a shortcut function to look at only non-dunder methods\n\n\nCode\ndef pdir(obj):\n    '''\n    pdir(): Return only the public attributes of an object\n    \n    Returns a list of only the non-dunder attributes \n    of an object by checking each attribute to see \n    if it starts with '__'\n    \n    \n    '''\n    public_attributes = []\n    for x in dir(obj):\n        if not x.startswith('__'):\n            public_attributes.append(x)\n\n    return public_attributes\n\n\nmy_list = ['a', 'b', 'c']\npdir(my_list)\n\n\n['append',\n 'clear',\n 'copy',\n 'count',\n 'extend',\n 'index',\n 'insert',\n 'pop',\n 'remove',\n 'reverse',\n 'sort']\n\n\n\n\nTab Completion for object introspection\n\nJupyter Environments:\nYou can use the &lt;tab&gt; key in iPython (or Jupyter environments) to explore object methods. By default, only ‚Äúpublic‚Äù (non-dunder) methods are returned.\n\n\nVSCode:\nYou can usually just pause typing and VSCode will provide object introspection:\n\n\nCode\nstring = 'some letters'\n\n\n\n\n\nGetting help() inside Python\nMost objects - especially packages and libraries - provide help documentation that can be accessed using the python helper function‚Ä¶ called‚Ä¶ help()\n\n\nCode\n# 3, help, str, soil...\nimport math\nhelp(math)\n\n\nHelp on module math:\n\nNAME\n    math\n\nMODULE REFERENCE\n    https://docs.python.org/3.10/library/math.html\n    \n    The following documentation is automatically generated from the Python\n    source files.  It may be incomplete, incorrect or include features that\n    are considered implementation detail and may vary between Python\n    implementations.  When in doubt, consult the module reference at the\n    location listed above.\n\nDESCRIPTION\n    This module provides access to the mathematical functions\n    defined by the C standard.\n\nFUNCTIONS\n    acos(x, /)\n        Return the arc cosine (measured in radians) of x.\n        \n        The result is between 0 and pi.\n    \n    acosh(x, /)\n        Return the inverse hyperbolic cosine of x.\n    \n    asin(x, /)\n        Return the arc sine (measured in radians) of x.\n        \n        The result is between -pi/2 and pi/2.\n    \n    asinh(x, /)\n        Return the inverse hyperbolic sine of x.\n    \n    atan(x, /)\n        Return the arc tangent (measured in radians) of x.\n        \n        The result is between -pi/2 and pi/2.\n    \n    atan2(y, x, /)\n        Return the arc tangent (measured in radians) of y/x.\n        \n        Unlike atan(y/x), the signs of both x and y are considered.\n    \n    atanh(x, /)\n        Return the inverse hyperbolic tangent of x.\n    \n    ceil(x, /)\n        Return the ceiling of x as an Integral.\n        \n        This is the smallest integer &gt;= x.\n    \n    comb(n, k, /)\n        Number of ways to choose k items from n items without repetition and without order.\n        \n        Evaluates to n! / (k! * (n - k)!) when k &lt;= n and evaluates\n        to zero when k &gt; n.\n        \n        Also called the binomial coefficient because it is equivalent\n        to the coefficient of k-th term in polynomial expansion of the\n        expression (1 + x)**n.\n        \n        Raises TypeError if either of the arguments are not integers.\n        Raises ValueError if either of the arguments are negative.\n    \n    copysign(x, y, /)\n        Return a float with the magnitude (absolute value) of x but the sign of y.\n        \n        On platforms that support signed zeros, copysign(1.0, -0.0)\n        returns -1.0.\n    \n    cos(x, /)\n        Return the cosine of x (measured in radians).\n    \n    cosh(x, /)\n        Return the hyperbolic cosine of x.\n    \n    degrees(x, /)\n        Convert angle x from radians to degrees.\n    \n    dist(p, q, /)\n        Return the Euclidean distance between two points p and q.\n        \n        The points should be specified as sequences (or iterables) of\n        coordinates.  Both inputs must have the same dimension.\n        \n        Roughly equivalent to:\n            sqrt(sum((px - qx) ** 2.0 for px, qx in zip(p, q)))\n    \n    erf(x, /)\n        Error function at x.\n    \n    erfc(x, /)\n        Complementary error function at x.\n    \n    exp(x, /)\n        Return e raised to the power of x.\n    \n    expm1(x, /)\n        Return exp(x)-1.\n        \n        This function avoids the loss of precision involved in the direct evaluation of exp(x)-1 for small x.\n    \n    fabs(x, /)\n        Return the absolute value of the float x.\n    \n    factorial(x, /)\n        Find x!.\n        \n        Raise a ValueError if x is negative or non-integral.\n    \n    floor(x, /)\n        Return the floor of x as an Integral.\n        \n        This is the largest integer &lt;= x.\n    \n    fmod(x, y, /)\n        Return fmod(x, y), according to platform C.\n        \n        x % y may differ.\n    \n    frexp(x, /)\n        Return the mantissa and exponent of x, as pair (m, e).\n        \n        m is a float and e is an int, such that x = m * 2.**e.\n        If x is 0, m and e are both 0.  Else 0.5 &lt;= abs(m) &lt; 1.0.\n    \n    fsum(seq, /)\n        Return an accurate floating point sum of values in the iterable seq.\n        \n        Assumes IEEE-754 floating point arithmetic.\n    \n    gamma(x, /)\n        Gamma function at x.\n    \n    gcd(*integers)\n        Greatest Common Divisor.\n    \n    hypot(...)\n        hypot(*coordinates) -&gt; value\n        \n        Multidimensional Euclidean distance from the origin to a point.\n        \n        Roughly equivalent to:\n            sqrt(sum(x**2 for x in coordinates))\n        \n        For a two dimensional point (x, y), gives the hypotenuse\n        using the Pythagorean theorem:  sqrt(x*x + y*y).\n        \n        For example, the hypotenuse of a 3/4/5 right triangle is:\n        \n            &gt;&gt;&gt; hypot(3.0, 4.0)\n            5.0\n    \n    isclose(a, b, *, rel_tol=1e-09, abs_tol=0.0)\n        Determine whether two floating point numbers are close in value.\n        \n          rel_tol\n            maximum difference for being considered \"close\", relative to the\n            magnitude of the input values\n          abs_tol\n            maximum difference for being considered \"close\", regardless of the\n            magnitude of the input values\n        \n        Return True if a is close in value to b, and False otherwise.\n        \n        For the values to be considered close, the difference between them\n        must be smaller than at least one of the tolerances.\n        \n        -inf, inf and NaN behave similarly to the IEEE 754 Standard.  That\n        is, NaN is not close to anything, even itself.  inf and -inf are\n        only close to themselves.\n    \n    isfinite(x, /)\n        Return True if x is neither an infinity nor a NaN, and False otherwise.\n    \n    isinf(x, /)\n        Return True if x is a positive or negative infinity, and False otherwise.\n    \n    isnan(x, /)\n        Return True if x is a NaN (not a number), and False otherwise.\n    \n    isqrt(n, /)\n        Return the integer part of the square root of the input.\n    \n    lcm(*integers)\n        Least Common Multiple.\n    \n    ldexp(x, i, /)\n        Return x * (2**i).\n        \n        This is essentially the inverse of frexp().\n    \n    lgamma(x, /)\n        Natural logarithm of absolute value of Gamma function at x.\n    \n    log(...)\n        log(x, [base=math.e])\n        Return the logarithm of x to the given base.\n        \n        If the base not specified, returns the natural logarithm (base e) of x.\n    \n    log10(x, /)\n        Return the base 10 logarithm of x.\n    \n    log1p(x, /)\n        Return the natural logarithm of 1+x (base e).\n        \n        The result is computed in a way which is accurate for x near zero.\n    \n    log2(x, /)\n        Return the base 2 logarithm of x.\n    \n    modf(x, /)\n        Return the fractional and integer parts of x.\n        \n        Both results carry the sign of x and are floats.\n    \n    nextafter(x, y, /)\n        Return the next floating-point value after x towards y.\n    \n    perm(n, k=None, /)\n        Number of ways to choose k items from n items without repetition and with order.\n        \n        Evaluates to n! / (n - k)! when k &lt;= n and evaluates\n        to zero when k &gt; n.\n        \n        If k is not specified or is None, then k defaults to n\n        and the function returns n!.\n        \n        Raises TypeError if either of the arguments are not integers.\n        Raises ValueError if either of the arguments are negative.\n    \n    pow(x, y, /)\n        Return x**y (x to the power of y).\n    \n    prod(iterable, /, *, start=1)\n        Calculate the product of all the elements in the input iterable.\n        \n        The default start value for the product is 1.\n        \n        When the iterable is empty, return the start value.  This function is\n        intended specifically for use with numeric values and may reject\n        non-numeric types.\n    \n    radians(x, /)\n        Convert angle x from degrees to radians.\n    \n    remainder(x, y, /)\n        Difference between x and the closest integer multiple of y.\n        \n        Return x - n*y where n*y is the closest integer multiple of y.\n        In the case where x is exactly halfway between two multiples of\n        y, the nearest even value of n is used. The result is always exact.\n    \n    sin(x, /)\n        Return the sine of x (measured in radians).\n    \n    sinh(x, /)\n        Return the hyperbolic sine of x.\n    \n    sqrt(x, /)\n        Return the square root of x.\n    \n    tan(x, /)\n        Return the tangent of x (measured in radians).\n    \n    tanh(x, /)\n        Return the hyperbolic tangent of x.\n    \n    trunc(x, /)\n        Truncates the Real x to the nearest Integral toward 0.\n        \n        Uses the __trunc__ magic method.\n    \n    ulp(x, /)\n        Return the value of the least significant bit of the float x.\n\nDATA\n    e = 2.718281828459045\n    inf = inf\n    nan = nan\n    pi = 3.141592653589793\n    tau = 6.283185307179586\n\nFILE\n    /Users/kellycaylor/mambaforge/envs/eds217_2023/lib/python3.10/lib-dynload/math.cpython-310-darwin.so\n\n\n\n\n\n\nGetting help?\nIn the iPython shell (or the Jupyter Notebook/Jupyter Lab environment), you can also access the help() command using ?.\n\n\nCode\nmath\n\n\n&lt;module 'math' from '/Users/kellycaylor/mambaforge/envs/eds217_2023/lib/python3.10/lib-dynload/math.cpython-310-darwin.so'&gt;\n\n\n\n\nGetting more help??\nIn the iPython shell (or the Jupyter Notebook/Jupyter Lab environment) you can use ?? to see the actual source code of python code\n\n\nCode\nhelp\n\n\nType help() for interactive help, or help(object) for help about object.\n\n\n\n\nGetting more help??\n?? only shows source code for for python functions that aren‚Äôt compiled to C code. Otherwise, it will show the same information as ?\n\n\n&lt;tab&gt; completion + ? = discovery & introspection\n\n\nCode\nmy_list = ['a', 'b', 'c']\nmy_list.append('d')\nmy_list\n\na = my_list.append\ntype(a)\n\na('c')\nprint(my_list)\n\n\n\n['a', 'b', 'c', 'd', 'c']\n\n\n\n\nCode\n# The End\n\n\n\n\nDebugging Code\nThe print command is the most commonly used debugging tool for beginners.\n# This code generates a `TypeError` that \n# x is not the right kind of variable.\ndo_something(x) \nThe print command is the most commonly used debugging tool for beginners.\n\n\nprint Formatting in Python 3.6 and greater.\nPython 3.6 introduced new format strings callded f-strings. These are strings that are prefixed with an f character and allow in-line variable substitution.\n\n\nCode\n# print using c-style format statements\nx = 3.45\nprint(f\"x = {x}\")\n\n\nx = 3.45\n\n\n\n\nCode\ndef do_something(x):\n    x = x / 2 \n    return x\n\n# This code generates a `TypeError` that \n# x is not the right kind of variable.\nx = 'f'\n# Check and see what is X?\nprint(\n    f\"calling do_something() with x={x}\" # Python f-string\n)\n\ndo_something(x) \n\n\ncalling do_something() with x=f\n\n\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n\n\n\nHow to get help outside of Python\nAs of Fall 2002: - O‚ÄôRielly Books (Requires UCSB login) - My O‚ÄôRielly pdf library: https://bit.ly/eds-217-books (Requires UCSB login)\nAs of Fall, 2022: - Python Docs - Stack Overflow - Talk Python - Ask Python\nAs of Fall, 2023:\nLLMs.\n\nChatGPT - Need $ for GPT-4, 3.X fine debugger, but not a great programmer.\nGitHub CoPilot - Should be able to get a free student account. Works great in VSCode.\n\n\n\nDebugging Code (using iPython as a debugger)\n\n\nCode\ndef do_somcething(x):\n    x = x * 2v\n    return x\nv\nx = 10\n\n# You can start an interactive python terminal _inside_ an active cell:\n#from IPython import embed; embed()\n\nx = do_something(x)\n\nprint(\"final: x =\",x)\n\n\n\n\nCode\nfrom IPython.core.debugger import set_trace as breakpoint\n\ndef do_something(x):\n    breakpoint()\n    x = x * 2\n    breakpoint()\n    return x\n\nx = 10\nbreakpoint()\ndo_something(x)\n\n\n\n\nDebugging Code (Using JupyterLab)\nFinally, JupyterLab has a visual debugging environment. You can activate it in any notebook by clicking the little üêû icon in the upper right of the notebook window.\n\n\nDebugging Code (REPL)\nThe Python REPL has a built-in debugging console. It can be instanced using the breakpoint() function anywhere in your code.\nNote: breakpoint() won‚Äôt currently work in Jupyter Notebooks or JupyterLab, unless you use from IPython.core.debugger import set_trace as breakpoint like we did in an earlier slide.\n\n\nCode\n## The End"
  },
  {
    "objectID": "lectures/01_the_zen_of_python.html",
    "href": "lectures/01_the_zen_of_python.html",
    "title": "EDS 217, Lecture 2: The Zen of Python",
    "section": "",
    "text": "Code\n# What is the Zen of Python??\nimport this"
  },
  {
    "objectID": "lectures/01_the_zen_of_python.html#python-errors",
    "href": "lectures/01_the_zen_of_python.html#python-errors",
    "title": "EDS 217, Lecture 2: The Zen of Python",
    "section": "Python Errors",
    "text": "Python Errors\nThere are two types of errors in Python: SyntaxErrors and Exceptions.\n\nSyntaxErrors\nA SyntaxError happens when the Python language interpreter (the parser) detects an incorrectly formatted statement.\nThis code is trying to divide two numbers, but there are mismatched parentheses. What happens when we run it?\n&gt;&gt;&gt; print( 5 / 4 ))\n\n\nCode\nprint( 5 / 4 ))\n\n\nSyntaxError: unmatched ')' (2701704956.py, line 1)\n\n\nWhen python says SyntaxError, you should read this as I don't know what you want me to do!?\nOften the error includes some indication of where the problem is, although this indication can sometimes be misleading if the detection occurs far away from the syntax problem that created the error. Often the interpreter will attempt to explain what the problem is!\n\n\nExceptions\nAn Exception happens the code you have written violates the Python language specification.\nThis code is trying to divide zero by 0. Its syntax is correct. But what happens when we run it?\n&gt;&gt;&gt; print( 0 / 0 )\n\n\nCode\nprint( 0 / 0 )\n\n\nZeroDivisionError: division by zero\n\n\nWhen python says anything other than SyntaxError, you should read this as You are asking to do something I can't do\nIn this case, the ZeroDivisionError is raised because the Python language specification does not allow for division by zero.\n\n\nTypes of Exceptions\nPython has a lot of builtin Errors that correspond to the definition of the Python language.\nA few common Exceptions you will see include TypeError, IndexError, and KeyError.\n\n\nTypeError\nA TypeError is raised when you try to perform a valid method on an inappropriate data type.\n\n\nCode\n# TypeError Examples:\n'a' + 3\n\n\n\n\nIndexError\nAn IndexError is raised when you try to access an undefined element of a sequence. Sequences are structured data types whose elements are stored in a specific order. A list is an example of a sequence.\n\n\nCode\n# IndexError Example:\nmy_list = ['a', 'b', 'c', 'd']\nmy_list[4]\n\n\n\n\nKeyError\nA KeyError is raised when you try to perform a valid method on an inappropriate data type.\n\n\nCode\n# KeyError Examples:\n\nmy_dict = {'column_1': 'definition 1', 'another_word': 'a second definition'}\nmy_dict['column12']\n\n\n\n\nDeciphering Tracebacks\nWhen an exception is raised in python the interpreter generates a ‚ÄúTraceback‚Äù that shows where and why the error occurred. Generally, the REPL has most detailed Traceback information, although Jupyter Notebooks and iPython interactive shells also provide necessary information to debug any exception.\n\n\nCode\n# defining a function\ndef multiply(num1, num2):\n    result = num1 * num2\n    print(results)\n \n# calling the function\nmultiply(10, 2)\n\n\nNameError: name 'results' is not defined\n\n\n\n\nCode\n## The End"
  },
  {
    "objectID": "lectures/00_intro_to_python.html#lecture-agenda",
    "href": "lectures/00_intro_to_python.html#lecture-agenda",
    "title": "Lecture 1 - Intro to Python and Environmental Data Science",
    "section": "Lecture Agenda",
    "text": "Lecture Agenda\n\nüêç What Python?\n‚ùì Why Python?\nüíª How Python?\n\n\n‚ÄúPython is powerful‚Ä¶ and fast; plays well with others; runs everywhere; is friendly & easy to learn; is Open.‚Äù"
  },
  {
    "objectID": "lectures/00_intro_to_python.html#what-is-python",
    "href": "lectures/00_intro_to_python.html#what-is-python",
    "title": "Lecture 1 - Intro to Python and Environmental Data Science",
    "section": "What is Python?",
    "text": "What is Python?\nPython is a general-purpose, object-oriented programming language that emphasizes code readability through its generous use of white space. Released in 1989, Python is easy to learn and a favorite of programmers and developers.\n\nHigh-level languages\n(Python, C, C++, Java, Javascript, R, Pascal) - Take less time to write - Shorter and easier to read - Portable, meaning that they can run on different kinds of computers with few or no modifications.\nThe engine that translates and runs Python is called the Python Interpreter\n\n\nCode\n\"\"\" \nEntering code into this notebook cell \nand pressing [SHIFT-ENTER] will cause the \npython interpreter to execute the code\n\"\"\"\nprint(\"Hello world!\")\nprint(\"[from this notebook cell]\")\n\n\nHello world!\n[from this notebook cell]\n\n\n\n\nCode\n\"\"\"\nAlternatively, you can run a \nany python script file (.py file)\nso long as it contains valid\npython code.\n\"\"\"\n!python hello_world.py\n\n\nHello world!\n[from hello_world.py]\n\n\n\n\nNatural vs.¬†Formal Languages\nNatural languages are the languages that people speak. They are not designed (although they are subjected to various degrees of ‚Äúorder‚Äù) and evolve naturally.\nFormal languages are languages that are designed by people for specific applications. - Mathematical Notation E=mc^2 - Chemical Notation: \\text{H}_2\\text{O}\nProgramming languages are formal languages that have been designed to express computations.\nParsing: The process of figuring out what the structure of a sentence or statement is (in a natural language you do this subconsciously).\nFormal Languages have strict syntax for tokens and structure:\n\nMathematical syntax error: E=\\$mü¶Ü_2 (bad tokens & bad structure)\nChemical syntax error: \\text{G}_3\\text{Z} (bad tokens, but structure is okay)\n\n\n\nDifferences between Natural and Formal languages\n\nAmbiguity: Natural languages are full of ambiguity, which people parse using contextual clues. Formal languages are nearly or completely unambiguous; any statement has exactly one meaning, regardless of context.\nRedundancy: In order to make up for ambiguity, natural languages employ lots of redundancy. Formal languages are less redundant and more concise.\nLiteralness: Formal languages mean exactly what they say. Natural languages employ idioms and metaphors.\n\nThe inherent differences between familiar natural languages and unfamiliar formal languages creates one of the greatest challenges in learning to code.\n\n\nA continuum of formalism\n\npoetry: Words are used for sound and meaning. Ambiguity is common and often deliberate.\nprose: The literal meaning of words is important, and the structure contributes meaning. Amenable to analysis but still often ambiguous.\nprogram: Meaning is unambiguous and literal, and can be understood entirely by analysis of the tokens and structure.\n\n\nStrategies for parsing formal languages:\n\nFormal languages are very dense, so it takes longer to read them.\nStructure is very important, so it is usually not a good idea to read from top to bottom, left to right. Instead, learn to parse the program in your head, identifying the tokens and interpreting the structure.\nDetails matter. Little things like spelling errors and bad punctuation, which you can get away with in natural languages, will make a big difference in a formal language."
  },
  {
    "objectID": "lectures/00_intro_to_python.html#why-python",
    "href": "lectures/00_intro_to_python.html#why-python",
    "title": "Lecture 1 - Intro to Python and Environmental Data Science",
    "section": "Why Python?",
    "text": "Why Python?\nIBM: R vs.¬†Python\nPython is a multi-purpose language with a readable syntax that‚Äôs easy to learn. Programmers use Python to delve into data analysis or use machine learning in scalable production environments.\nR is built by statisticians and leans heavily into statistical models and specialized analytics. Data scientists use R for deep statistical analysis, supported by just a few lines of code and beautiful data visualizations.\nIn general, R is better for initial exploratory analyses, statistical analyses, and data visualization.\nIn general, Python is better for working with APIs, writing maintainable, production-ready code, working with a diverse array of data, and building machine learning or AI workflows.\nBoth languages can do anything. Most data science teams use both languages. (and others too.. Matlab, Javascript, Go, Fortran, etc‚Ä¶)\n\n\nCode\nfrom IPython.lib.display import YouTubeVideo\nYouTubeVideo('MkNnAwkvvP8')\n\n\n\n        \n        \n\n\n\nLanguage Usage by Data Scientists\nAnaconda State of Data Science\nData from 2021:"
  },
  {
    "objectID": "lectures/00_intro_to_python.html#what-about-2022-data",
    "href": "lectures/00_intro_to_python.html#what-about-2022-data",
    "title": "Lecture 1 - Intro to Python and Environmental Data Science",
    "section": "What about 2022 data?",
    "text": "What about 2022 data?\nThe data are available here‚Ä¶\nLet‚Äôs do some python data science!\n\n\nCode\n# First, we need to gather our tools\nimport pandas as pd  # This is the most common data science package used in python!\nimport matplotlib.pyplot as plt # This is the most widely-used plotting package.\n\nimport requests # This package helps us make https requests \nimport io # This package is good at handling input/output streams\n\n\n\n\nCode\n# Here's the url for the 2022 data that we just looked at:\nurl = \"https://static.anaconda.cloud/content/Anaconda_2022_State_of_Data_Science_+Raw_Data.csv\"\n\n# Try to access the file using the requests library\nresponse = requests.get(url)\nresponse.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n\n# A 200 response code means our request was successful:\nprint(response)\n\n\n&lt;Response [200]&gt;\n\n\n\n\nCode\n# Read the response into a dataframe, using the io.StringIO function to feed the response.txt.\n# Also, skip the first three rows\ndf = pd.read_csv(io.StringIO(response.text), skiprows=3)\n\n# Our very first dataframe!\ndf.head()\n\n# Jupyter notebook cells only output the last value requested...\n\n\n\n\n\n\n\n\n\nIn which country is your primary residence?\nWhich of the following age groups best describes you?\nWhat is the highest level of education you've achieved?\nGender: How do you identify? - Selected Choice\nThe organization I work for is best classified as a:\nWhat is your primary role? - Selected Choice\nFor how many years have you been in your current role?\nWhat position did you hold prior to this? - Selected Choice\nHow would you rate your job satisfaction in your current role?\nWhat would cause you to leave your current employer for a new job? Please select the top option besides pay/benefits. - Selected Choice\n...\nWhat should an AutoML tool do for data scientists? Please drag answers to rank from most important to least important. (1=most important) - Help choose the best model types to solve specific problems\nWhat should an AutoML tool do for data scientists? Please drag answers to rank from most important to least important. (1=most important) - Speed up the ML pipeline by automating certain workflows (data cleaning, etc.)\nWhat should an AutoML tool do for data scientists? Please drag answers to rank from most important to least important. (1=most important) - Tune the model once performance (such as accuracy, etc.) starts to degrade\nWhat should an AutoML tool do for data scientists? Please drag answers to rank from most important to least important. (1=most important) - Other (please indicate)\nWhat do you think is the biggest problem in the data science/AI/ML space today? - Selected Choice\nWhat tools and resources do you feel are lacking for data scientists who want to learn and develop their skills? (Select all that apply). - Selected Choice\nHow do you typically learn about new tools and topics relevant to your role? (Select all that apply). - Selected Choice\nWhat are you most hoping to see from the data science industry this year? - Selected Choice\nWhat do you believe is the biggest challenge in the open-source community today? - Selected Choice\nHave supply chain disruption problems, such as the ongoing chip shortage, impacted your access to computing resources?\n\n\n\n\n0\nUnited States\n26-41\nDoctoral degree\nMale\nEducational institution\nData Scientist\n1-2 years\nData Scientist\nVery satisfied\nMore flexibility with my work hours\n...\n4.0\n2.0\n5.0\n6.0\nA reduction in job opportunities caused by aut...\nHands-on projects,Mentorship opportunities\nReading technical books, blogs, newsletters, a...\nFurther innovation in the open-source data sci...\nUndermanagement\nNo\n\n\n1\nUnited States\n42-57\nDoctoral degree\nMale\nCommercial (for-profit) entity\nProduct Manager\n5-6 years\nNaN\nVery satisfied\nMore responsibility/opportunity for career adv...\n...\n2.0\n5.0\n4.0\n6.0\nSocial impacts from bias in data and models\nTailored learning paths\nFree video content (e.g. YouTube)\nMore specialized data science hardware\nPublic trust\nYes\n\n\n2\nIndia\n18-25\nBachelor's degree\nFemale\nEducational institution\nData Scientist\nNaN\nNaN\nNaN\nNaN\n...\n1.0\n4.0\n2.0\n6.0\nA reduction in job opportunities caused by aut...\nHands-on projects,Mentorship opportunities\nReading technical books, blogs, newsletters, a...\nFurther innovation in the open-source data sci...\nUndermanagement\nI'm not sure\n\n\n3\nUnited States\n42-57\nBachelor's degree\nMale\nCommercial (for-profit) entity\nProfessor/Instructor/Researcher\n10+ years\nNaN\nModerately satisfied\nMore responsibility/opportunity for career adv...\n...\n1.0\n5.0\n4.0\n6.0\nSocial impacts from bias in data and models\nHands-on projects\nReading technical books, blogs, newsletters, a...\nNew optimized models that allow for more compl...\nTalent shortage\nNo\n\n\n4\nSingapore\n18-25\nHigh School or equivalent\nMale\nNaN\nStudent\nNaN\nNaN\nNaN\nNaN\n...\n4.0\n2.0\n3.0\n6.0\nSocial impacts from bias in data and models\nCommunity engagement and learning platforms,Ta...\nReading technical books, blogs, newsletters, a...\nFurther innovation in the open-source data sci...\nUndermanagement\nYes\n\n\n\n\n5 rows √ó 120 columns\n\n\n\n\n\nCode\n# Read the response into a dataframe, using the io.StringIO function to feed the response.txt.\n# Also, skip the first three rows\ndf = pd.read_csv(io.StringIO(response.text), skiprows=3)\n\n# Our very first dataframe!\ndf.head()\n\n# Jupyter notebook cells only output the last value... unless you use print commands!\nprint(f'Number of survey responses: {len(df)}')\nprint(f'Number of survey questions: {len(df.columns)}')\n\n\nNumber of survey responses: 3493\nNumber of survey questions: 120\n\n\n\n\nCode\n# 1. Filter the dataframe to only the questions about programming language usage, and \nfiltered_df = df.filter(like='How often do you use the following languages?').copy() # Use copy to force python to make a new copy of the data, not just a reference to a subset.\n\n# 2. Rename the columns to just be the programming languages, without the question preamble\nfiltered_df.rename(columns=lambda x: x.split('-')[-1].strip() if '-' in x else x, inplace=True)\n\n\n\n\nCode\n# Calculate the percentage of each response for each language\npercentage_df = filtered_df.apply(lambda x: x.value_counts(normalize=True).fillna(0) * 100).transpose()\n\n# Remove the last row, which is the \"Other\" category\npercentage_df = percentage_df[:-1]\n\n# Sort the DataFrame based on the 'Always' responses\nsorted_percentage_df = percentage_df.sort_values(by='Always', ascending=True)\n\n\n\n\nCode\n# Let's get ready to plot the 2022 data...\nfrom IPython.display import display\n\n# We are going to use the display command to update our figure over multiple cells. \n# This usually isn't necessary, but it's helpful here to see how each set of commands updates the figure\n\n# Define the custom order for plotting\norder = ['Always', 'Frequently', 'Sometimes', 'Rarely', 'Never']\n\ncolors = {\n    'Always': (8/255, 40/255, 81/255),       # Replace R1, G1, B1 with the RGB values for 'Dark Blue'\n    'Frequently': (12/255, 96/255, 152/255),   # Replace R2, G2, B2 with the RGB values for 'Light Ocean Blue'\n    'Sometimes': (16/255, 146/255, 136/255),    # and so on...\n    'Rarely': (11/255, 88/255, 73/255),\n    'Never': (52/255, 163/255, 32/255)\n}\n\n\n\n\nCode\n# Make the plot\nfig, ax = plt.subplots(figsize=(10, 7))\nsorted_percentage_df[order].plot(kind='barh', stacked=True, ax=ax, color=[colors[label] for label in order])\nax.set_xlabel('Percentage')\nax.set_title('Frequency of Language Usage, 2022',y=1.05)\n\nplt.show() # This command draws our figure. \n\n\n\n\n\n\n\nCode\n# Add labels across the top, like in the original graph\n\n# Get the patches for the top-most bar\nnum_languages = len(sorted_percentage_df)\n\npatches = ax.patches[num_languages-1::num_languages]\n# Calculate the cumulative width of the patches for the top-most bar\ncumulative_widths = [0] * len(order)\nwidths = [patch.get_width() for patch in patches]\nfor i, width in enumerate(widths):\n    cumulative_widths[i] = width + (cumulative_widths[i-1] if i &gt; 0 else 0)\n\n# Add text labels above the bars\nfor i, (width, label) in enumerate(zip(cumulative_widths, order)):\n    # Get the color of the current bar segment\n    # Calculate the position for the text label\n    position = width - (patches[i].get_width() / 2)\n    # Add the text label to the plot\n    # Adjust the y-coordinate for the text label\n    y_position = len(sorted_percentage_df) - 0.3  # Adjust the 0.3 value as needed\n    ax.text(position, y_position, label, ha='center', color=colors[label], fontweight='bold')\n\n# Remove the legend\nax.legend().set_visible(False)\n\n#plt.show()\ndisplay(fig) # This command shows our updated figure (we can't re-use \"plt.show()\")\n\n\n\n\n\n\n\nCode\n# Add percentage values inside each patch\nfor patch in ax.patches:\n    # Get the width and height of the patch\n    width, height = patch.get_width(), patch.get_height()\n    \n    # Calculate the position for the text label\n    x = patch.get_x() + width / 2\n    y = patch.get_y() + height / 2\n    \n    # Get the percentage value for the current patch\n    percentage = \"{:.0f}%\".format(width)\n    \n    # Add the text label to the plot\n    ax.text(x, y, percentage, ha='center', va='center', color='white', fontweight='bold')\n\ndisplay(fig) # Let's see those nice text labels!\n\n\n\n\n\n\n\nCode\n# Clean up the figure to remove spines and unecessary labels/ticks, etc..\n\n# Remove x-axis label\nax.set_xlabel('')\n\n# Remove the spines\nax.spines['top'].set_visible(False)\nax.spines['right'].set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(False)\n\n# Remove the y-axis tick marks\nax.tick_params(axis='y', which='both', length=0)\n\n# Remove the x-axis tick marks and labels\nax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n\ndisplay(fig) # Now 100% less visually cluttered!"
  },
  {
    "objectID": "lectures/00_intro_to_python.html#the-end",
    "href": "lectures/00_intro_to_python.html#the-end",
    "title": "Lecture 1 - Intro to Python and Environmental Data Science",
    "section": "The End",
    "text": "The End"
  },
  {
    "objectID": "resources/labs/Lab-1_Intro_Slides.html#setting-up-the-lab",
    "href": "resources/labs/Lab-1_Intro_Slides.html#setting-up-the-lab",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "1. Setting up the Lab",
    "text": "1. Setting up the Lab\n\npygeohydro is a Python library designed to aid in watershed analysis.\nPygeohydro is capable of downloading, preprocessing, and visualizing climatological, hydrological, and geographical datasets pertaining to a given watershed.\nSupported datasets include: Daymet climate, USGS streamflow, and data from the National Land Cover Dataset.\n\n\n2.1 PyGeoHydro (formerly hydrodata)\n\nIt‚Äôs easy to use pygeohydro to get streamflow data for any USGS gauge.\n\nYour lab work will focus on a stream gauge near to some favorite (or personally interesting) place in the U.S.\n\n\n\nCode\nimport pandas as pd\nfrom pygeohydro import Station\nimport pygeohydro.datasets as hds\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\") # Don't output warnings"
  },
  {
    "objectID": "resources/labs/Lab-1_Intro_Slides.html#viewing-watershed-hydrography",
    "href": "resources/labs/Lab-1_Intro_Slides.html#viewing-watershed-hydrography",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "Viewing Watershed Hydrography",
    "text": "Viewing Watershed Hydrography\nWe can use some of the dataset interfaces provided by pygeohydro to map the stream channels within our watershed.\nAccess to the NHDPlus dataset is provided by the NLDI object, which is part of pygeohydro‚Äôs datasets class, which we imported earlier as hds. To query these datasets, we use the following code:\n\n\nCode\n# Find the hydrography associated with this station using the wshed.station_id:\ntributaries = hds.NLDI.tributaries(wshed.station_id)\nmain_channel = hds.NLDI.main(wshed.station_id)\nstations = hds.NLDI.stations(wshed.station_id)"
  },
  {
    "objectID": "resources/labs/Lab-1_Intro_Slides.html#viewing-watershed-hydrography-1",
    "href": "resources/labs/Lab-1_Intro_Slides.html#viewing-watershed-hydrography-1",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "Viewing Watershed Hydrography",
    "text": "Viewing Watershed Hydrography\nWe can then plot these data using internal pygeohydro plotting functions, which are special versions of matplotlib plotting functions with extra features for viewing our stream channels.\n\n\nCode\n# Create a plot of the basin boundaries, main channel, tributaries, and station locations. \nax = wshed.basin.plot(color='white', edgecolor='black', zorder=1, figsize = (10, 10))\ntributaries.plot(ax=ax, label='Tributaries', zorder=2)\nmain_channel.plot(ax=ax, color='green', lw=3, label='Main', zorder=3)\nstations.plot(ax=ax, color='black', label='All stations', marker='s', zorder=5)\nax.legend(loc='upper left');\nax.figure.set_dpi(100);\nax.set_title(u'Sisquoc River near Sisquoc, CA ({lon:.2f}\\U000000b0W, {lat:.2f}\\U000000b0N)'.format(\n    lon=abs(wshed.lon),\n    lat=wshed.lat))"
  },
  {
    "objectID": "resources/labs/Lab-1_Intro_Slides.html#viewing-basin-topography-using-digital-elevation-data",
    "href": "resources/labs/Lab-1_Intro_Slides.html#viewing-basin-topography-using-digital-elevation-data",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "Viewing Basin Topography Using Digital Elevation Data",
    "text": "Viewing Basin Topography Using Digital Elevation Data\nWe can get a better sense of this topography using Digital Elevation Data released by the USGS.\n\n\nCode\n# Grab the watershed Digital Elevation Data from the USGS and display it:\ndem = hds.nationalmap_dem(wshed.geometry, resolution=1)\ndem.plot(size=8);"
  },
  {
    "objectID": "resources/labs/Lab-1_Intro_Slides.html#a-catchment-as-a-physical-system",
    "href": "resources/labs/Lab-1_Intro_Slides.html#a-catchment-as-a-physical-system",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "A Catchment as a Physical System",
    "text": "A Catchment as a Physical System\nThe water balance of a catchment is determined by the sum of inputs and outputs of water. If inputs are greater than outputs, the catchment is gaining water. If outputs are greater than inputs, the catchment is losing water. In the special case where outputs and inputs are equal, the catchment is neither gaining nor losing water, and the amount of water in the catchment is constant with time, meaning it is in steady state.\nThe change in storage of water in the catchment (\\Delta S, [m^3]) can be written as:\n$ S = P - ET - Q $\nUsually, we calculate the water balance over discrete time intervals, \\Delta t (e.g.¬†days, or years), in which case, the equation becomes:\n$ = P(t) - ET(t) - Q(t) $\nand we have implicitly re-defined the units of P, ET, and Q to be mass per time, [m^3/t]."
  },
  {
    "objectID": "resources/labs/Lab-1_Intro_Slides.html#obtaining-precipitation-discharge-and-rainfall-data-for-a-usgs-station",
    "href": "resources/labs/Lab-1_Intro_Slides.html#obtaining-precipitation-discharge-and-rainfall-data-for-a-usgs-station",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "Obtaining Precipitation, Discharge, and Rainfall data for a USGS station",
    "text": "Obtaining Precipitation, Discharge, and Rainfall data for a USGS station\nSo far, we‚Äôve been able to define a catchment upstream of a USGS gauge and visualize basin stream networks and topography. In order to study our simple basin water balance, we will need to obtain data on each of the three main fluxes: Q, P, and ET.\n\nDischarge Data for our Station\nThe pygeohydro allows us to access streamflow data stored in the National Water Information System (NWIS), which contains water resource data for almost 1.5 million locations across the United States and US Territories.\n\n\nCode\nQ = hds.nwis_streamflow(wshed.station_id, wshed.start, wshed.end)\nQ.columns = ['Q [m^3/s]']\nQ.index.name = 'Datetime'\nQ['Dates'] = Q.index\nQ.tail()\n\n\n\n\nPrecipitation Data for our Station\nUnfortunately, most USGS streamflow gauges aren‚Äôt at locations where rainfall data is also collected. Even if they were, rainfall varies across even small basins, so we‚Äôd still need to have some idea of what the average rainfall was over the entire watershed. Helpfully, the hydromet library includes the ability to query the Daily Surface Weather and Climatological Summaries (Daymet), which is hosted at Oak Ridge National Laboratory‚Äôs (ORNL‚Äôs) Distributed Active Archive Center (DAAC).\n\n\nCode\ndaymet = hds.daymet_byloc(wshed.lon, wshed.lat, start=wshed.start, end=wshed.end)\ndaymet.head()\n\n\n\n\nCode\nP = pd.DataFrame(daymet['prcp (mm/day)'])\nP.columns = ['P [mm/day]']\nP.index.name = 'Datetime'\nP['Dates'] = P.index\nP.head()\n\n\n\n\nEvapotranspiration Data for our Station\nEvapotranspiration data is the hardest to obtain as there are very few direct observations of evapotranspiration (but we will work with some of these direct measurements in our next lab!). For this lab, we are going to use a model-based estimate of actual evapotranspiration called the Operational Simplified Surface Energy Balance (SSEBop) model, which was developed at the USGS‚Äôs Earth Resources Observation and Science (EROS) Center.\nWe can access the SSEBop model estimates of evapotranspiration using the ssedopeta_byloc method:\n\n\nCode\n# Let's, load some SSEBop data that we've already downloaded from a csv using the pd.read_csv() command:\nET = pd.read_csv('../data/lab_1/SSEBop_11123500.csv', usecols=['ET [mm/day]', 'datetime'], index_col='datetime')\n\n# Just as with our P and Q data, we assign the index and create a column of dates.\nET.index.name = 'Datetime'\nET['Dates'] = ET.index\nET.head()"
  },
  {
    "objectID": "resources/labs/Lab-1_Intro_Slides.html#hydromet-plotting-tools",
    "href": "resources/labs/Lab-1_Intro_Slides.html#hydromet-plotting-tools",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "Hydromet Plotting tools",
    "text": "Hydromet Plotting tools\nThe hydrodata package includes a handy utility for plotting streamflow and rainfall. I‚Äôve modified this utility to give us a way to quickly look at the time series of our rainfall and streamflow data.\n\n\nCode\n# Visualize streamflow and rainfall data from our station\nfrom plot_Q_and_P import plot_Q_and_P\n\nplot_Q_and_P({\"Q\": (Q['Q [m^3/s]'], wshed.drainage_area)}, P['P [mm/day]'], figsize=(13,6))"
  },
  {
    "objectID": "resources/labs/Lab-1_Intro_Slides.html#extra-stuff-land-use-data",
    "href": "resources/labs/Lab-1_Intro_Slides.html#extra-stuff-land-use-data",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "Extra Stuff: Land Use Data",
    "text": "Extra Stuff: Land Use Data\nWhile we don‚Äôt use it in this lab, hydrodata also makes it possible to access land use and land cover data can be obtained from the Multi-Resolution Land Characteristics (MRLC) Consortium. By default the data are downloaded from NLCD 2016.\n\n\nCode\nimport rasterio\nimport matplotlib.pyplot as plt\nfrom rasterio.plot import show\nfrom hydrodata import plot as hydroplot\n\ncmap, norm, levels = hydroplot.cover_legends()\n# Download LULC data for this watershed\nlulc = hds.nlcd(wshed.geometry, resolution=1)\n\n\n\n\nCode\n# Plot the watershed Land Cover and Canopy Cover for 2016\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 7), dpi=300)\nshow(lulc['cover'], ax=ax1, title=f'Cover 2016', cmap=cmap, norm=norm)\nshow(lulc['canopy'], ax=ax2, title=f'Canopy 2016', cmap='Greens')"
  },
  {
    "objectID": "resources/labs/Lab-2_Energy_Balance.html",
    "href": "resources/labs/Lab-2_Energy_Balance.html",
    "title": "Lab 2: Landscape Energy Balance",
    "section": "",
    "text": "In the last lab, we used mass balance concepts to calculate the water balance within a watershed. In this lab, will will use energy balance concepts to explore the fluxes of energy across a landscape (more specifically, \\uparrow this landscape!)."
  },
  {
    "objectID": "resources/labs/Lab-2_Energy_Balance.html#shortwave-radiation",
    "href": "resources/labs/Lab-2_Energy_Balance.html#shortwave-radiation",
    "title": "Lab 2: Landscape Energy Balance",
    "section": "2.1. Shortwave radiation",
    "text": "2.1. Shortwave radiation\nThe vast majority of energy entering the Earth‚Äôs atmosphere is shortwave radiation from the sun, R^{\\ \\downarrow}_{_{SW}}, and falls in the ultraviolet, visible, and infrared portions of the electromagnetic spectrum. As we saw in Exercise 1.1, the flux density of the sun‚Äôs energy at the Earth‚Äôs atmosphere, known as the solar constant, is about 1370 W m-2. This is the maximum value of R^{\\downarrow}_{_{SW}}, but this quantity can fluctuate based on seasonal and diurnal geometry, attenuation by the atmosphere, and the amount of clouds and aerosols in the atmosphere. Of the solar radiation that does pass through the atmosphere, about 30% is reflected by the Earth‚Äôs surface, leading to outgoing shortwave radiation, R^{\\uparrow}_{_{SW}}. The ratio of reflected to incoming shortwave radiation is known as albedo, \\alpha.\n\\begin{equation}\n\\tag{2.2}\n    \\alpha \\ = \\ \\frac{R^{\\uparrow}_{_{SW}}}{R^{\\downarrow}_{_{SW}}}\n\\end{equation}\nBecause ~30% of incoming solar radiation is reflected on average, the Earth‚Äôs average global albedo is 0.3. This value varies between 0 and 1, however, based on the surface. Dark objects like asphalt absorb most of the incident radiation causing them to have a very low albedo. In contrast, light objects like freshly fallen snow have an albedo close to 1 as they reflect nearly all incident energy.\nUsing Equation 2.2, we can write the net shortwave radiation (i.e.¬†the difference between incoming and outgoing shortwave radiation) as\n\\begin{equation}\n\\tag{2.3}\n    R^{\\downarrow}_{_{SW}} \\ - \\ R^{\\uparrow}_{_{SW}} \\ = \\ (1 \\ - \\ \\alpha) R^{\\downarrow}_{_{SW}}\n\\end{equation}"
  },
  {
    "objectID": "resources/labs/Lab-2_Energy_Balance.html#longwave-radiation",
    "href": "resources/labs/Lab-2_Energy_Balance.html#longwave-radiation",
    "title": "Lab 2: Landscape Energy Balance",
    "section": "2.2. Longwave radiation",
    "text": "2.2. Longwave radiation\nSome incoming solar radiation is absorbed by larger particles in the Earth‚Äôs atmosphere, causing the atmosphere to warm. The heat generated by this absorption is emitted in the far infrared portion of the elecromagnetic spectrum, some of which radiates to the surface to produce incoming longwave radiation, R^{\\downarrow}_{_{LW}}. The value of R^{\\downarrow}_{_{LW}} varies depending on the temperature profile of the atmosphere and the amount of water vapor and other particles present. As the surface of the Earth heats up, thermal energy is re-emitted as outgoing longwave radiation, R^{\\uparrow}_{_{LW}}. This quantity depends on the temperature and emissivity of the surface, \\varepsilon. Both incoming and outgoing longwave radiation can be calculated from the Stefan-Boltzmann law, which describes the energy emitted from an object in terms of its temperature:\n\\begin{equation}\n\\tag{2.4}\n    j^* \\ = \\ \\varepsilon \\sigma T^{4}\n\\end{equation}\nwhere \\sigma is the Stefan-Boltzman constant with a value of 5.67 \\times 10^{-8} \\text{ W m}^{-2} \\text{ K}^{-4}.\nThus, we can rewrite Equation 2.1 to express net radiation in terms of these relationships in Equations 2.3 and 2.4:\n\\begin{equation}\n\\tag{2.5}\n    R^{}_{n} \\ = \\ (1-\\alpha) \\ R^{\\downarrow}_{_{SW}} \\ + \\ \\varepsilon_a \\sigma {T^{}_{a}}^{4} \\ - \\ \\varepsilon_s \\sigma {T^{}_{s}}^{4}\n\\end{equation}\nwhere \\varepsilon_a and \\varepsilon_s are the emissivities and T_a and T_s are the temperatures (in Kelvin) of the air and the surface, respectively.\nThe sum of the incoming shortwave radiation (corrected for albedo) and incoming longwave radiation represents the total energy available to the system, Q_{av}. We can rewrite Equation 2.5 in terms of available energy:\n\\begin{equation}\n\\tag{2.6}\n    Q_{av} \\ = \\ (1-\\alpha) \\ R^{\\downarrow}_{_{SW}} \\ + \\ \\varepsilon_a \\sigma {T^{}_{a}}^{4} \\ = \\ R^{\\downarrow}_{_{SW}} \\, - \\, R^{\\uparrow}_{_{SW}} \\, + \\, R^{\\downarrow}_{_{LW}}  \\ = \\varepsilon_s \\sigma {T^{}_{s}}^{4}\n\\end{equation}"
  },
  {
    "objectID": "resources/labs/Lab-2_Energy_Balance.html#problems",
    "href": "resources/labs/Lab-2_Energy_Balance.html#problems",
    "title": "Lab 2: Landscape Energy Balance",
    "section": "Problems",
    "text": "Problems\n\n2.1. Determine the albedo of the landscape.\n\nüìö  &lt;b&gt; Question 2.1. &lt;/b&gt; \nCalculate daily albedo values using the average daily $R^{\\downarrow}_{_{SW}}$ and $R^{\\uparrow}_{_{SW}}$ values. What is the mean daily albedo of the landscape over the entire dataset?\n\n\n\n2.2. Estimate the surface temperature of the earth from Q_{av}\n\nüìö  &lt;b&gt; Question 2.2. &lt;/b&gt; \nCalculate $Q_{av}$ from $R^{\\downarrow}_{_{SW}}$, $R^{\\uparrow}_{_{SW}}$, and $R^{\\downarrow}_{_{LW}}$. Using the mean value of $Q_{av}$ over the entire dataset, use Equation 2.6 to calculate the apparent surface temperature of the earth, $T_s$, in both $^{\\circ}$C and $^{\\circ}$F assuming all of the available energy is used to heat the surface. Assume the surface has the same emissivity as the atmosphere, $\\varepsilon_s \\ = \\ 0.95$.\n\n\n\n2.3. Net radiation\nNow, calculate the actual net radiation from the dataset.\n\nüìö  &lt;b&gt; Question 2.3. &lt;/b&gt; \nUse Equation 2.1 and the actual values of outgoing longwave radiation to calculate net radiation. Plot net radiation from August to December for 2016 and 2019 on the same plot (Hint: use &lt;code&gt;rad.index.dayofyear&lt;/code&gt; on the x-axis to display the time series on top of one another). Label your plot appropriately.\n\n\n\n2.4. Diurnal radiation\nNext, let‚Äôs look at the diurnal variation in radiation values. Pick a single day of your choosing and plot all four components of radiation, as well as net radiation. You can use a boolean mask on your DataFrame to select a single date. For instance, to select the 11th of October 2019:\nrad[rad.index.date == datetime.date(2019,10,11)]\n\nüìö  &lt;b&gt; Question 2.4. &lt;/b&gt; \nPlot net radiation, as well as its four components, over the course of your chosen day. Which component is the largest? When is $R_n$ at its highest during the day? Its lowest? As always, be sure to label your plot appropriately."
  },
  {
    "objectID": "resources/labs/Lab-2_Energy_Balance.html#ground-heat-flux",
    "href": "resources/labs/Lab-2_Energy_Balance.html#ground-heat-flux",
    "title": "Lab 2: Landscape Energy Balance",
    "section": "3.1. Ground Heat Flux",
    "text": "3.1. Ground Heat Flux\nThe transfer of energy between two materials in contact with one another is known as conduction. Conduction is driven by temperature gradients between the two surfaces. In the Earth system context, thermal conduction of heat into the soil is quantified by the ground heat flux, G. In most landscapes, very little energy is stored in the ground, so G is often ignored in the landscape energy balance."
  },
  {
    "objectID": "resources/labs/Lab-2_Energy_Balance.html#sensible-heat-flux",
    "href": "resources/labs/Lab-2_Energy_Balance.html#sensible-heat-flux",
    "title": "Lab 2: Landscape Energy Balance",
    "section": "3.2. Sensible Heat Flux üå°Ô∏è",
    "text": "3.2. Sensible Heat Flux üå°Ô∏è\nSensible heat flux, H, refers to the loss of energy from the surface to the atmosphere via convection. Like conduction, convection is driven by differences in temperature, but this time between the surface and the air. That is,\n\\begin{equation}\n\\tag{3.2}\n    H \\ \\propto \\ T_s - T_a\n\\end{equation}\nThis proportionality between sensible heat flux and the temperature difference is determined by the properties of the surface and the turbulent flow across the surface:\n\\begin{equation}\n\\tag{3.2}\n    H \\ = \\ k_H \\rho_a c_p (T_s - T_a)\n\\end{equation}\nwhere k_H is the atmospheric conductivity, \\rho_a is the density of air, and c_p is the specific heat capacity of air (at constant pressure). Heat capacity refers to the amount of energy required to change the temperature of a substance. Though c_p varies slightly with air temperature, to simplify our calculations we will assume a constant value of 1004.67 \\text{ J kg}^{-1} \\text{ K}^{-1} for c_p.\nThe atmospheric conductivity depends on the roughness of the surface and the wind speed. While we cannot measure this quantity directly, we can use eddy covariance to calculate H. In the air, heat is transported by the turbulent flow of air in multiple layers. By averaging the product of the fluctuations of temperature and vertical wind, we can determine the amount of energy transported to or from the surface via the air ‚Äì i.e.¬†the sensible heat flux. Thus, our equation for H becomes\n\\begin{equation}\n\\tag{3.3}\n    H \\ = \\ \\overline{\\rho_{a}} \\ c_p \\ \\overline{w'T'}\n\\end{equation}\nThe overbars in this equation represent averages taken over a period of time when the mean vertical flow is negligible (conventionally 30 minutes). This method relies on sensors to measure the fluctuations of temperature, wind, and gas concentrations at very high frequencies in order to adequately sample the eddies that are responsible for transport. The two sensors - an anemometer and a gas analyzer ‚Äì must be a close together as possible to effectively sample the same pocket of air at the same time.\n\nThe gas analyzer provides a measurement of the concentration of water vapor in the air, which allows for calculation of air density over time. The average air density over any period of time can be expressed as the sum of the average density of dry air, \\overline{\\rho_d}, and the average vapor density, \\overline{\\rho_v}:\n\\begin{equation}\n\\tag{3.4}\n    \\overline{\\rho_{a}} \\ = \\ \\overline{\\rho_{v}} + \\overline{\\rho_d}\n\\end{equation}\nBecause we know the concentration of water vapor in the air, we can determine the atmospheric vapor pressure (or partial pressure of water vapor in the air), \\overline{e_a}. Using the ideal gas law, which relates the amount, pressure, and temperature of a gas, we can calculate vapor density:\n\\begin{equation}\n\\tag{3.5}\n    \\overline{\\rho_v} \\ = \\ \\frac{M_{_{\\text{H}_2\\text{O}}} \\cdot \\overline{e_a}}{R \\ \\overline{T_a}}\n\\end{equation}\nwhere M_{_{\\text{H}_2\\text{O}}} is the molar mass of water, R is the ideal gas constant, and \\overline{T_a} is the average air temperature.\nSimilarly, we can compute the dry air density over the same time period using the partial pressure of dry air, which is simply the difference between total air pressure, \\overline{P_a}, and the vapor pressure:\n\\begin{equation}\n\\tag{3.6}\n    \\overline{\\rho_d} \\ = \\ \\frac{M_{_{\\text{air}}} \\left( \\overline{P_a} \\ - \\ \\overline{e_a}\\right)}{R \\ \\overline{T_a}}\n\\end{equation}\nwhere M_{_{\\text{air}}} is the molar mass of dry air."
  },
  {
    "objectID": "resources/labs/Lab-2_Energy_Balance.html#latent-heat-flux",
    "href": "resources/labs/Lab-2_Energy_Balance.html#latent-heat-flux",
    "title": "Lab 2: Landscape Energy Balance",
    "section": "3.3. Latent Heat Flux üíß",
    "text": "3.3. Latent Heat Flux üíß\nLatent heat flux, \\lambda E, is the energy flux associated with phase changes (evaporation and condensation) between water at the Earth‚Äôs surface and water vapor in the air. \\lambda E is driven by gradients of vapor pressure between the surface (e_s) and the air (e_a):\n\\begin{equation}\n\\tag{3.7}\n    \\lambda E \\ \\propto \\ e_s - \\ e_a\n\\end{equation}\nThis relationship also depends on the density and total pressure of the air, the atmospheric conductivity to vapor transport, k_E, and the latent heat of vaporization of water, \\lambda_v (the energy required to convert one mole of H2O from liquid to gas at 100^{\\circ}C):\n\\begin{equation}\n\\tag{3.8}\n    \\lambda E \\ = \\ k_E \\lambda_v \\rho_a \\frac{M_{_{\\text{air}}}}{M_{_{\\text{H}_2\\text{O}}}} \\frac{e_s - e_a}{P_a}\n\\end{equation}\nLike the sensible heat flux, it is difficult to measure \\lambda E directly, but we can calculate it using eddy covariance:\n\\begin{equation}\n\\tag{3.9}\n    \\lambda E \\ = \\ \\overline{\\rho_a} \\lambda_v \\overline{w' \\chi_{v}'}\n\\end{equation}\nwhere \\chi_v is the mole fraction of water vapor in the air.\nWhile Equation 3.9 provides a rough approximation of latent heat flux, in practice, a number of corrections must be made to this equation to account for the separation of the sensors and the varying density of air due to both sensible and latent heat fluxes. These corrections have been implemented in a function (lhf.py) for ease of our computations. Thus, we can import the function and calculate \\lambda E as follows:\nfrom lhf import lhf_wpl\n\nlhf_wpl(df)\nFor now, we‚Äôll just import the function.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nfrom lhf import lhf_wpl"
  },
  {
    "objectID": "resources/labs/Lab-2_Energy_Balance.html#problems-1",
    "href": "resources/labs/Lab-2_Energy_Balance.html#problems-1",
    "title": "Lab 2: Landscape Energy Balance",
    "section": "Problems",
    "text": "Problems\n\n3.1. Density of air\nBefore calculating H, it‚Äôs useful to calculate the vapor density and densities of dry and wet air.\n\nüìö  &lt;b&gt; Question 3.1. &lt;/b&gt; \nUse equations 3.4, 3.5, and 3.6 to calculate $\\rho_v$, $\\rho_d$, and $\\rho_a$. Hint: be sure to convert temperature to K and check your units!. To make your life easier later on, add $T$ in K and each of the densities to the original &lt;span class=\"code\"&gt;DataFrame&lt;/span&gt; with the following names: &lt;code&gt;'T_K'&lt;/code&gt;, &lt;code&gt;'rho_v'&lt;/code&gt;, &lt;code&gt;'rho_d'&lt;/code&gt;, and &lt;code&gt;'rho_a'&lt;/code&gt;.\n\n\n\n3.2. Calculate H\nNow that you‚Äôve determined \\rho_a, you can use this value to calculate sensible heat flux.\n\nüìö  &lt;b&gt; Question 3.2. &lt;/b&gt; \nUse Equation 3.3 to determine sensible heat flux, $H$.\n\n\n\n3.3. Latent heat flux\n\nüìö  &lt;b&gt; Question 3.3. &lt;/b&gt; \nUse the &lt;code&gt;lhf_wpl&lt;/code&gt; function to determine latent heat flux, $\\lambda E$.\n\n\n\n3.4 Comparing turbulent fluxes\nNow that you‚Äôve calculated both sensible and latent heat, compare how the two vary. Consider the conditions under which H and \\lambda E are positive or negative. What physical processes drive these fluctuations?\n\nüìö  &lt;b&gt; Question 3.4. &lt;/b&gt; \nPlot sensible and latent heat for the last week of each year (beginning on December 25th). Be sure your plot(s) is/are appropriately scaled and labeled. How do they compare? When does $H$ exceed $\\lambda E$? When does $\\lambda E$ exceed $H$? Why do are the fluxes so different for the same week of 2016 and 2019 (Hint: there is one column of data you've been provided that you have not yet used.)?"
  },
  {
    "objectID": "resources/labs/Lab-1_Catchment_Water_Balance.html#instructions",
    "href": "resources/labs/Lab-1_Catchment_Water_Balance.html#instructions",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "1. Instructions",
    "text": "1. Instructions\nWork through the exercise, writing code where indicated. To run a cell, click on the cell and press ‚ÄúShift‚Äù + ‚ÄúEnter‚Äù or click the ‚ÄúRun‚Äù button in the toolbar at the top. Note: Do not restart the kernel and clear all outputs. If this happens, run the last cell in the notebook before proceeding.\n\nüêç ¬† ¬† This symbol designates an important note about Python structure, syntax, or another quirk.\n\n\nüìä ¬† ¬† This symbol designates an important note about Environmental Data methods, sources, and access.\n\n\n‚ñ∂Ô∏è ¬† ¬† This symbol designates a cell with code to be run.\n\n\n‚úèÔ∏è ¬† ¬† This symbol designates a partially coded cell with an example.\n\n\nüìö ¬† ¬† This symbol designates a practice question."
  },
  {
    "objectID": "resources/labs/Lab-1_Catchment_Water_Balance.html#setting-up-the-lab",
    "href": "resources/labs/Lab-1_Catchment_Water_Balance.html#setting-up-the-lab",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "2. Setting up the Lab",
    "text": "2. Setting up the Lab\nhydrodata is a Python library designed to aid in watershed analysis. Hydrodata is capable of downloading, preprocessing, and visualizing climatological, hydrological, and geographical datasets pertaining to a given watershed. Supported datasets include: Daymet climate, USGS streamflow, and data from the National Land Cover Dataset.\n\n2.1 Hydrodata\nIt‚Äôs easy to use hydrodata to get streamflow data for any USGS gauge. In the background for this lab, we will be looking at some gauges across southern California. Your lab will focus on a stream gauge near to some favorite (or personally interesting) place in the U.S.\nfrom hydrodata import Station\nimport hydrodata.datasets as hds\n\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nimport pandas as pd\nfrom hydrodata import Station\nimport hydrodata.datasets as hds\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\") # Don't output warnings\n\n\nModuleNotFoundError: No module named 'hydrodata'"
  },
  {
    "objectID": "resources/labs/Lab-1_Catchment_Water_Balance.html#background",
    "href": "resources/labs/Lab-1_Catchment_Water_Balance.html#background",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "3. Background",
    "text": "3. Background\nThis lab is going to give you some first exposure to doing analysis in pandas and creating your own Jupyter notebooks for presenting results and information. The main concept we will be considering is mass balance. Along with energy and momentum, mass is one of the three consitutents that are conserved in environmental systems. More generally, physical systems can be written as a statement of mass, energy, or momentum. The key to creating these statements (or equations) of balance is determining the bounds of the system (its geographic extent and structure) as well as the specific processes acting across those boundaries.\nLet‚Äôs take a look at a typical river basin, the upper portions of the Sisquoc River, which is a westward flowing river in northeastern Santa Barbara County, California. The Sisquoc is a tributary of the Santa Maria River, which is formed when the Sisquoc River meets the Cuyama River at the Santa Barbara County and San Luis Obispo County border just north of Garey. The river is 57.4 miles (92.4 km) long and originates on the north slopes of Big Pine Mountain, at approximately 6,320 feet (1,930 m). Big Pine Mountain is part of the San Rafael Mountains, which are part of the Transverse Ranges. Sisquoc is the Chumash word meaning ‚Äúquail‚Äù and the Sisquoc River is a designated National Wild and Scenic River managed by the US Forest Service.\nWe can use the hydrodata library Station object, which we imported earlier. To create an instance of the Station class, we need to specify a USGS station_id, which is a unique code that identifies each of the USGS gauging locations across the United States. In addition, we need to specify a start and anend date for our data request. These dates are used to determine what streamflow data to collect, as well as other meteorological data, which we will see later in the background section.\nFor this lab, we will be looking at the 2016, 2017, and 2018 water years. Water years are a bit like school years - they start in the fall instead of on January 1st. Specifically, a single water year begins on October 1 and ends on September 30th of the following year. Water years are designated by the calendar years in which they end (e.g.¬†WY 2015 began on October 1, 2014 and ended on September 30, 2015). Therefore, we will start our collection on the 1st of October 2015 and end our data on September 30, 2018.\nstation_id = '11138500'  # This is a USGS station ID. Later, I will show you how to find one on your own!\nstart = '2015-10-01'     # Start date is the Oct. 1, 2015 which is the start of the 2016 water year.\nend = '2018-09-30'       # End date is the Sept 30, 2018 which is the end of the 2018 water year.\n\n# Create our watershed object using the station_id, start, and end dates\nwshed = Station(start=start, end=end, station_id=station_id, data_dir=\"data/\")\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nstation_id = '11138500'  # This is a USGS station ID. Later, I will show you how to find one on your own!\nstart = '2015-10-01'     # Start date is the Oct. 1, 2015 which is the start of the 2016 water year.\nend = '2018-09-30'       # End date is the Sept 30, 2018 which is the end of the 2018 water year.\n\n# Create our watershed object using the station_id, start, and end dates\nwshed = Station(start=start, end=end, station_id=station_id, data_dir=\"../data/lab_1/\")"
  },
  {
    "objectID": "resources/labs/Lab-1_Catchment_Water_Balance.html#visualizing-watersheds",
    "href": "resources/labs/Lab-1_Catchment_Water_Balance.html#visualizing-watersheds",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "3.1 Visualizing watersheds",
    "text": "3.1 Visualizing watersheds\nA watershed (or catchment, or drainage basin) is any area of land where precipitation collects and drains off into a common outlet, such as into a river, bay, or other body of water. The watershed includes all the surface water from rain, runoff, snowmelt, hail, sleet and nearby streams that run downslope towards the shared outlet, as well as the groundwater underneath the Earth‚Äôs surface. Watersheds are hierarchical, which means they connect into other watersheds at lower elevations, with smaller sub-drainage basins, which in turn drain into another common outlet. This pattern of self-similar, hierarchical structures means that watersheds and their associated stream networks form a stochastic fractal network that collectively routes water and sediment over the Earth‚Äôs surface. Visualizing a watershed requires defining an outlet, which identifies the lowest point of the catchment and the location through which all surface water drains. In this lab, we will define the outlet of a watershed as the location of a USGS streamflow gauge. By linking the definition of our watersheds to the location of streamflow gauges, we will be able to examine the mass balance of the watershed. Before diving into the principles of mass balance as they apply to watersheds, let‚Äôs first spend a little time learning how to acquire and visualize watersheds based on the location of a USGS streamgauge.\n\n3.1.1 Viewing Watershed Hydrography\nWe can use some of the dataset interfaces provided by hydrodata to map the stream channels within our watershed.\n\nüìä &lt;b&gt;HNDPlus&lt;/b&gt;: The best source for hydrography data is the National Hydrography Dataset known as &lt;a href=\"https://www.usgs.gov/core-science-systems/ngp/national-hydrography/nhdplus-high-resolution\"&gt;NHDPlus&lt;/a&gt;. The NHDPlus High Resolution (HR) is a national, geospatial model of the flow of water across the landscape and through the stream network. The NHDPlus HR is built using the National Hydrography Dataset High Resolution data at 1:24,000 scale or better, the 1/3 arc-second (10 meter ground spacing) 3D Elevation Program data, and the nationally complete Watershed Boundary Dataset. \n\nAccess to the NHDPlus dataset is provided by the NLDI object, which is part of hydrodata‚Äôs datasets class, which we imported earlier as hds. To query these datasets, we use the following code:\n# Find the hydrography associated with this station using the wshed.station_id:\ntributaries = hds.NLDI.tributaries(wshed.station_id)\nmain_channel = hds.NLDI.main(wshed.station_id)\nstations = hds.NLDI.stations(wshed.station_id)\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Find the hydrography associated with this station using the wshed.station_id:\ntributaries = hds.NLDI.tributaries(wshed.station_id)\nmain_channel = hds.NLDI.main(wshed.station_id)\nstations = hds.NLDI.stations(wshed.station_id)\n\n\nWe can then plot these data using internal hydrodata plotting functions, which are special versions of matplotlib plotting functions with extra features for viewing our stream channels.\n\n# Create a plot of the basin boundaries, main channel, tributaries, and station locations. \nax = wshed.basin.plot(color='white', edgecolor='black', zorder=1, figsize = (10, 10))\ntributaries.plot(ax=ax, label='Tributaries', zorder=2)\nmain_channel.plot(ax=ax, color='green', lw=3, label='Main', zorder=3)\nstations.plot(ax=ax, color='black', label='All stations', marker='s', zorder=5)\nax.legend(loc='upper left');\nax.figure.set_dpi(100);\nax.set_title(u'Sisquoc River near Sisquoc, CA ({lon:.2f}\\U000000b0W, {lat:.2f}\\U000000b0N)'.format(\n    lon=abs(my_station.lon),\n    lat=my_station.lat))\n\nüêç &lt;b&gt;A Brief Aside About Python & Unicode.&lt;/b&gt; In the code above, we include a &lt;code&gt;u&lt;/code&gt; preceding the string in the &lt;code&gt;ax.set_title()&lt;/code&gt; command. The letter &lt;code&gt;u&lt;/code&gt; before a string is an indicatation that python should convert any &lt;a href=\"https://en.wikipedia.org/wiki/Unicode\"&gt;&lt;code&gt;unicode&lt;/code&gt;&lt;/a&gt; symbols within the string when displaying it. In this case, we included &lt;code&gt;\\U000000b0&lt;/code&gt;, which is the unicode symbol for a degree sign (¬∞), so our figure title will include these symbols when we run the code. \nYou can find the most recent unicode definitions for over one hundred and fifty thousand written characters, symbols, and emojis at https://unicode-table.com/en/. Any of these Unicode representations can be expressed in a python string using an encoding that starts with a /code&gt; and is followed by exactly 8 values. Therefore, to use any unicode value in your python strings - for example, the smiling emoji, üòÅ, which is listed as unicode symbol U+1F601 - you would include the unicode number (1F601) inside an 8-digit python unicode representation that would look like this: 001F601. The most common unicode symbols that were developed first, like the ¬∞ sign, only have 4 digits (U+00b0), so we must add 4 zeros before the symbol to get to a the 8-element representation that python requires: 00000b0.\n\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. (Feel free to experiment with some unicode characters in the title!)&lt;/b&gt;\n\n\n\nCode\n# Create a plot of the basin boundaries, main channel, tributaries, and station locations. \nax = wshed.basin.plot(color='white', edgecolor='black', zorder=1, figsize = (10, 10))\ntributaries.plot(ax=ax, label='Tributaries', zorder=2)\nmain_channel.plot(ax=ax, color='green', lw=3, label='Main', zorder=3)\nstations.plot(ax=ax, color='black', label='All stations', marker='s', zorder=5)\nax.legend(loc='upper left');\nax.figure.set_dpi(100);\nax.set_title(u'Sisquoc River near Sisquoc, CA ({lon:.2f}\\U000000b0W, {lat:.2f}\\U000000b0N)'.format(\n    lon=abs(wshed.lon),\n    lat=wshed.lat))\n\n\nIn the map above, we can the main branch (green line) of the portion of the Sisquoc River above the town of Sisquoc, CA. The river is flowing in an East-West direction. You can also get a sense of the basin strucuture, which is very typical for a headwater river. There are many side tributaries that extend outward from the main branch, and the overall shape of the basin is such that it is widest just below the mid-point of the main branch. The fractal structure of the draininge network is very apparent.\n\nüìä &lt;b&gt; Fractal River Basins.&lt;/b&gt;\nIf you‚Äôd like to learn more about the fractal geometry of river networks and their importance for governing mass and energy fluxes across the land surface, check out Fractal River Basins: Chance and Self-Organization written by Ignacio Rodriguez-Iturbe and Andrea Rinaldo.\n\n\n\n3.1.2 Viewing Basin Topography Using Digital Elevation Data\nWe can get a better sense of this topography using Digital Elevation Data released by the USGS.\n\nüìä &lt;b&gt;National Elevation Dataset.&lt;/b&gt; The National Elevation Dataset (NED) is a seamless raster product primarily derived from USGS 10- and 30-meter Digital Elevation Models (DEMs). NED data are available from The National Map Viewer as 1 arc-second (approximately 30 meters) for the CONUS, and at 1/3 and 1/9 arc-seconds (approximately 10 and 3 meters, respectively) for parts of the United States. The NED can be accessed through the &lt;a href=\"https://www.usgs.gov/core-science-systems/national-geospatial-program/national-map\"&gt;National Geospatial Program&lt;/a&gt;. The next generation of US elevation data is being created as part of the &lt;a href=\"https://www.usgs.gov/core-science-systems/ngp/3dep\"&gt;3DEP&lt;/a&gt; program. This program's goal is to complete acquisition of nationwide lidar (IfSAR in AK) by 2023 to provide the first-ever national baseline of consistent high-resolution elevation data ‚Äì both bare earth and 3D point clouds ‚Äì collected in a timeframe of less than a decade. \n\nThe hydrodata library makes it easy to query the National Elevation Dataset to obtain 1 arc-second (~30 meter) digital elevation model (DEM) for any watershed in the United States. Because we already have our basin boundaries, we can make a targeted request to get only the elevation data inside our basin. Then we just make another figure that depicts the elevation data for our basin using a builtin method that is part of the class created by our data request:\n\n# Grab the watershed Digital Elevation Data from the USGS:\ndem = hds.dem_bygeom(wshed.geometry, resolution=1)\ndem.plot(size=8);\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Grab the watershed Digital Elevation Data from the USGS:\ndem = hds.nationalmap_dem(wshed.geometry, resolution=1)\ndem.plot(size=8);"
  },
  {
    "objectID": "resources/labs/Lab-1_Catchment_Water_Balance.html#a-catchment-as-a-physical-system",
    "href": "resources/labs/Lab-1_Catchment_Water_Balance.html#a-catchment-as-a-physical-system",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "3.2 A Catchment as a Physical System",
    "text": "3.2 A Catchment as a Physical System\nIn hydrological sciences, we use the idea of a catchment (or river basin) as the boundary of a system through which water flows. Because water has a relatively constant density (1 g H2O = 1 cm3 H2O), we can measure water balance in terms of water volume (m3) in addition to water mass (103 kg).\nThe water balance of a catchment is determined by the sum of inputs and outputs of water. If inputs are greater than outputs, the catchment is gaining water. If outputs are greater than inputs, the catchment is losing water. In the special case where outputs and inputs are equal, the catchment is neither gaining nor losing water, and the amount of water in the catchment is constant with time, meaning it is in steady state.\nThe primary input of water into the land surface is through precipitation (P [m3]). Precipitation usually takes the form of either rain or snow. There are two main ways that water exits a catchment: (1) the transport of water out of the catchment through streamflow (or river discharge); and (2) the transport of water back into the atmosphere in the form of vapor, which is released from either soils (evaporation, E [m3]) or plants (transpiration, T [m3]). For simplicity, we often combine evaporation and transpiration into a single quantity, evapotranspiration (ET).\nHaving specified the important inputs and outputs, the change in storage of water in the catchment (\\Delta S, [m3]) can be written as:\n \\Delta S = P - ET - Q \nIn small basins, most of this stored water, S, is held in the soil. In larger basins, the storage of water in groundwater can be very important, as can the net movement of groundwater into and out of the basin. Finally, in basins where freezing conditions are common, large amounts of water can be stored as snow and ice.\nUsually, we calculate the water balance over discrete time intervals, \\Delta t (e.g.¬†days, or years), in which case, the equation becomes:\n\\frac{\\Delta S}{\\Delta t} = P(t) - ET(t) - Q(t) \nand we have implicitly re-defined the units of P, ET, and Q to be mass per time, [m3/t]."
  },
  {
    "objectID": "resources/labs/Lab-1_Catchment_Water_Balance.html#obtaining-precipitation-discharge-and-rainfall-data-for-a-usgs-station",
    "href": "resources/labs/Lab-1_Catchment_Water_Balance.html#obtaining-precipitation-discharge-and-rainfall-data-for-a-usgs-station",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "3.3 Obtaining Precipitation, Discharge, and Rainfall data for a USGS station",
    "text": "3.3 Obtaining Precipitation, Discharge, and Rainfall data for a USGS station\nSo far, we‚Äôve been able to define a catchment upstream of a USGS gauge and visualize basin stream networks and topography. In order to study our simple basin water balance, we will need to obtain data on each of the three main fluxes: Q, P, and ET.\n\n3.3.1 Discharge Data for our Station\nThe hydrodata allows us to access streamflow data stored in the National Water Information System (NWIS), which contains water resource data for almost 1.5 million locations across the United States and US Territories. The NWIS contains all station data on streamflow. In addition, it also has all federally-collected data on groundwater levels and water quality. The water quality data includes information on water temperature, conductance, nutrients, pH, pesticides, and volatile organic compounds. In this lab, we will only be looking at streamflow (discharge) data, but you should definitely explore some of the other data available!\nWe can obtain daily discharge data for our station using the nwis_streamflow method and specifying a station_id, start, and end. These three attributes were stored in the wshed object when we initialized it, so we can grab the values that way. The nwis_streamflow conveniently returns our data as a pandas DataFrame, with a single column of streamflow values indexed by their datetime and reported in cubic meters per second. We use the column method and index.name attribute to clarify what variable and units our new DataFrame contains.\nQ = hds.nwis_streamflow(wshed.station_id, wshed.start, wshed.end)\nQ.columns = ['Q [m^3/s]']\nQ.index.name = 'Datetime'\nQ['Dates'] = Q.index\nQ.head()\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nQ = hds.nwis_streamflow(wshed.station_id, wshed.start, wshed.end)\nQ.columns = ['Q [m^3/s]']\nQ.index.name = 'Datetime'\nQ['Dates'] = Q.index\nQ.head()\n\n\n\n\n3.3.2 Precipitation Data for our Station\nUnfortunately, most USGS streamflow gauges aren‚Äôt at locations where rainfall data is also collected. Even if they were, rainfall varies across even small basins, so we‚Äôd still need to have some idea of what the average rainfall was over the entire watershed. Helpfully, the hydromet library includes the ability to query the Daily Surface Weather and Climatological Summaries (Daymet), which is hosted at Oak Ridge National Laboratory‚Äôs (ORNL‚Äôs) Distributed Active Archive Center (DAAC).\n\nüìä &lt;b&gt;The ORNL DAAC:&lt;/b&gt; The ORNL DAAC is the world's largest archive of biogeochemical and spatial environmental data. It contains most of NASA's Earth Observing System satellite data, as well as many contributed data on environmental patterns and processes. In fact, the author's PhD &lt;a href=\"https://daac.ornl.gov/S2K/guides/kt_stem_map.html\"&gt;dissertation data&lt;/a&gt; from 2000 on the location, sizes, and characteristics of 1,000s of savanna trees in southern Africa is hosted on this site. The DAAC maintains a suite of &lt;a href=\"https://daac.ornl.gov/tools/\"&gt;tools&lt;/a&gt; that can be used to query and subset some its largest datasets. Many of these tools - like Daymet - have associated Python libraries that can make access even easier. \n\nJust as with the NWIS and the NLDI, we will access our daymet data using an interface that is defined in hydrodata‚Äôs datasets module. To begin with, we will just retrieve the meterological data for a single location. The specific method we will use is the daymet_byloc function. This function requires the latitiude and longitude of our location, as well as start and end dates for the observations.\n\ndaymet = hds.daymet_byloc(wshed.lon, wshed.lat, start=wshed.start, end=wshed.end)\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\ndaymet = hds.daymet_byloc(wshed.lon, wshed.lat, start=wshed.start, end=wshed.end)\ndaymet.head()\n\n\nWe can see that - just like our discharge data stored in Q - the hds.daymet_loc function has returned a pandas Dataframe object that contains columns for:\n\nLength of daylight in seconds: dayl (s)\nPrecipitation in mm/day: prcp (mm/day)\nAverage incoming solar radiation in Watts per m2: srad (W/m^2)\nSnow water equivalent in kg/m2: swe (kg/m^2)\nMaximum daily temperature in degrees Celsius: tmax (deg c)\nMinimum daily temperature in degrees Celsius: tmin (deg c)\nAverage water vapor pressure in Pascals: vp (Pa)\n\nFor this lab, we are only interested in the Precipitation data, although the snow water equivalent data could be important for a cooler basin. Therefore we simply create a new DataFrame containing only the prcp (mm/day) data. We also create a column of the dates for each observation as we did with the discharge data.\n\nP = pd.DataFrame(daymet['prcp (mm/day)'])\nP.columns = ['P [mm/day]']\nP['Dates'] = P.index\nP.head()\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nP = pd.DataFrame(daymet['prcp (mm/day)'])\nP.columns = ['P [mm/day]']\nP.index.name = 'Datetime'\nP['Dates'] = P.index\nP.head()\n\n\n\n\n3.3.3 Evapotranspiration Data for our Station\nEvapotranspiration data is the hardest to obtain as there are very few direct observations of evapotranspiration (but we will work with some of these direct measurements in our next lab!). Because evapotranspiration is dependent on atmospheric conditions and the amount of energy available to evaporate water at the surface, most attempts to characterize evapotranspiration focus on the potential rate at which water vapor can be transported from the surface into the atmosphere. However, in places where the availability of water limits the rate of evapotranspiration, the potential evapotranspiration (PET) can be much higher than actual rates. This makes determiniation of actual evapotranspiration quite difficult in water-limited regions.\nEstimation of actual evapotranspiration can be accomplished using either mass balance approaches, energy balance approaches, or a combination of both. For this lab, we are going to use a model-based estimate of actual evapotranspiration called the Operational Simplified Surface Energy Balance (SSEBop) model, which was developed at the USGS‚Äôs Earth Resources Observation and Science (EROS) Center. This approach is presented in Senay (2018) and Senay et al., (2013). Unfortunately, there‚Äôs no web service available for subsetting the data. Therefore, the hydrodata routine, ssebopeta_bygeom first downloads the entire national dataset for the requested period and then subsets the data based on the provided geometry locally. For this reason, it‚Äôs not nearly fast as other operations and the bottleneck is download speed, which can vary greatly based on available bandwidth.\n\nüìä &lt;b&gt;EROS Data Center&lt;/b&gt; EROS is home to the world's largest collection of remotely sensed images of the Earth‚Äôs land surface and the primary source of Landsat satellite images and data products. NASA‚Äôs Land Processes Distributed Active Archive Center (LP DAAC) is also located at EROS. EROS developed the SSEBop model that we are using in today's lab. A recent comparison of methods for estimating actual evapotranspiration can be found &lt;a href=\"https://pubs.usgs.gov/sir/2017/5087/sir20175087.pdf\"&gt;here&lt;/a&gt;.\n\nWe can access the SSEBop model estimates of evapotranspiration using the ssedopeta_byloc method:\n\nET = hds.ssebopeta_byloc(wshed.lon, wshed.lat, start=wshed.start, end=wshed.end)\n\nüìä &lt;b&gt;SSEBop Data Availability&lt;/b&gt; The SSEBop output at EROS isn't provided in real-time. &lt;b&gt;As of May 2020 only data through the end of 2018 were available&lt;/b&gt;. More details on these products can be found &lt;a href=\"https://earlywarning.usgs.gov/ssebop/modis/daily\"&gt;here&lt;/a&gt;.\n\n\nDownloading a single water year of ET data\nAs mentioned above, downloading the SSEBop data can take quite some time. We‚Äôve written a helper function that downloads a single water year of ET data at a time. The function requires a station_id as the first argument, and the requested water year as the second argument. It returns a pandas DataFrame containing ET values for each day in the water year.\nfrom get_ET_wateryear import get_ET_wateryear\n\nET_2015 = get_ET_wateryear(station_id, 2015)\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt; &lt;br&gt;&lt;hr style=\"border-top: 0.5px solid gray;\"&gt; NOTE: Currently, there are no methods to subset the SSEBop output before download. It takes a &lt;i&gt;very&lt;/i&gt; long time to download the daily national estimates for three full years (1,096 files). So we've pre-loaded the data as a CSV file for you in the class &lt;code&gt;data&lt;/code&gt; folder, and we will use that for now. When you complete your lab work, you can use the &lt;code&gt;get_ET_wateryear()&lt;/code&gt; function to get data for your gauge.\n\n\n\n\nCode\n# Instead, load the data from a csv using the pd.read_csv() command:\nET = pd.read_csv('../data/lab_1/SSEBop_11123500.csv', usecols=['ET [mm/day]', 'datetime'], \n                 index_col='datetime', parse_dates=True)\n\n# Just as with our P and Q data, we assign the index and create a column of dates.\nET.index.name = 'Datetime'\nET['Dates'] = ET.index\nET.head()\n\n\n\n\n\n3.3.4 Summing Fluxes by Water Year\nAs we analyze our basin‚Äôs water balance, we will want to quickly look at the total amount of a flux at a gauge location for a single water year. To do so, we‚Äôd need to create a function that averages the correct months (October to September of the following year). To do so, we need to add a column that is water year. This is easy to do. First we create a water_year function.\ndef water_year(date):\n    \"\"\" Determines the water year for a Dateteime date\n    \n    Parameters\n    ----------\n    \n        date : Datetime obj\n            A datetime stored as the pandas datetime object. \n    \n    Returns\n    -------\n        water_year : int\n            An integer corresponding to the water year of the datetime\n    \"\"\" \n    if date.month&gt;=10:\n        return date.year+1\n    else:\n        return date.year\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\ndef water_year(date):\n    \"\"\" Determines the water year for a Dateteime date\n\n    Parameters\n    ----------\n\n        date : Datetime obj\n            A datetime stored as the pandas datetime object. \n\n    Returns\n    -------\n        water_year : int\n            An integer corresponding to the water year of the datetime\n    \"\"\" \n    if date.month&gt;=10:\n        return date.year+1\n    else:\n        return date.year\n\n\nHaving defined the water_year() function, we can easily apply this function to every value in the Dates column of the P dataframe. We assign this value to a new column in the P dataframe, which we will call water_year.\n\nP['water_year'] = P['Dates'].apply(water_year)"
  },
  {
    "objectID": "resources/labs/Lab-1_Catchment_Water_Balance.html#additional-hydromet-plotting-tools",
    "href": "resources/labs/Lab-1_Catchment_Water_Balance.html#additional-hydromet-plotting-tools",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "3.4 Additional Hydromet Plotting tools",
    "text": "3.4 Additional Hydromet Plotting tools\nThe hydrodata package includes a handy utility for plotting streamflow and rainfall. We can import this utility and use our data on Q and P to plot the timeseries of both over the past three years with the signatures function.\nfrom hydrodata import plot as hydroplot\n\nhydroplot.signatures({\"Q\": (Q['Q [m^3/s]'], wshed.drainage_area)}, P['P [mm/day]'])\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below.\n\n\n\nCode\nfrom hydrodata import plot as hydroplot\n\nhydroplot.signatures({\"Q\": (Q['Q [m^3/s]'], wshed.drainage_area)}, P['P [mm/day]'])\n\n\nThe graphs above depict the timeseries of daily data as well as monthly and annual plots. Finally, it also includes a Flow Duration Curve plot, which shows the percent of time that flow was above different thresholds from the highest to the lowest values. These curves give a sense of how dynamic a river basin is with respect to discharge as well as the distribution of flow amounts. Reading the x-axis of the flow duration curve, we can see that the 50% exceedence flow (i.e.¬†the average amount of streamflow) is somewhere around 0.005 mm/day, which is very low. That corresponds to the fact that we only really see high flows in the Sisquoc during the 2017 water year, and only very small peaks in flow for the 2016 and 2018 water years."
  },
  {
    "objectID": "resources/labs/Lab-1_Catchment_Water_Balance.html#assessing-basin-water-balance",
    "href": "resources/labs/Lab-1_Catchment_Water_Balance.html#assessing-basin-water-balance",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "4.1 Assessing Basin Water Balance",
    "text": "4.1 Assessing Basin Water Balance\n\n4.1.0 Assign a water year for each entry in the Q, ET, and P dataframes.\n\nüìö  &lt;b&gt; Question 4.1.0. &lt;/b&gt; \nUse the &lt;code&gt;water_year()&lt;/code&gt; function that we created in Section 3.3.4 to create a new &lt;code&gt;water_year&lt;/code&gt; column in the DataFrames for your three fluxes (Note: You may want ‚Äì but are not required ‚Äì to combine the three DataFrames into a single DataFrame that contains separate columns for &lt;code&gt;Q [mm/day]&lt;/code&gt;, &lt;code&gt;P [mm/day]&lt;/code&gt;, and &lt;code&gt;ET [mm/day]&lt;/code&gt;.  \n\n\n\n4.1.1 Convert Discharge to units of mm/day\n\nüìä &lt;b&gt;Converting Units of Q&lt;/b&gt; The units of &lt;code&gt;Q&lt;/code&gt; that you have are in cubic meters per second, while your units of &lt;code&gt;P&lt;/code&gt; and &lt;code&gt;ET&lt;/code&gt; are both in units of millimeters per day. You need to convert the &lt;code&gt;Q&lt;/code&gt; values to mm/day. To do so, we need to figure out the discharge amount per unit area of the basin, which is converting the &lt;i&gt;flow&lt;/i&gt; (amount per time) to a &lt;i&gt;flux&lt;/i&gt; amount per time per unit area).\n\nHere are some helpful conversions and their rationale:\n\nThere are 86,400 seconds in one day. Therefore: $ 1 = 86,400 $\nConvert 1 \\frac{ \\mbox{m}^3}{\\mbox{day}} of streamflow to depth by dividing by the basin area in m^2. (recall there are 1,000,000 m2 per km2)\n\n\nüìö  &lt;b&gt; Question 4.1.1. &lt;/b&gt; \nUse the conversions above to create a function that converts a streamflow in units of cubic meters per second into units of mm/day. Use the docstrings below to help make your function.\n\n\n\nCode\ndef Q_mm(Q_cms, basin_area=None):\n    \"\"\" Convert discharge in units of m^3/s into units of mm/day.\n    \n    # The lines below are a doctest. \n    # If your function is working correctly, you should get 86.4\n    # when you call it using Q_mm(1, basin_area=1)\n    \n    &gt;&gt;&gt; Q_mm(1, basin_area=1)\n    86.4\n    \n    Parameters\n    ----------\n    \n        Q_cms : float\n            Streamflow in units of [m^3/s]\n    \n        basin_area : float\n            Upstream area of basin at location of flow measurement, km^2\n    \n    Returns\n    -------\n        Q_mm : float\n            Streamflow in units of mm/day\n    \n    \n    \n    \"\"\"\n\n    # Add your code here!\n\n\n\nüêç &lt;b&gt;Doctests.&lt;/b&gt; \nThe &lt;code&gt;docstrings&lt;/code&gt; in the cell above contain a &lt;code&gt;doctest&lt;/code&gt;. A &lt;code&gt;doctest&lt;/code&gt; is a portion of the &lt;code&gt;docstring&lt;/code&gt; that begins with &lt;code&gt;&gt;&gt;&gt;&lt;/code&gt;. This code can be run using the &lt;code&gt;doctest&lt;/code&gt; module, which then checks to make sure that the result of the code is the same as the value in the line below the doctest string. In this case, the &lt;doctest&gt; would check to see that when we call &lt;code&gt;Q_mm(1, basin_area=1)&lt;/code&gt;, the function returns &lt;code&gt;86.4&lt;/code&gt; (the correct value). Doctests ensure that your code is doing what you want, and are a good way to check your functions.\n\nWe can run doctests for any function in a Jupyter notebook by importing the doctest module and then using the run_docstring_examples method:\nimport doctest\ndoctest.run_docstring_examples(Q_mm, globals(), verbose=True)\n\n‚úèÔ∏è &lt;b&gt; Try it. &lt;/b&gt; \nOnce you've written your &lt;code&gt;Q_mm&lt;/code&gt; function so it returns Q in mm/day, run the docstrings using the code above to make sure it works. \n\n\n\n4.1.2 Apply your Q_mm function to the Q dataframe to get a new column of Q in mm/day.\n\nüìö  &lt;b&gt; Question 4.1.2. &lt;/b&gt; \nUse the &lt;code&gt;apply&lt;/code&gt; method and your &lt;code&gt;Q_mm&lt;/code&gt; function to create a new column, &lt;code&gt;'Q [mm/day]'&lt;/code&gt; that has values of streamflow in mm/day. \n\n\n\n4.1.3 Create a plot of the daily change in storage over the three water years.\nNow that you have Q, P, and ET all in the same units (mm/day), you can determine the daily change in storage:\n$ S = P - ET - Q $\n\nüìö  &lt;b&gt; Question 4.1.3. &lt;/b&gt; \nCreate a new DataFrame with the change in storage and plot the data for the three water years.\n\n\n\n4.1.4 Determine average monthly water balance\nUse your measure of the change in storage to create a plot that contains the average monthly water balance across the three years of data. There will be one value of change in storage for each month of the water year.\n\nüìö  &lt;b&gt; Question 4.1.4. &lt;/b&gt; \nUse your measure of the change in storage to create a plot that contains the average monthly water balance across the three years of data. There will be one value of change in storage for each month of the water year.\n\n\n\n4.1.5 Assess the degree to which this basin is in steady state\nAs discussed above, under steady-state conditions the change in storage, \\Delta S, is zero and our equation reduces to:\n$ P = ET + Q$\n\nüìö  &lt;b&gt; Question 4.1.5. &lt;/b&gt; \nUse your three years of data to determine if this basin is in steady state over the entire time period.\n\n\n\n4.1.6 Determine Basin Runoff Coefficients\nThe runoff ratio or runoff coefficient, R_c, is defined as the fraction of rainfall that leaves a basin through streamflow over a period of time.\n$ R_{c} = $\nThe runoff coefficient is larger value for areas with low infiltration and high runoff (pavement, steep gradient), and lower for permeable, well vegetated areas (forest, flat land). The ratio is important for flood control, channel construction, and for possible flood zone hazard delineation. A high R_c value may indicate flash flooding areas during storms as water moves fast overland on its way to a river channel or a valley floor.\n\nüìö  &lt;b&gt; Question 4.1.6.&lt;/b&gt; \nCalculate the runoff ratio for the months with the 5 highest rainfalls during the three year record."
  },
  {
    "objectID": "resources/labs/Lab-1_Catchment_Water_Balance.html#choose-your-own-basin.",
    "href": "resources/labs/Lab-1_Catchment_Water_Balance.html#choose-your-own-basin.",
    "title": "Lab 1: Estimating Basin Waterbalance",
    "section": "4.2 Choose your own basin.",
    "text": "4.2 Choose your own basin.\nUse all of the example and assignment text to create an analysis of a USGS basin of your choosing. You can browse a map of all stream gauge locations in the US at the maps.waterdata.usgs.gov site. Select a basin that is near a place you‚Äôve lived, or a part of the country that you have visited or has some meaning to you.\nYour analysis should include:\n\nA plot of the basin hydrography\nA plot of the basin topography\nA plot of average rainfall, streamflow and runoff ratio by month\nA single year of waterbalance analysis. Note: Because obtaining ET data is slow, you should pick a single year between 2000 and 2018 to calculate your basin waterbalance. Use the get_ET_wateryear() function.\n\n\nSome Local Gauges of Possible Interest:\n\nCarpinteria Creek in Carpinteria, Station ID 11119500\n\ncarpinteria_id = '11119500' \n\nMission Creek in Santa Barbara, Station ID 11119750\n\nmission_id = '11119750'\n\nAtascadero Creek in Goleta, Station ID 11120000\n\natascadero_id = '11120000'"
  },
  {
    "objectID": "resources/labs/Labs.html",
    "href": "resources/labs/Labs.html",
    "title": "EDS 217: Python for Environmental Data Science",
    "section": "",
    "text": "Return to Course Home Page\n\n\n\n\nüè† index\nLab 1 Introduction Slides\nLab 1 - Catchment Water Balance\nLab 2 - Energy Balance"
  },
  {
    "objectID": "trypy/99_trypy_data_types/data_types_exercise.html#part-0.-setup-steps",
    "href": "trypy/99_trypy_data_types/data_types_exercise.html#part-0.-setup-steps",
    "title": "TryPy 2 - Data Types, Indexing, Imports and Plotting",
    "section": "Part 0. Setup Steps",
    "text": "Part 0. Setup Steps\n\nCreate a repo on GitHub named eds217-trypy-02\nClone to create a version-controlled project\nCreate some subfolder infrastructure (docs, data)\nCreate a new quarto in RStudio new .ipynb file called eds217-trypy-02.ipynb in VSCode. If working on your local machine, make sure to associate the notebook with the eds217_2023 environment."
  },
  {
    "objectID": "trypy/99_trypy_data_types/data_types_exercise.html#part-1.-checking-data-types",
    "href": "trypy/99_trypy_data_types/data_types_exercise.html#part-1.-checking-data-types",
    "title": "TryPy 2 - Data Types, Indexing, Imports and Plotting",
    "section": "Part 1. Checking data types",
    "text": "Part 1. Checking data types\n\nCreate some data, check the classes, index!\n\nVectors, lists & data frames\nIn your notebook (or quarto) document:\n\nCreate a list called vec_1containing the following:\n\n[2, 5, 9, 10, 8, 12, 1, 0]\nCheck the following for that list:\n\nWhat is the type of the list? type()\nAccess the 3rd element and store as vec_1_e3\nAccess the 1st element and store as vec_1_e1\nAccess the 5th through 7th elements and store as vec_1_e5to7\nReassign each element in vec_1 as a string and store the new list as vec_1_char. What does the output look like?\n\n\n\n&lt;class 'list'&gt;\n['2', '5', '9', '10', '8', '12', '1', '0']\n\n\n\nCreate a dictionary called dict_1\n\ndict_1 should contained named elements, where town = \"Santa Barbara, location = \"Rincon\", `swell = ‚Äúsouth‚Äù\n\nTake a look at what you‚Äôve made\nWhat is the type of dictionary values? type()\nWhat is the length of dict_1?\nAccess the ‚Äòlocation‚Äô value and store as dict_1_v2\n\n\nCreate a data frame in pandas\n\nWrite code to create a data frame called df_1 that looks like this:\n(Note: translate the R code below into python‚Ä¶ don‚Äôt forget to import pandas as pd)\n{r, echo = FALSE} df_1 &lt;- data.frame(     region = c(\"A\", \"B\", \"A\", \"D\"),     species = c(\"otter\", \"great white\", \"sea lion\", \"gray whale\"),     count = c(12, 2, 36, 6)     ) df_1 max_count &lt;- max(df_1$count)\n\nFind the maximum value of the count column, store as max_count\n\n\n\n36"
  },
  {
    "objectID": "trypy/99_trypy_data_types/data_types_exercise.html#part-2.-wild-data",
    "href": "trypy/99_trypy_data_types/data_types_exercise.html#part-2.-wild-data",
    "title": "TryPy 2 - Data Types, Indexing, Imports and Plotting",
    "section": "Part 2. Wild data",
    "text": "Part 2. Wild data\n\nSet-up\n\nThe first CSV listed (AS00601.csv) take a look at it (outside of python is fine as a first step, e.g.¬†you can open the CSV in Excel.)\nSave the AS00601.csv in the data folder of your repo.\n\n\n\nRead in the data\n\nRead in the data using pd.read_csv(), store as mack_verts\nLook at what you‚Äôve read in\n\n\n\nA bit of wrangling & exploring\n\nIn a new code chunk, practice accessing individual pieces of the data frame (there is no real functionality to this, but just to reinforce stuff we learned in our interactive session):\n\nStore the 5th value in column \"WEIGHT\" as mc_wt_5. Check by looking at your data frame to confirm.\nStore the 8th - 20th value in the \"LENGTH1\" column as mc_length_8_20. Check by looking at your data frame to confirm.\nStore everything in column SAMPLEDATE as a pd.Series called mc_dates\n\n\n\n\nMake a salamander subset\n\nCreate a subset that only contains observations for Pacific Giant Salamanders (species Dicamptodon tenebrosus, stored in SPECIES as DITE). Store the subset as mc_salamanders.\n\nHint: use a logical operator to filter for the rows you want.\nFor example, the following code block creates a new dataframe df_new containing only the rows in df_old where the value of column named ID is equal to Batman.\n    df_new = df_old[df_old['ID'] == 'Batman']\n\n\nMake a scatterplot of salamander length x weight\n\nCreate a scatterplot of LENGTH1 (snout-vent length in millimeters) versus WEIGHT (grams) for all salamanders in the subset you created above, mc_salamanders. Update axis labels, title, subtitle, and add a caption with the data source. Customize point color and size, possibly opacity, and the style.\n\n\n\n/Users/kellycaylor/mambaforge/envs/eds217_2023/lib/python3.10/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n\n\n\n\nExport your scatterplot as salamander_size.png to your figs folder using the fig.savefig() command.\n\n\n\nMake a cutthroat plot\n\nSimilar to above, make a subset called mc_trout that only contains observations for cutthroat trout (species ‚ÄúONCL‚Äù)\n\nNote: In the following, it is way easier to make use of the sns.relplot() command in the seaborn library than matplotlib\n\nCreate a scatterplot of \"LENGHTH1\" by \"WEIGHT\" for all trout in the dataset\nCustomize so that the point color depends on reach\n\nhue=\"REACH\" in sns.relplot()\n\nFacet your plot by creek reach\n\ncol=\"REACH\" in sns.relplot()\n\nstore the output of sns.relplot() as ax\nUpdate facet plot axis labels and title\n\nax.set_axis_labels(xlabel,ylabel)\nax.fig.suptitle(title)\n\nExport your graph as cutthroat_size.png to the figs folder (fig.savefig())\n\n\n\n/Users/kellycaylor/mambaforge/envs/eds217_2023/lib/python3.10/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;"
  },
  {
    "objectID": "trypy/00_trypy_stl_blood/stl_blood_exercise.html#part-0.-setup-steps",
    "href": "trypy/00_trypy_stl_blood/stl_blood_exercise.html#part-0.-setup-steps",
    "title": "TryPy 00 - Exploring St.¬†Louis Blood Toxicity Data",
    "section": "Part 0. Setup Steps",
    "text": "Part 0. Setup Steps\n\nCreate a repo on GitHub named eds217-trypy-00\nClone to create a version-controlled project\nCreate some subfolder infrastructure (nbs, data, figs)\nCreate and save a new quarto in RStudio called jupyter notebook (.ipynb file) named stl-lead-yourinitials.ipynb in the nbs folder (for example, mine would be stl-lead-kc.ipynb).\nMake sure to associate the notebook with the eds217_2023 environment.\n\n\nPart 1 - Get the data\n\n\"\"\"\nCreate a new variable containing \nthe link to the .csv file on \nthe EDS_221 github repository.\n\"\"\"\nurl = 'https://raw.githubusercontent.com/'\\\n'allisonhorst/EDS_221_programming-essentials/'\\\n'main/activities/stl_blood_lead.csv'\n\n\"\"\" \npandas can read a csv file into a \ndataframe directly from a url:\n\"\"\"\nstl_lead = pd.read_csv(url)\nRead more about the data here.\n\n\nPart 2 - Explore the data\nIn your .ipynb file:\n\nCreate a code cell that imports the numpy and pandas packages and run the cell to import the packages.\nUse the code above to read the url for stl_blood_lead.csv into a pandas DataFrame called stl_lead\nDo some basic exploration of the dataset using the DataFrame info and describe commands.\nIn a new code chunk, from stl_lead create a new column called prop_white that contains the percent of each census tract identifying as white (variable white in the dataset divided by variable totalPop, times 100).\n\nHint: df['new_col'] = df['col_a'] / df['col_b'] will create a new column new_col in the dataframe df that contains the value of col_a / col_b\n\n\nPart 3 - Create a scatterplot\n\nImport matplotlib (import matplotlib.pyplot as plt)\nCreate a scatterplot graph of the percentage of children in each census tract with elevated blood lead levels (pctElevated) versus the percent of each census tract identifying as white.\n\n\n\n\n\n\n\n\nPart 4 - Create a histogram\n\nCreate a histogram of only the pctElevated column in the data frame\nCustomize the fill, color, and size aesthetics - test some stuff! Feel free to make it awful."
  },
  {
    "objectID": "trypy/trypy.html#trypy-exercises",
    "href": "trypy/trypy.html#trypy-exercises",
    "title": "TryPy - Re-examining R exercises through Python",
    "section": "TryPy Exercises",
    "text": "TryPy Exercises\n\nSt.¬†Louis Lead data exercise (reading csv, dataframe calculations, simple plots)\n\n\nData types, indexing, import and plot data\n\n\nConditionals and Loops"
  },
  {
    "objectID": "trypy/02_trypy_conditional_and_loops/conditionals_and_loops.html#part-0.-setup-steps",
    "href": "trypy/02_trypy_conditional_and_loops/conditionals_and_loops.html#part-0.-setup-steps",
    "title": "TryPy 3 - For Loops and Conditionals",
    "section": "Part 0. Setup Steps",
    "text": "Part 0. Setup Steps\n\nCreate a repo on GitHub named eds217-trypy-03\nClone to create a version-controlled project\nCreate some subfolder infrastructure (docs, data)\nCreate a new python notebook.\nComplete all tasks for Part 1 in this .ipynb*"
  },
  {
    "objectID": "trypy/02_trypy_conditional_and_loops/conditionals_and_loops.html#part-1.-conditional-statements-for-loops",
    "href": "trypy/02_trypy_conditional_and_loops/conditionals_and_loops.html#part-1.-conditional-statements-for-loops",
    "title": "TryPy 3 - For Loops and Conditionals",
    "section": "Part 1. Conditional statements & for loops",
    "text": "Part 1. Conditional statements & for loops\nComplete each of the following in a separate code chunk.\n\nConditional statements\n\nTask 1\nCreate an object called pm2_5 with a value of 48 (representing Particulate Matter 2.5, an indicator for air quality, in \\frac{\\mu g}{m^3} (see more about PM2.5 here).\nWrite an if - else if - else statement that returns ‚ÄúLow to moderate risk‚Äù if pm2_5 (for Particulate Matter 2.5) is less than 100, ‚ÄúUnhealthy for sensitive groups‚Äù if PM 2.5 is 100 &lt;= pm2_5 &lt; 150, and ‚ÄúHealth risk present‚Äù if PM 2.5 is &gt;= 150.\nTest by changing the value of your pm2_5 object and re-running your statement to check.\n\n\nTask 2\nStore the string ‚Äúblue whale‚Äù as an object called species.\nWrite an if statement that returns ‚ÄúYou found a whale!‚Äù if the string ‚Äúwhale‚Äù is detected in species, otherwise return nothing.\nTest by changing the species string & re-running to see output.\n\n\nTask 3\nCreate a vector stored as max_airtemp_c with a value of 24.1.\nWrite an if else statement that will print ‚ÄúTemperature too high‚Äù if max_airtemp_c is greater than 27, or ‚ÄúTemperature OK‚Äù if temperature is less than or equal to 27.\n\n\nTask 4\nStore the base price of a burrito as base_burrito with a value of 6.50. Store main_ingredent with a starting string of ‚Äúveggie.‚Äù\nWrite a statement that will return the price of a burrito based on what a user specifies as ‚Äúmain_ingredient‚Äù (either ‚Äúveggie‚Äù, ‚Äúchicken‚Äù or ‚Äústeak‚Äù) given the following:\n\nA veggie burrito is the cost of a base burrito\nA chicken burrito costs 3.00 more than a base burrito\nA steak burrito costs 3.25 more than a base burrito\n\n\n\n\nFor loops\nComplete each of the following in a separate code chunk.\n\nTask 5\nCreate a new vector called fish that contains the values 8, 10, 12, 23 representing counts of different fish types in a fish tank (goldfish, tetras, guppies, and mollies, respectively).\nWrite a for loop that iterates through fish, and returns what proportion of total fish in the tank are that species.\nAssume that these counts represent all fish in the tank.\n\n\nTask 6\nPython has a list of month names stored in the calendar library (part of the standard library in Python). You can load this list using from calendar import month_name. These items are stored so that ‚ÄúJanuary‚Äù is in month_name[1], meaning this is one of the rare arrays in python that is not zero-indexed.\nWrite a for loop that iterates over all months in month_name and prints ‚ÄúJanuary is month 1,‚Äù ‚ÄúFebruary is month 2‚Äù, etc.\nHint: you can index values in the month_name vector just like you would any other vector (e.g., try running month_name[5])."
  },
  {
    "objectID": "trypy/02_trypy_conditional_and_loops/conditionals_and_loops.html#part-2.-real-data",
    "href": "trypy/02_trypy_conditional_and_loops/conditionals_and_loops.html#part-2.-real-data",
    "title": "TryPy 3 - For Loops and Conditionals",
    "section": "Part 2. Real data",
    "text": "Part 2. Real data\nYou will complete Part 3 in a separate notebook\nExplore this data package from EDI, which contains a ‚ÄúData file describing the biogeochemistry of samples collected at various sites near Toolik Lake, North Slope of Alaska‚Äù. Familiarize yourself with the metadata (particularly, View full metadata &gt; expand ‚ÄòData entities‚Äô to learn more about the variables in the dataset).\nCitation: Kling, G. 2016. Biogeochemistry data set for soil waters, streams, and lakes near Toolik on the North Slope of Alaska, 2011. ver 5. Environmental Data Initiative. https://doi.org/10.6073/pasta/362c8eeac5cad9a45288cf1b0d617ba7\n\nDownload the CSV containing the Toolik biogeochemistry data\nTake a look at it - how are missing values stored? Keep that in mind.\nDrop the CSV into your data folder of your project\nCreate a new qmd document, save in docs as toolik_chem.ipynb\nImport the pandas and janitor package in your setup code chunk.\nRead in the data as toolik_biochem. Remember, you‚Äôll want to specify here how NA values are stored. Use the clean_names() function to convert all column names to lower case/underscore format.\nCreate a subset of the data that contains only observations from the ‚ÄúToolik Inlet‚Äù site, and that only contains the variables (columns) for pH, dissolved organic carbon (DOC), and total dissolved nitrogen (TDN). Store this subset as inlet_biochem. Make sure to LOOK AT the subset you‚Äôve created.\nFind the mean value of each column in inlet_biochem 2 different ways:\n\n\nWrite a for loop from scratch to calculate the mean for each\nUse one other method (e.g.¬†.mean(), or .apply()) to find the mean for each column.\n\n\nSave, stage, commit, pull, push!"
  },
  {
    "objectID": "trypy/02_trypy_conditional_and_loops/conditionals_and_loops.html#end-activities",
    "href": "trypy/02_trypy_conditional_and_loops/conditionals_and_loops.html#end-activities",
    "title": "TryPy 3 - For Loops and Conditionals",
    "section": "END activities",
    "text": "END activities"
  },
  {
    "objectID": "interactive_sessions/3-1_numpy.html",
    "href": "interactive_sessions/3-1_numpy.html",
    "title": "Session 3-1: Introduction to NumPy",
    "section": "",
    "text": "‚¨ÖÔ∏è Previous Session | üè† Course Home | ‚û°Ô∏è Next Session |\nHaving covered the basics of Python, we will now explore its applications for data science. Bypassing the hype, data science is an interdisciplinary subject that lies at the intersection of statistics, computer programming, and domain expertise. It is best to think of data science not as a new field of knowledge itself, but rather as a set of skills for analysing and interrogating datasets within your existing area of expertise ‚Äì¬†in our case, environmental science and management.\nPython‚Äôs extensive, active ‚Äúecosystem‚Äù of packages like NumPy, Pandas, SciPy, and Matplotlib ‚Äì¬†all of which we will explore in this next set of sessions ‚Äì¬†lends itself well to data analysis and scientific computing. In addition, this section outlines techniques for importing, manipulating, visualizing, and exporting data in Python.\nWhile data come in a wide variety of formats, it is useful to conceptualize all data as arrays of numbers (recall the spreadsheet analogy from Session 1-4). For example, an image is, at its core, a two-dimensional array of numbers representing the brightness of each pixel across the image area. When envisioned this way, it is easy to see how the image can be transformed and analysed by manipulating values in the array:"
  },
  {
    "objectID": "interactive_sessions/3-1_numpy.html#introduction-to-numpy",
    "href": "interactive_sessions/3-1_numpy.html#introduction-to-numpy",
    "title": "Session 3-1: Introduction to NumPy",
    "section": "Introduction to NumPy",
    "text": "Introduction to NumPy\n\nNumPy, an abbreviation for Numerical Python, is the core library for scientific computing in Python. In addition to manipulation of array-based data, NumPy provides an efficient way to store and operate on very large datasets. In fact, nearly all Python packages for data storage and computation are built on NumPy arrays.\nThis exercise will provide an overview of NumPy, including how arrays are created, NumPy functions to operate on arrays, and array math. While most of the basics of the NumPy package will be covered here, there are many, many more operations, functions, and modules. As always, you should consult the NumPy Docs to explore its additional functionality.\nBefore jumping into NumPy, we should take a brief detour through importing libraries in Python. While most packages we will use ‚Äì including NumPy ‚Äì¬†are developed by third-parties, there are a number of ‚Äústandard‚Äù packages that are built into the Python API. The following table contains a description of a few of the most useful modules worth making note of.\n\n\n\n\n\n\n\n\nModule\nDescription\nSyntax\n\n\n\n\n os \nProvides access to operating system functionality\n import os \n\n\n math \nProvides access to basic mathematical functions\n import math \n\n\n random \nImplements pseudo-random number generators for various distributions\n import random \n\n\n datetime \nSupplies classes for generating and manipulating dates and times\n import datetime as dt \n\n\n\n\nüêç &lt;b&gt;Import syntax.&lt;/b&gt; \nAs we've seen already, modules and packages can be loaded into a script using an &lt;code&gt;import&lt;/code&gt; statement: &lt;code&gt;import [module]&lt;/code&gt; for the entire module, or &lt;code&gt;from [module] import [identifier]&lt;/code&gt; to import a certain class of the module. All modules and packages used in a program should be imported at the beginning of the program.\nMany packages are imported with standard abbreviations (such as dt for the datetime module) using the following syntax:\n\nimport [module] as [name]\n\nThe standard syntax for importing NumPy is:\n\nimport numpy as np\n\n\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nimport numpy as np\n\n\n\nNumPy Arrays\n\n\nThe n-dimensional array object in NumPy is referred to as an ndarray, a multidimensional container of homogeneous items ‚Äì¬†i.e.¬†all values in the array are the same type and size. These arrays can be one-dimensional (one row or column vector), two-dimensional (m rows x n columns), or three-dimensional (arrays within arrays).\n\n Constructing arrays from lists \n\nThere are two main ways to construct NumPy arrays. The first involves using the np.array() function to generate an array from one or more lists:\nnp.array([8,0,9,1,4])\n&gt;&gt;&gt; array([8, 0, 9, 1, 4])\nRecall that unlike lists, all elements within an array must be of the same type. If the types do not match, NumPy will ‚Äúupcast‚Äù if possible (e.g.¬†convert integers to floats):\nnp.array([8.14,0.12,9,1.77,4])\n&gt;&gt;&gt; array([8.14, 0.12, 9.  , 1.77, 4.  ])\nIn these examples, we have created one-dimensional arrays. By default, elements in a one-dimensional array are cast as rows in a column (i.e.¬†a column vector). If, however, we wanted a row vector instead, we could use double brackets [[]] to create an array with one row and multiple columns:\nnp.array([[8,0,9,1,4]]) # row vector with 5 columns\n&gt;&gt;&gt; array([[8, 0, 9, 1, 4]])\nThis is because NumPy treats the inner element(s) or list(s) as rows. This is easier to see with a multidimensional array:\nnp.array([[3,2,0,1],[9,1,8,7],[4,0,1,6]]) # array with 3 rows x 4 columns\n\n&gt;&gt;&gt; array([[3, 2, 0, 1],\n           [9, 1, 8, 7],\n           [4, 0, 1, 6]])"
  },
  {
    "objectID": "interactive_sessions/3-1_numpy.html#practice-1.",
    "href": "interactive_sessions/3-1_numpy.html#practice-1.",
    "title": "Session 3-1: Introduction to NumPy",
    "section": "üìö Practice 1.",
    "text": "üìö Practice 1.\nCreate the following arrays and assign the corresponding variable names:\n\na\n\n\\begin{bmatrix}\n     4 & 5 & 0 & 12 & -1 \\\\\n     8 & -21 & -4 & 6 & 3 \\\\\n     17 & 1 & -13 & 7 & 0\n\\end{bmatrix}\n\nb\n\n\\begin{bmatrix}\n     1.0 & 2.7 & 0 & 0.188 & 4.07 & 0.24\n\\end{bmatrix}\n\nc\n\n\\begin{bmatrix}\n     0.4 \\\\\n     0.8 \\\\\n     1.2 \\\\\n     1.6 \\\\\n     2.0 \\\\\n     2.4\n\\end{bmatrix}\n\n\n\n Constructing arrays using functions \n\nOftentimes, it will be more efficient to construct arrays from scratch using NumPy functions. The np.arange() function is used to generate an array with evenly spaced values within a given interval. np.arange() can be used with one, two, or three parameters to specify the start, stop, and step values. If only one value is passed to the function, it will be interpreted as the stop value:\n# Create an array of the first seven integers \nnp.arange(7)\n&gt;&gt;&gt; array([0, 1, 2, 3, 4, 5, 6])\n\n# Create an array of floats from 1 to 12\nnp.arange(1.,13.)\n&gt;&gt;&gt; array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.])\n\n# Create an array of values between 0 and 20, stepping by 2\nnp.arange(0,20,2)\n&gt;&gt;&gt; array([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])\nSimilarly, the np.linspace() function is used to construct an array with evenly spaced numbers over a given interval. However, instead of the step parameter, np.linspace() takes a num parameter to specify the number of samples within the given interval:\n# Create an array of 5 evenly spaced values between 0 and 1\nnp.linspace(0,1,5)\n&gt;&gt;&gt; array([0.  , 0.25, 0.5 , 0.75, 1.  ])\nNote that unlike np.arange(), np.linspace() includes the stop value by default (this can be changed by passing endpoint=True). Finally, it should be noted that while we could have used np.arange() to generate the same array in the above example, it is recommended to use np.linspace() when a non-integer step (e.g.¬†0.25) is desired.\n\nüìö  &lt;b&gt; Practice 2. &lt;/b&gt; \n\n\nCreate a new array d of integers the multiples of 3 between 0 and 100.\n\n\nCreate an array f 10 evenly spaced elements between 0 and 2.\n\n\nRe-create array c from Practice 1c using a function. Assign this to variable name g.\n\n\n\nThere are several functions that take a shape argument to generate single-value arrays with specified dimensions passed as a tuple (rows,columns):\n# Create a 1D array of zeros of length 4\nnp.zeros(4)\n&gt;&gt;&gt; array([0., 0., 0., 0.]\n\n# Create a 4 x 3 array filled with zeros\nnp.zeros((4,3))\n&gt;&gt;&gt; array([[0., 0., 0.],\n           [0., 0., 0.],\n           [0., 0., 0.],\n           [0., 0., 0.]])\n\n# Create a 4 x 3 array filled with ones\nnp.ones((4,3))\n&gt;&gt;&gt; array([[1., 1., 1.],\n           [1., 1., 1.],\n           [1., 1., 1.],\n           [1., 1., 1.]])\n\n\n# Create a 4 x 3 array filled with 3.14\nnp.full((4,3),9.87)\n&gt;&gt;&gt; array([[9.87, 9.87, 9.87],\n           [9.87, 9.87, 9.87],\n           [9.87, 9.87, 9.87],\n           [9.87, 9.87, 9.87]])\nThe np.random.rand() function is used to generate n-dimensional arrays filled with random numbers between 0 and 1:\n# Create a 4 x 3 array of uniformly distributed random values\nnp.random.rand(4,3)\n&gt;&gt;&gt; array([[0.17461878, 0.74586348, 0.9770975 ],\n           [0.77861373, 0.28807114, 0.10639001],\n           [0.09845499, 0.36038089, 0.58533369],\n           [0.30983962, 0.74786381, 0.27765305]])\nAs we will see, the np.random.rand() function is very useful for sampling and modeling.\nThe last array-construction function we will consider (but by no means the last in the NumPy API!) is the np.eye() function, which is used to generate the two-dimensional identity matrix:\n# Create a 4 x 4 identity matrix\nnp.eye(4)\n&gt;&gt;&gt; array([[1., 0., 0., 0.],\n           [0., 1., 0., 0.],\n           [0., 0., 1., 0.],\n           [0., 0., 0., 1.]])\nLastly, it‚Äôs worth noting that nearly all of these functions contain an optional dtype parameter, which can be used to specify the data-type of the resulting array (e.g.¬†np.ones((4,3),dtype=int) would return a 4 x 3 array of ones as integers, rather than the default floats)."
  },
  {
    "objectID": "interactive_sessions/3-1_numpy.html#practice-3.-assign-the-following-to-variables",
    "href": "interactive_sessions/3-1_numpy.html#practice-3.-assign-the-following-to-variables",
    "title": "Session 3-1: Introduction to NumPy",
    "section": "üìö  Practice 3.  Assign the following to variables:",
    "text": "üìö  Practice 3.  Assign the following to variables:\n\n\nA 5x3 array of ones.\n\n\nA one-dimensional array of 6 zeros.\n\n\nA 7x7 identity array.\n\n\nA random 10x10 array.\n\n\n\nArray Manipulation\n\n\nHaving established how to construct arrays in NumPy, let‚Äôs explore some of the attributes of the ndarray, including how to manipulate arrays. Nearly all data manipulation in Python involves NumPy array manipulation; many other Python data tools like Pandas (Session 2-2) are built on the NumPy array. Thus, while many of the examples below may seem trivial, understanding these operations will be critical to understanding more complex operations and Python data manipulation more broadly.\n\n Array attributes \n\nArray attributes are properties that are intrinsic to the array itself. While there are quite a few attributes of NumPy arrays, the ones we will use most often provide information about the size, shape, and type of the arrays:\n\n\n\n\n\n\n\nMethod\nDescription\n\n\n\n\n ndarray.ndim \nNumber of array dimensions\n\n\n ndarray.shape \nTuple of array dimensions (rows, columns)\n\n\n ndarray.size \nTotal number of elements in the array\n\n\n ndarray.dtype \nData-type of array elements\n\n\n\nFor example, let‚Äôs create a random two-dimensional array and explore its attributes using the above methods.\n# Initialize array\na = np.random.rand(4,7)\n\n# Determine array dimensions\na.ndim\n&gt;&gt;&gt; 2\n\n# Determine array shame\na.shape\n&gt;&gt;&gt; (4, 7)\n\n# Determine array size\na.size\n&gt;&gt;&gt; 28\n\n# Determine data-type\na.dtype\n&gt;&gt;&gt; dtype('float64')"
  },
  {
    "objectID": "interactive_sessions/3-1_numpy.html#try-it.",
    "href": "interactive_sessions/3-1_numpy.html#try-it.",
    "title": "Session 3-1: Introduction to NumPy",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nConstruct two array vectors, a column vector and a row vector, from the list [8,0,9,1,4], as in the first example. Using the ndarray.ndim and ndarray.shape methods, show the difference between constructing an array with single vs.¬†double brackets."
  },
  {
    "objectID": "interactive_sessions/3-1_numpy.html#practice-4.-use-array-methods-and-the-array-you-created-in-practice-2a-d-to-count-the-number-of-multiples-of-3-between-0-and-100.",
    "href": "interactive_sessions/3-1_numpy.html#practice-4.-use-array-methods-and-the-array-you-created-in-practice-2a-d-to-count-the-number-of-multiples-of-3-between-0-and-100.",
    "title": "Session 3-1: Introduction to NumPy",
    "section": "üìö  Practice 4.  Use array methods and the array you created in Practice 2a (d) to count the number of multiples of 3 between 0 and 100.",
    "text": "üìö  Practice 4.  Use array methods and the array you created in Practice 2a (d) to count the number of multiples of 3 between 0 and 100.\n\n Indexing + slicing \n\nIndexing arrays is analogous to indexing lists:\n# Initialize a one-dimensional array\nx1 = np.array([8,0,9,1,4])\n\n# Return the value in position 1\nx1[1]\n&gt;&gt;&gt; 0\nWith multidimensional arrays, a tuple of indices can be passed to access the rows and columns of an array: ndarray[row,column]. If a single index is passed, the corresponding row element will be returned:\n# Initialize a two-dimensional array\nx2 = np.array([[3,2,0,1],\n               [9,1,8,7],\n               [4,0,1,6]])\n\n# Return the value of the element in the 2nd row, 3rd column\nx2[1,2]\n&gt;&gt;&gt; 8 \n\n# Return the entire second row\nx2[1]\n&gt;&gt;&gt; array([9, 1, 8, 7])\nSlicing of arrays allows you to access parts of arrays or subarrays. Just like with lists, slicing follows the syntax ndarray[start:stop:step].\n# Return the elements in positions 1-4\nx1[1:]\n&gt;&gt;&gt; array([0, 9, 1, 4])\nFor multidimensional arrays, a tuple of slices is used: ndarray[row_start:row_end:row_step, col_start:col_end:col_step].\n# Return the entire third column\nx2[:,2]\n&gt;&gt;&gt; array([0, 8, 1])\n\n# Return the first two rows and two columns\nx2[:2,:2]\n&gt;&gt;&gt; array([[3, 2],\n           [9, 1]])\n\n# Return all rows and every other column\nx2[:,::2]\n&gt;&gt;&gt; array([[3, 0],\n           [9, 8],\n           [4, 1]])"
  },
  {
    "objectID": "interactive_sessions/3-1_numpy.html#practice-5.",
    "href": "interactive_sessions/3-1_numpy.html#practice-5.",
    "title": "Session 3-1: Introduction to NumPy",
    "section": "üìö  Practice 5. ",
    "text": "üìö  Practice 5. \nUsing the array you created in Practice 3d,\n\n\nPrint all the elements in column 4.\n\n\nPrint all the elements in row 7.\n\n\nExtract the 4x4 subarray at the center of the array and assign it as a new variable.\n\n\nPrint the last two values in column 10."
  },
  {
    "objectID": "interactive_sessions/3-1_numpy.html#practice-6.",
    "href": "interactive_sessions/3-1_numpy.html#practice-6.",
    "title": "Session 3-1: Introduction to NumPy",
    "section": "üìö  Practice 6. ",
    "text": "üìö  Practice 6. \nCreate a blank 8x8 matrix and fill it with a checkerboard pattern of 0s and 1s using indexing.\n\n\nCode\n&lt;h4 style=\"border:1px; border-style:solid; border-color:black; padding: 0.5em;\"&gt; &lt;span style=\"color:black\"&gt; Array reduction &lt;/span&gt; &lt;/h4&gt;\n\n**Array reduction** refers to the computation of summary statistics on an array ‚Äì i.e. *reducing* an array to a single aggregate value, such as the mean, minimum, maximum, etc. These array reduction methods are similar to those used for lists:\n\n```python\nx2 = np.array([[3,2,0,1],\n               [9,1,8,7],\n               [4,0,1,6]])\n\n# Sum of all values in array\nx2.sum()\n&gt;&gt;&gt; 42\n\n# Maximum value of the array\nx2.max()\n&gt;&gt;&gt; 9\n\n# Minimum value of the array\nx2.min()\n&gt;&gt;&gt; 0\n\n# Mean value of the array\nx2.mean()\n&gt;&gt;&gt; 3.5\n\n# Standard deviation of the array\nx2.std()\n&gt;&gt;&gt; 3.095695936834452\n\n```\n\nAll of these methods can be passed with an *`axis`* argument, which allows for aggregation across the rows or columns of the array. In NumPy ‚Äì as well as the many libraries built on NumPy, axis `0` always refers to the *rows* of an array, while axis `1` refers to the *columns*:\n\n```python\n# Mean of each row (calculated across columns)\nx2.mean(axis=1)\n&gt;&gt;&gt; array([1.5 , 6.25, 2.75])\n\n# Maximum value of each column (calculated across rows)\nx2.max(axis=0)\n&gt;&gt;&gt; array([9, 2, 8, 7])\n```\n\n\n\nüêç &lt;b&gt;Functions vs. Methods.&lt;/b&gt; \nAs we'll explore later in this course, &lt;i&gt;functions&lt;/i&gt; and &lt;i&gt;methods&lt;/i&gt; in Python are essentially the same thing. The key difference, however, is that functions can be called generically, while methods are always attached to and called on objects. It is also worth noting that while a method may alter the object itself, a function &lt;i&gt;usually&lt;/i&gt; simply operates on an object without changing it, and then prints something or returns a value.\nFor each of the array reduction methods demonstrated above, there is a corresponding function. For example, the mean of an array can be calculated using the method ndarray.mean() or the function np.mean(ndarray).\nThese ‚Äì¬†and the many additional ‚Äì aggregation functions in NumPy can be used, not only on arrays, but on any numerical object.\n\n\n Reshaping, resizing, + rearranging arrays \n\nOther useful array operations include reshaping, resizing, and rearranging arrays. The ndarray.reshape() method is used to change the shape of an array:\n# Initialize a one-dimensional array with 16 elements\na = np.arange(1.0,17.0)\n\na\n&gt;&gt;&gt; array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\n       14., 15., 16.])\n\n# Reshape array a into a 4x4 array\nb = a.reshape(4,4)\n\nb\n&gt;&gt;&gt; [[ 1.  2.  3.  4.]\n     [ 5.  6.  7.  8.]\n     [ 9. 10. 11. 12.]\n     [13. 14. 15. 16.]]\n\nThere are a few important things to note about the ndarray.reshape() method. First and unsurprisingly, the size of array must be preserved (i.e.¬†the size of the reshaped array must match that of the original array). Secondly, and perhaps more importantly, the ndarray.reshape() method creates a view of the original array a, rather than a copy, which would allow the two variables to exist independently. Because b is a view of a, any changes made to b will also be applied to a:\n# Reset the value in the third row, third column (11.0)\nb[2,2] = 0.0\n\nb\n&gt;&gt;&gt; array([[ 1.,  2.,  3.,  4.],\n           [ 5.,  6.,  7.,  8.],\n           [ 9., 10.,  0., 12.],\n           [13., 14., 15., 16.]])\n\na\n&gt;&gt;&gt; array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.,  0., 12., 13., \n           14., 15., 16.])\nUnlike ndarray.reshape(), the ndarray.resize() method operates in-place on the original array. The ndarray.resize() method is used to add or delete rows and/or columns:\n# Initialize a 2 x 3 array\na = np.array([[1,2,3],[4,5,6]])\n\n# Copy the original array\nsmaller = a.copy()\n# Use ndarray.resize() to reshape to a 2x2 array and delete the last two elements\nsmaller.resize(2,2)\n\nsmaller\n&gt;&gt;&gt; array([[1, 2],\n           [3, 4]])\n\n# Copy the original array\nbigger = a.copy()\n# Use ndarray.resize() to reshape to a 6x6 array by adding zeros\nbigger.resize(6,6)\n\nbigger\n&gt;&gt;&gt; array([[1, 2, 3, 4, 5, 6],\n           [0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0]])\n\nüêç &lt;b&gt;Copies vs. Views&lt;/b&gt; \nThis is just one example of many occasions when it is advisable to create a &lt;b&gt;copy&lt;/b&gt; of the original object before manipulating it. Had we not copied &lt;code&gt;a&lt;/code&gt; before resizing it to a 2x2 array, the last two elements would have been permanently deleted, as &lt;code&gt;a&lt;/code&gt; itself would have been resized. A good rule of thumb is to &lt;b&gt;always create a copy&lt;/b&gt; before changing or deleting any data.\n\nOften it is useful to rearrange the elements in an array. The ndarray.transpose() method ‚Äì¬†or simply ndarray.T, transposes the array, switching the rows and columns, while the np.flip(), np.flipud(), and np.fliplr() functions reverse the order of elements in the array along a given axis:\n# Initialize a new 4x5 array\nx = np.array([[4, 2, 0, 1, 5],\n              [9, 4, 1, 3, 0],\n              [6, 0, 8, 5, 9],\n              [7, 3, 2, 7, 4]])\n\n# Transpose rows + columns\nx.T\n&gt;&gt;&gt; array([[4, 9, 6, 7],\n           [2, 4, 0, 3],\n           [0, 1, 8, 2],\n           [1, 3, 5, 7],\n           [5, 0, 9, 4]])\n\n# Flip the array (reverse the order of all elements)\nnp.flip(x)\n&gt;&gt;&gt; array([[4, 7, 2, 3, 7],\n           [9, 5, 8, 0, 6],\n           [0, 3, 1, 4, 9],\n           [5, 1, 0, 2, 4]])\n\n# Flip the array up/down (reverse the order of the rows)\nnp.flipud(x)\n&gt;&gt;&gt; array([[7, 3, 2, 7, 4],\n           [6, 0, 8, 5, 9],\n           [9, 4, 1, 3, 0],\n           [4, 2, 0, 1, 5]])\n\n# Flip the array left/right (reverse the order of the columns)\nnp.fliplr(x)\n&gt;&gt;&gt; array([[5, 1, 0, 2, 4],\n           [0, 3, 1, 4, 9],\n           [9, 5, 8, 0, 6],\n           [4, 7, 2, 3, 7]])\nWhen passed with the axis argument, np.flip() mimics the np.flipud() and np.fliplr() functions:\n# Flip the array over the row axis (same as np.flipud(x))\nnp.flip(x, axis=0)\n&gt;&gt;&gt; array([[7, 3, 2, 7, 4],\n           [6, 0, 8, 5, 9],\n           [9, 4, 1, 3, 0],\n           [4, 2, 0, 1, 5]])\n\n# Flip the array over the column axis (same as np.fliplr(x))\nnp.flip(x, axis=1)\n&gt;&gt;&gt; array([[5, 1, 0, 2, 4],\n           [0, 3, 1, 4, 9],\n           [9, 5, 8, 0, 6],\n           [4, 7, 2, 3, 7]])"
  },
  {
    "objectID": "interactive_sessions/3-1_numpy.html#practice-7.",
    "href": "interactive_sessions/3-1_numpy.html#practice-7.",
    "title": "Session 3-1: Introduction to NumPy",
    "section": "üìö  Practice 7. ",
    "text": "üìö  Practice 7. \n\n\nCreate a 3x3 matrix with values ranging from 0 to 8.\n\n\nReverse the order of elements in your random 10x10 array from excercise 3d.\n\n\n\n Joining + splitting arrays \n\nSo far, we have considered array manipulation routines that operatee on a single array. We will encounter many scenarios in which it is necessary to combine multiple arrays into one or, conversely, to split a single array into two or more separate objects.\nConcatenation in computer programming refers to the process of joining multiple objects end-to-end. The most common way of concatenating arrays in NumPy is with the np.concatenate() function, which takes a tuple of arrays:\n# Initialize a 3x3 array\nx = np.array([[4,2,0],\n              [9,4,1],\n              [6,0,8]])\n# Initialize a 1x3 array\ny = np.array([[2,8,6]])\n\n# Concatenate x and y\nnp.concatenate((x,y))\n&gt;&gt;&gt; array([[4, 2, 0],\n           [9, 4, 1],\n           [6, 0, 8],\n           [2, 8, 6]])\nNote that, by default, np.concatenate() operates along the row axis (0). To concatenate along the column axis, we must specify axis=1 as an argument:\n# Concatenate x and y along the column axis\nnp.concatenate((x,y), axis=1)\n&gt;&gt;&gt; ---------------------------------------------------------------------------\n    ValueError                                Traceback (most recent call last)\n    &lt;ipython-input-65-6c2205ef28d2&gt; in &lt;module&gt;\n          5 y = np.array([[2,8,6,0]])\n          6 \n    ----&gt; 7 np.concatenate((x,y),axis=1)\n\n    &lt;__array_function__ internals&gt; in concatenate(*args, **kwargs)\n\n    ValueError: all the input array dimensions for the concatenation axis must match exactly, but along \n    dimension 0, the array at index 0 has size 3 and the array at index 1 has size 1\nUh-oh! Unsurprisingly, when we tried to concatenate an array with 1 row to an array with 3 rows, we got a ValueError. For np.concatenate() to work, the dimensions must match. Thus, we must first transpose y before adding it to x as a column:\n# Transpose y and concatenate x and y along the column axis\nnp.concatenate((x,y.T),axis=1)\n&gt;&gt;&gt; array([[4, 2, 0, 2],\n           [9, 4, 1, 8],\n           [6, 0, 8, 6]])\nEquivalently, we could use the np.vstack() or np.hstack() function to concatenate directly along the row or column axis, respectively:\n# Stack rows of x and y (same as np.concatenate((x,y), axis=0)\nnp.vstack((x,y))\n&gt;&gt;&gt; array([[4, 2, 0],\n           [9, 4, 1],\n           [6, 0, 8],\n           [2, 8, 6]])\n\n# Stack columns of x and y (same as np.concatenate((x,y), axis=1)\nnp.hstack((x,y.T))\n&gt;&gt;&gt; array([[4, 2, 0, 2],\n           [9, 4, 1, 8],\n           [6, 0, 8, 6]])"
  },
  {
    "objectID": "interactive_sessions/3-1_numpy.html#practice-8.",
    "href": "interactive_sessions/3-1_numpy.html#practice-8.",
    "title": "Session 3-1: Introduction to NumPy",
    "section": "üìö  Practice 8. ",
    "text": "üìö  Practice 8. \nCreate two random 1-D arrays of length 10. Merge them into a 2x10 array and then a 10x2 array.\nConversely, splitting allows you to breakdown a single array into multiple arrays. Splitting is implemented with the np.split(), np.vsplit(), and np.hsplit() functions.\n# Initialize a 4x3 array\nz = np.array([[4, 2, 0],\n              [9, 4, 1],\n              [6, 0, 8],\n              [2, 8, 6]])\n\n# Split z into two arrays at row 1\nnp.split(z,[1])\n&gt;&gt;&gt; [array([[4, 2, 0]]), array([[9, 4, 1],\n                                [6, 0, 8],\n                                [2, 8, 6]])]\n\n# OR\nnp.vsplit(z,[1])\n&gt;&gt;&gt; [array([[4, 2, 0]]), array([[9, 4, 1],\n                                [6, 0, 8],\n                                [2, 8, 6]])]\n\n# Split z into two arrays at column 1\nnp.hsplit(z,[1])\n&gt;&gt;&gt; [array([[4],\n            [9],\n            [6],\n            [2]]), \n     array([[2, 0],\n            [4, 1],\n            [0, 8],\n            [8, 6]])]\nMultiple indices can be passed to the np.split() and related functions, with n indices (split points) resulting in n + 1 subarrays."
  },
  {
    "objectID": "interactive_sessions/3-1_numpy.html#practice-9.",
    "href": "interactive_sessions/3-1_numpy.html#practice-9.",
    "title": "Session 3-1: Introduction to NumPy",
    "section": "üìö  Practice 9. ",
    "text": "üìö  Practice 9. \n\n\nSplit your random 10x10 array from 3d into two 10x5 arrays.\n\n\nCombine the first 10x5 array from (a), the 10x2 array from 8b, and the other 10x5 array from (a). In other words, recombine the 10x10 array from 3d with two new columns in index positions 5 and 6. Your final array should have 10 rows and 12 columns. Verify this by printing the shape of the resulting array.\n\n\n\nArray Math\n\n\nOne of the key advantages of NumPy is its ability to perform vectorized operations using universal functions (ufuncs), which perform element-wise operations on arrays very quickly. For example, say we had a very large list of data, and we wanted to perform some mathematical operation on all of the data elements. We could store this data as a list or an ndarray:\n# Create a list of the first 10,000 integers\na = list(range(10000))\n\n# Create a one-dimensional array of the first 10,000 integers\nb = np.arange(10000)\nNow, let‚Äôs multiply each element in our dataset by 2. We can accomplish this by using a for loop for the list a and a ufunc for array b. (The %timeit module is a built-in Python function used to calculate the time it takes to execute short code snippets.)\n# Use a for loop to multiply every element in a by 2\n%timeit [i*2 for i in a]\n# Use a ufunc to multiply every element in b by 2\n%timeit b * 2\n\n&gt;&gt;&gt; 388 ¬µs ¬± 30.6 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1000 loops each)\n    3.58 ¬µs ¬± 41.1 ns per loop (mean ¬± std. dev. of 7 runs, 100000 loops each)\nThe %timeit module is a built-in Python function used to calculate the time it takes to execute short code snippets.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Create a list of the first 10,000 integers\nlist10 = list(range(10000))\n# Use a for loop to multiply every element in a by 2\n%timeit [i*2 for i in list10]\n\n# Create a one-dimensional array of the first 10,000 integers\narray10 = np.arange(10000)\n# Use a ufunc to multiply every element in b by 2\n%timeit array10 * 2\n\n\n361 ¬µs ¬± 2.25 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n4.18 ¬µs ¬± 33.5 ns per loop (mean ¬± std. dev. of 7 runs, 100,000 loops each)\n\n\nAs you can see, the for loop took about 100 times longer than the exact same element-wise array operation!\n\n Computation on single arrays using ufuncs \n\nUfuncs are fairly straightforward to use, as they rely on Python‚Äôs native operators (e.g.¬†+, -, *, /):\n# Create a 2x4 array of floats\nx  = np.array([[1.,2.,3.,4.],\n               [5.,6.,7.,8.]])\n\n# Do some math\n# Addition\nx + 12\n&gt;&gt;&gt; array([[13., 14., 15., 16.],\n           [17., 18., 19., 20.]])\n\n# Subtraction\nx - 400\n&gt;&gt;&gt; array([[-399., -398., -397., -396.],\n           [-395., -394., -393., -392.]])\n\n# Exponentiation\nx ** 2\n&gt;&gt;&gt; array([[ 1.,  4.,  9., 16.],\n           [25., 36., 49., 64.]])\n\n# Combine operations\n10 ** (x/2)\n&gt;&gt;&gt; array([[3.16227766e+00, 1.00000000e+01, 3.16227766e+01, 1.00000000e+02],\n           [3.16227766e+02, 1.00000000e+03, 3.16227766e+03, 1.00000000e+04]])\nThese arithmetic operators act as wrappers (effectively shortcuts) around specific built-in NumPy functions; for example, the + operator is a convenient shortcut for the np.add() function:\nx + 2\n&gt;&gt;&gt; array([[ 3.,  4.,  5.,  6.],\n           [ 7.,  8.,  9., 10.]])\n\nnp.add(x,2)\n&gt;&gt;&gt; array([[ 3.,  4.,  5.,  6.],\n           [ 7.,  8.,  9., 10.]])\nThe following table contains a list of arithmetic operators implemented by NumPy. Note that these functions work on all numerical objects, not just arrays.\n\n\n\nArithmetic functions in NumPy \n\n\n\n\n\n\n\n\n\nOperator\nufunc\nDescription\n\n\n\n\n +\n np.add() \nAddition\n\n\n -\n np.subtract() \nSubtraction\n\n\n * \n np.multiply() \nMultiplication\n\n\n / \n np.divide() \nDivision\n\n\n // \n np.floor_divide() \nFloor division (returns largest integer)\n\n\n ** \n np.power() \nExponentiation\n\n\n % \n np.mod() \nModulus/remainder\n\n\n **(1/2) \n np.sqrt() \nSquare root-alize\n\n\n\nFurthermore, as a numerical package, NumPy implements many additional mathematical operations for use in Python ‚Äì¬†on arrays or otherwise. The following tables show some of the more commonly used mathematical functions in NumPy. The x is used to denote a numerical object ‚Äì this could be an int, float, list, ndarray, etc.\n\n\n\nLogarithmic functions \n\n\n\n\n\n\n\n\nufunc\nOperation\n\n\n\n\n np.exp(x) \ne^x\n\n\n np.log(x) \n\\ln x\n\n\n np.log10(x) \n\\log x\n\n\n\n\n\n\n Trigonometric functions \n\n\n\n\n\n\n\n\nufunc\nDescription\n\n\n\n\n np.sin(x) \n\\sin{x}\n\n\n np.cos(x) \n\\cos{x}\n\n\n np.tan(x) \n\\tan{x}\n\n\n np.arcsin(x) \n\\sin^{-1}{x}\n\n\n np.arccos(x) \n\\cos^{-1}{x}\n\n\n np.arctan(x) \n\\tan^{-1}{x}\n\n\n\n\n\nNote: NumPy assumes all inputs to trigonometic functions are in units of radians. The np.radians() function can be used to convert from degrees to radians, while the np.degrees() function does the opposite.\n\n\n\n Useful mathematical constants \n\n\n\n\n\n\n\n\nConstants\nDescription\n\n\n\n\n np.e \ne\n\n\n np.pi \n\\pi\n\n\n\n\n Array-to-array math \n\nSo far, we have only considered operations between a single array and an integer, but often it is necessary to perform mathematical operations on multiple arrays. Much like NumPy handles single array operations, array-to-array math in NumPy uses ufuncs to perform element-wise calculations. For arrays of the same dimensions, this is straight forward:\nx  = np.array([[1.,2.,3.,4.],\n               [5.,6.,7.,8.]])\n\ny = np.array([[9.,87.,3.,5.6],\n              [-1.,4.,7.1,8.]])\n\n# Addition\nx + y\n&gt;&gt;&gt; array([[10. , 89. ,  6. ,  9.6],\n           [ 4. , 10. , 14.1, 16. ]])\n\n# Division\nx / y\n&gt;&gt;&gt; array([[ 0.11111111,  0.02298851,  1.        ,  0.71428571],\n           [-5.        ,  1.5       ,  0.98591549,  1.        ]])\nFor arrays whose dimensions do not match, NumPy does something called broadcasting. So long as one dimension of each array matches and one array has a dimension of 1 in one direction, the smaller array is ‚Äúbroadcast‚Äù to the dimensions of the larger array. In this process, the row or column is replicated to match the dimensions of the larger array. This is best illustrated in the following diagram:\n\na = np.array([[1.,2.,3.,4.],\n             [5.,6.,7.,8.]])\n\nb = np.array([10,11,12,13])\n\nc = np.array([[1.],\n             [20.]])\n\n# Row-wise\na + b\n&gt;&gt;&gt; array([[11., 13., 15., 17.],\n           [15., 17., 19., 21.]])\n\n# Column-wise\na + c\n&gt;&gt;&gt; array([[ 2.,  3.,  4.,  5.],\n           [25., 26., 27., 28.]])\n\n# Multiple operations\na + c**2\n&gt;&gt;&gt; array([[  2.,   3.,   4.,   5.],\n           [405., 406., 407., 408.]])"
  },
  {
    "objectID": "interactive_sessions/3-1_numpy.html#practice-10.",
    "href": "interactive_sessions/3-1_numpy.html#practice-10.",
    "title": "Session 3-1: Introduction to NumPy",
    "section": "üìö  Practice 10. ",
    "text": "üìö  Practice 10. \n\n\nRaise array b to the power of array c.\n\n\nCreate a new 5x10 array of random values. Subtract the mean of each row from every value.\n\n\n\nMissing Data\n\n\nMost real-world datasets ‚Äì¬†environmental or otherwise ‚Äì¬†have data gaps. Data can be missing for any number of reasons, including observations not being recorded or data corruption. While a cell corresponding to a data gap may just be left blank in a spreadsheet, when imported into Python, there must be some way to handle ‚Äúblank‚Äù or missing values.\nMissing data should not be replaced with zeros, as 0 can be a valid value for many datasets, (e.g.¬†temperature, precipitation, etc.). Instead, the convention is to fill all missing data with the constant NaN. NaN stands for ‚ÄúNot a Number‚Äù and is implemented in NumPy as np.nan.\nNaNs are handled differently by different packages. In NumPy, all computations involving NaN values will return nan:\ndata = np.array([[2.,2.7.,1.89.],\n                 [1.1, 0.0, np.nan],\n                 [3.2, 0.74, 2.1]])\n\ndata.mean()\n&gt;&gt;&gt; nan\nIn this case, we‚Äôd want to use the alternative np.nanmean() function, which ignores NaNs:\ndata.nanmean()\n&gt;&gt;&gt; 1.71625\nNumPy has several other functions ‚Äì including np.nanmin(), np.nanmax(), np.nansum() ‚Äì that are analogous to the regular ufuncs covered above, but allow for computation of arrays containing NaN values.\n\n\n\n\nWrapping up\nThe topics covered in this session are but a small window into the wide world of NumPy, but by now you should be familiar with the basic objects and operations in the NumPy library, which are the building blocks of data science in Python. As always ‚Äì especially now that we‚Äôve begun exploring third-party packages ‚Äì refer to the NumPy docs for comprehensive information on all functions, methods, routines, etc. and to check out more of NumPy‚Äôs capabilities.\nNext, we‚Äôll explore one of data scientists‚Äô favorite libraries: üêº."
  },
  {
    "objectID": "interactive_sessions/test_environment.html",
    "href": "interactive_sessions/test_environment.html",
    "title": "Test Environment",
    "section": "",
    "text": "Return to Course Home Page\n\n\n\n\nThis notebook contains code that tests to make sure your JupyterLab instance has all the necessary libraries for working with EDS 217 course materials.\nTo use this notebook, select Run -&gt; Run All Cells from the JupyterLab menu bar at the top of this window.\n\n\nCode\ntry:\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    print(\"You're all set!\\n\")\n    pd.show_versions()\nexcept:\n    print(\"Uh-oh!\")\n\n\nYou're all set!\n\n\nINSTALLED VERSIONS\n------------------\ncommit           : 0f437949513225922d851e9581723d82120684a6\npython           : 3.10.12.final.0\npython-bits      : 64\nOS               : Darwin\nOS-release       : 22.5.0\nVersion          : Darwin Kernel Version 22.5.0: Thu Jun  8 22:22:20 PDT 2023; root:xnu-8796.121.3~7/RELEASE_ARM64_T6000\nmachine          : x86_64\nprocessor        : i386\nbyteorder        : little\nLC_ALL           : None\nLANG             : en_US.UTF-8\nLOCALE           : en_US.UTF-8\n\npandas           : 2.0.3\nnumpy            : 1.25.2\npytz             : 2023.3\ndateutil         : 2.8.2\nsetuptools       : 68.1.2\npip              : 23.2.1\nCython           : None\npytest           : None\nhypothesis       : None\nsphinx           : None\nblosc            : None\nfeather          : None\nxlsxwriter       : None\nlxml.etree       : None\nhtml5lib         : None\npymysql          : None\npsycopg2         : None\njinja2           : 3.1.2\nIPython          : 8.14.0\npandas_datareader: None\nbs4              : 4.12.2\nbottleneck       : None\nbrotli           : 1.0.9\nfastparquet      : None\nfsspec           : None\ngcsfs            : None\nmatplotlib       : 3.7.2\nnumba            : None\nnumexpr          : None\nodfpy            : None\nopenpyxl         : None\npandas_gbq       : None\npyarrow          : None\npyreadstat       : None\npyxlsb           : None\ns3fs             : None\nscipy            : None\nsnappy           : None\nsqlalchemy       : None\ntables           : None\ntabulate         : None\nxarray           : None\nxlrd             : None\nzstandard        : None\ntzdata           : 2023.3\nqtpy             : 2.3.1\npyqt5            : None\n\n\n/Users/kellycaylor/mambaforge/envs/eds217_2023/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n  warnings.warn(\"Setuptools is replacing distutils.\")"
  },
  {
    "objectID": "interactive_sessions/1-1_variables.html",
    "href": "interactive_sessions/1-1_variables.html",
    "title": "Interactive Session: Variables & Operators",
    "section": "",
    "text": "‚¨ÖÔ∏è Previous Session | üè† Course Home | ‚û°Ô∏è Next Session |\nAll programming languages contain the same fundamental tools: variables, operators, and functions. This session will covers the first two of these basic elements of the Python language."
  },
  {
    "objectID": "interactive_sessions/1-1_variables.html#variables-operators",
    "href": "interactive_sessions/1-1_variables.html#variables-operators",
    "title": "Interactive Session: Variables & Operators",
    "section": "Variables + Operators",
    "text": "Variables + Operators\nVariables are used in Python to create references to an object (e.g.¬†string, float, DataFrame, etc.). Variables are assigned in Python using =.\n\nüêç &lt;b&gt;Note.&lt;/b&gt;\nVariable names should be chosen carefully and should indicate what the variable is used for. Python etiquette generally dictates using lowercase variable names. Underscores are common. Variable names cannot start with a number. Also, there are several names that cannot be used as variables, as they are reserved for built-in Python commands, functions, etc. We will see examples of these throughout this session.\n\n\n\n\nNumbers\nNumbers in Python can be either integers (whole numbers) or floats (floating point decimal numbers).\nThe following syntax is used to define an integer:\nx = 1\ny = 42\n\n‚ñ∂Ô∏è Run the cell below.\n\n\n\nCode\n# Define variables x and y as integers.\nx = 1\ny = 42\n\n\nThe following syntax is used to define a float:\na = 1.0\nb = 42.0\nc = 23.782043\n###\n\n‚úèÔ∏è Try it. Define variables a, b, and c according to the values above.\n\n\n\nCode\n# Define variables a, b, and c as floats.\na = 1.0\nb = 42.0\nc = 23.782043\n\n\n\n Arithmetic Operators \n\nJust like a calculator, basic arithmetic can be done on number variables. Python uses the following symbols\n\n\n\nSymbol\nTask\n\n\n\n\n+\nAddition\n\n\n-\nSubtraction\n\n\n*\nMultiplication\n\n\n/\nDivision\n\n\n%\nModular\n\n\n//\nFloor division\n\n\n**\nPower\n\n\n\n###\n\n‚úèÔ∏è Try it. Practice these arithmetic operations by running the code in the cell below. Feel free to add more to test the operators. Use the print() command to output your answers.\n\n\n\nCode\n# Do some math.\n\n\nNotice that the order of operations applies.\n\n Boolean Operators \n\nBoolean operators evaluate a condition between two operands, returning True if the condition is met and False otherwise. True and False are called booleans.\n\n\n\nSymbol\nTask\n\n\n\n\n==\nEquals\n\n\n!=\nDoes not equal\n\n\n&lt;\nLess than\n\n\n&gt;\nGreater than\n\n\n&lt;=\nLess than or equal to\n\n\n&gt;=\nGreater than or equal to\n\n\n\n\n‚ñ∂Ô∏è Run the cell below.\n\n\n\nCode\nprint( b &gt;= a )\nprint( 87 &lt; -2 )\nprint( c != 0 )\nprint( y == x)\n\n\n\n Built-in functions \n\nPython has a number of built-in functions. Here we will introduce a few of the useful built-in functions for numerical variables.\nThe type() function is used to check the data type of a variable. For numerical arguments, either float or int is returned.\n\n‚ñ∂Ô∏è Run the cell below.\n\n\n\nCode\ntype(y)\n\n\nThe isinstance() function is used to determine whether an argument is in a certain class. It returns a boolean value. Multiple classes can be checked at once.\nisinstance(12, int)\n&gt;&gt;&gt; True\n\nisinstance(12.0,int)\n&gt;&gt;&gt; False\n\nisinstance(12.0,(int,float))\n&gt;&gt;&gt; True\nThe commands int() and float() are used to convert between data types.\n\n‚ñ∂Ô∏è Run the cell below.\n\n\n\nCode\nprint( float(y) )\nprint( int(c) )\n\n\nNotice that when converting a float value to an integer, the int() command always rounds down to the nearest whole number.\nTo round a float to the nearest whole number, use the function round(). You can specify the number of decimal places by adding an integer as an argument to the round() function .\n\n‚ñ∂Ô∏è Run the cell below.\n\n\n\nCode\nprint( round(c) )\nprint( round(c,3) )\n\n\nThe complex() function is used to define a complex number. We won‚Äôt be using complex numbers in this course, but it‚Äôs important to know that python is happy to handle them.\n\n‚ñ∂Ô∏è Run the cell below.\n\n\n\nCode\nd = complex('5+2j')\n\n\nTo return the absolute value of a number, use the abs() function.\n\n‚ñ∂Ô∏è Run the cell below.\n\n\n\nCode\nprint( abs(d) )\nprint( abs(-12) )\n\n\nThe divmod() function returns the quotient and remainder of two input operands in a tuple. (Tuples are another data type that we will cover later.)\ndivmod(64, 4.2)\nOutput:\n(15.0, 0.9999999999999973)\n\nThe pow() function is an alternative to the ** operator for raising a number to an exponent, i.e.¬†x^y. An optional third argument is used to return the modulus (%) of the power of a number, i.e.¬†x^y % z.\npow(8,2)\nOutput:\n64\npow(8,2,3)\nOutput:\n1\n\n\n\n\nStrings\nPieces of text in Python are referred to as strings. Strings are defined with either single or double quotes. The only difference between the two is that it is easier to use an apostrophe with double quotes.\nmytext = 'This is a string.'\nmytext2 = \"This is also a string.\"\nTo use an apostrope or single quotes inside a string defined by single quotes (or to use double quotes), use a single backslash ( \\ ) referred to as an ‚Äúescape‚Äù character.\nq1a = \"What is Newton's 1st law of motion?\"\nq1b = 'What is Newton\\'s 1st law of motion?'\n\n Built-in functions \n\nJust like the int() and float commands, the str() command converts a number to a string.\n\n‚ñ∂Ô∏è Run the cell below.\n\n\n\nCode\nystr = str(y)\n\n\nThe + operator can be used to combine two or more strings.\n\n‚ñ∂Ô∏è Run the cell below.\n\n\n\nCode\ns = 'isaac' + ' ' + 'newton'\n\n\nThe commands string.upper() and string.capitalize() can be used to convert all letters in the string to uppercase and capitalize the first letter in the string, respectively.\n\n‚ñ∂Ô∏è Run the cell below.\n\n\n\nCode\nprint( s.upper() )\nprint( s.capitalize() )\n\n\n\n Formatted print statements \n\nPython uses C-style formatting to create new, formatted strings with the % operator. This is useful for printing variables in functions and when asking for user input, both of which we will discuss later. Formatted print statements contain a string argument with one of the following specifiers:\n\n\n\nSymbol\nTask\n\n\n\n\n%s\nStrings\n\n\n%d\nIntegers\n\n\n%f\nFloating point numbers\n\n\n\nThe second argument can contain a variable name or a tuple, which is a list of a fixed size. The arguments are separated by the % operator.\n\n‚ñ∂Ô∏è Run the cell below.\n\n\n\nCode\ncity = 'Santa Barbara'\nyrs = 3\nprint( 'I live in %s.' % city )\nprint( 'I have lived in %s for %d years.' % (city,yrs))\n\n\n\n\nCode\n# Define the variable info.\ninfo = (first, age)\n# Complete the sentence to be printed\nsentence = \"My name...\"\n# Print\nprint( sentence % info)\n\n\nWhen printing floats, the %f argument specifier can be accompanied by a number of decimal places to print only a certain number of digits.\n\n‚ñ∂Ô∏è Run the cell below.\n\n\n\nCode\nimport math\npi_sqrt = math.sqrt(math.pi)\nprint(\"The square root of pi is %f.\" % pi_sqrt)\nprint(\"The square root of pi is %.2f.\" % pi_sqrt)\n\n\n\n\nThe input() function allows for user input within a script or program. Importantly, when Python prompts the user for input, the input is stored as a string, regardless of what it is. Thus, if you write a function (a type of object we will explore in a future session) prompting the user for a number, you must be sure to convert the variable storing the input to an integer or float.\nTo demonstrate this, run the following cell, entering the month in which you were born in numerical format (e.g.¬†if you were born in April, your input would be 4) when prompted.\n\n‚ñ∂Ô∏è Run the cell below.\n\n\n\nCode\nmonth_in = input('Month of birth (1-12): ')\ntype(month_in)"
  },
  {
    "objectID": "interactive_sessions/4-1_pandas.html",
    "href": "interactive_sessions/4-1_pandas.html",
    "title": "Session 4-1: Pandas üêº",
    "section": "",
    "text": "‚¨ÖÔ∏è Previous Session | üè† Course Home | ‚û°Ô∏è Next Session |\nPandas (‚ÄúPython Data Analysis Library‚Äù) is arguably the most important tool for data scientists using Python. As the central component of the Python data science toolkit, pandas is essentially where your data will ‚Äúlive‚Äù when you‚Äôre working in Python. Pandas is built on NumPy, which means that many of the data structures of NumPy are used in pandas. Data stored in pandas DataFrames are often analysed statistically in SciPy, visualized using plotting functions from Matplotlib, and fed into machine learning algorithms in scikit-learn.\nThis session will cover the basics of pandas, including DataFrame construction, importing data with pandas, DataFrame attributes, working with datetime objects, and data selection and manipulation. While this tutorial is designed to give you an overview of pandas, the docs should the first place you look for more detailed information and additional pandas functionality. The pandas documentation is particularly well-written, making it easy to find methods and functions with numerous examples. Make the docs your best friend! üêº\nThe Pandas library was originally developed by Wes McKinney, who is the author of the excellent Python for Data Analysis book, which is now its third edition."
  },
  {
    "objectID": "interactive_sessions/4-1_pandas.html#introduction-to-pandas",
    "href": "interactive_sessions/4-1_pandas.html#introduction-to-pandas",
    "title": "Session 4-1: Pandas üêº",
    "section": "Introduction to pandas",
    "text": "Introduction to pandas\n\nAs always, we must begin by importing the pandas library. The standard import statement for pandas is:\nimport pandas as pd\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nimport pandas as pd\n\n\n\nSeries and DataFrame objects\n\n\nThe core components of pandas are the Series and the DataFrame objects. Both of these are essentially enhanced versions of the NumPy array, with a few key differences: 1) pandas DataFrames can be heterogeneous, meaning that the columns can contain different data types; and 2) the rows and columns of DataFrames can be identified with labels (usually strings) in addition to standard integer indexing.\nA Series is essentially a column of data, while a DataFrame is a multidimensional table made up of many Series, not unlike a spreadsheet:\n\nSeries and DataFrames are similar in many respects ‚Äì most common operations can be performed on both objects, though Series are more limited, as they can only ever contain a single column (i.e.¬†you cannot turn a Series into a DataFrame by adding a column).\nBoth Series and DataFrame objects contain an Index object similar to the row index of the ndarray or the index of a list. The pandas Index object can be conceptualized as an immutable array or an ordered multiset. Unless explicitly defined otherwise, the Index of a Series or DataFrame is initialized as the ordered set of positive integers beginning at 0 (see figure above).\n\nCreating Series and DataFrame objects from scratch\nA Series can be easily created from a list or array as follows:\n# Create a Series from a list\nseries = pd.Series([25.8, 16.2, 17.9, 18.8, 23.6, 29.9, 23.6, 22.1])\n\nseries\n\n\n\n\n\n0    25.8\n1    16.2\n2    17.9\n3    18.8\n4    23.6\n5    29.9\n6    23.6\n7    22.1\ndtype: float64\n\n\n\n\nThere are many ways to create a DataFrame, but the most common are to use a list of lists or a dictionary. First, let‚Äôs use a list of lists (or an array):\n# Create a df from a list of lists\ndf = pd.DataFrame([[25.8, 28.1, 16.2, 11.0],[17.9, 14.2, 18.8, 28.0],\n                   [23.6, 18.4, 29.9, 27.8],[23.6, 36.2, 22.1, 14.5]],\n                 columns=['A','B','C','D'])\ndf\n\n\n\n\n\n      A     B     C     D\n0  25.8  28.1  16.2  11.0\n1  17.9  14.2  18.8  28.0\n2  23.6  18.4  29.9  27.8\n3  23.6  36.2  22.1  14.5\n\n\n\n\nMuch like with NumPy arrays, each inner list element in the outer list corresponds to a row. Using the optional columns keyword argument, we can specify the name of each column. If this parameter is not passed, the columns would be displayed with integer index values (like the rows).\nNext, let‚Äôs create a DataFrame from a dict object:\n# Create a df from a dictionary\ndf = pd.DataFrame({'A': [25.8, 17.9, 23.6, 23.6],\n                   'B': [28.1, 14.2, 18.4, 36.2],\n                   'C': [16.2, 18.8, 29.9, 22.1],\n                   'D': [11.0, 28.0, 27.8, 14.5]})\n\ndf\n\n\n\n\n\n      A     B     C     D\n0  25.8  28.1  16.2  11.0\n1  17.9  14.2  18.8  28.0\n2  23.6  18.4  29.9  27.8\n3  23.6  36.2  22.1  14.5\n\n\n\n\nUsing this method, each key corresponds to a column name, and each value is a column."
  },
  {
    "objectID": "interactive_sessions/4-1_pandas.html#practice-1.",
    "href": "interactive_sessions/4-1_pandas.html#practice-1.",
    "title": "Session 4-1: Pandas üêº",
    "section": "üìö  Practice 1. ",
    "text": "üìö  Practice 1. \n\n\nUse a list of lists to construct a DataFrame named df1 containing the data in the table below.\n\n\n\n\n\nRiver\n\n\nLength (\\text{km})\n\n\nDrainage area (\\text{km}^2)\n\n\n\n\nAmazon\n\n\n6400\n\n\n7,050,000\n\n\n\n\nCongo\n\n\n4371\n\n\n4,014,500\n\n\n\n\nYangtze\n\n\n6418\n\n\n1,808,500\n\n\n\n\nMississippi\n\n\n3730\n\n\n3,202,230\n\n\n\n\n\n\n\nUse a dict to construct a DataFrame named df2 containing the data in the table below.\n\n\n\n\n\nRiver\n\n\nLength (\\text{km})\n\n\nDrainage area (\\text{km}^2)\n\n\n\n\nZambezi\n\n\n2574\n\n\n1,331,000\n\n\n\n\nMekong\n\n\n4023\n\n\n811,000\n\n\n\n\nMurray\n\n\n2508\n\n\n1,061,469\n\n\n\n\nRh√¥ne\n\n\n813\n\n\n98,000\n\n\n\n\nCubango\n\n\n1056\n\n\n530,000"
  },
  {
    "objectID": "interactive_sessions/4-1_pandas.html#practice-2.-using-the-dataframe-bsrn",
    "href": "interactive_sessions/4-1_pandas.html#practice-2.-using-the-dataframe-bsrn",
    "title": "Session 4-1: Pandas üêº",
    "section": "üìö Practice 2. Using the DataFrame bsrn:",
    "text": "üìö Practice 2. Using the DataFrame bsrn:\n\n\nPrint a list of column names.\n\n\nHow many values are there in the entire DataFrame?\n\n\nWhat is the data type of the first column?\n\n\n\nDataFrame indexing + data selection\n\n\nBecause DataFrames can contain labels as well as indices, indexing in pandas DataFrames is a bit more complicated than we‚Äôve seen with strings, lists, and arrays. Generally speaking, pandas allows indexing by either the integer index or the label, but the syntax is a bit different for each.\nThe index operator, which refers to the square brackets following an object [], does not work quite like we might expect it to.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\nInstead of a value, we get a KeyError. This is because the Index object in pandas is essentially a dictionary, and we have not passed proper keys.\nInstead, pandas uses df.iloc[] for integer-based indexing to select data by position:\nbsrn.iloc[1434,12]\n\n &gt;&gt;&gt;  19.6\n\ndf.iloc acts just like the index operator works with arrays. In addition to indexing a single value, df.iloc can be used to select multiple rows and columns via slicing: df.iloc[row_start:row_end:row_step, col_start:col_end:col_step].\n# Select 6 rows, last 3 columns\nbsrn.iloc[1434:1440,12:]\n\n\n\n\n\n      T_degC    RH  P_hPa\n1434    19.6  17.6    965\n1435    19.5  17.5    965\n1436    19.4  17.4    965\n1437    19.1  17.5    965\n1438    19.4  17.6    965\n1439    19.3  17.5    965\n\n\n\n\n# First 5 columns, every 40th row\nbsrn.iloc[::40,:5]\n\n\n\n\n\n DATE H_m SWD_Wm2 STD_SWD DIR_Wm2 0 2019-10-01 00:00:00 2 -3.0 0.0 0.0 40 2019-10-01 00:40:00 2 -3.0 0.0 0.0 80 2019-10-01 01:20:00 2 -3.0 0.0 0.0 120 2019-10-01 02:00:00 2 -3.0 0.0 0.0 160 2019-10-01 02:40:00 2 -2.0 0.0 0.0 ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ 44440 2019-10-31 20:40:00 2 -2.0 0.0 0.0 44480 2019-10-31 21:20:00 2 -2.0 0.0 0.0 44520 2019-10-31 22:00:00 2 -2.0 0.0 0.0 44560 2019-10-31 22:40:00 2 -2.0 0.0 0.0 44600 2019-10-31 23:20:00 2 -2.0 0.0 0.0\n\n\n\n[1116 rows x 5 columns] \n\n\nRow indexing\nIn addition to df.iloc, rows of a DataFrame can be accessed using df.loc, which ‚Äúlocates‚Äù rows based on their labels. Unless you have set a custom index (which we will see later), the row ‚Äúlabels‚Äù are the same as the integer index.\nWhen indexing a single row, df.loc (like df.iloc) transforms the row into a Series, with the column names as the index:\n# Classic indexing\nbsrn[1434]\n\n\n\n\n\nDATE       2019-10-01 23:54:00\nH_m                          2\nSWD_Wm2                     -2\nSTD_SWD                      0\nDIR_Wm2                      0\nSTD_DIR                      0\nDIF_Wm2                     -2\nSTD_DIF                      0\nLWD_Wm2                    307\nSTD_LWD                    0.1\nSWU_Wm2                      0\nLWU_Wm2                    385\nT_degC                    19.6\nRH                        17.6\nP_hPa                      965\nName: 1434, dtype: object\n\n\n\n\n\nüêç &lt;b&gt;DataFrames + data types.&lt;/b&gt;  Notice that the &lt;code&gt;dtype&lt;/code&gt; of the Series is an &lt;code&gt;object&lt;/code&gt;. This is because the column contains mixed data types ‚Äì floats, integers, and an &lt;code&gt;object&lt;/code&gt; in the first row. Unlike NumPy, pandas allows both rows and columns to contain mixed data types. However, while it is perfectly fine (and, in fact, almost always necessary) to have multiple data types within a single &lt;b&gt;&lt;i&gt;row&lt;/i&gt;&lt;/b&gt;, it is best if each &lt;b&gt;&lt;i&gt;column&lt;/i&gt;&lt;/b&gt; is comprised of a &lt;b&gt;&lt;i&gt;single data type&lt;/i&gt;&lt;/b&gt;.\n\nSlicing using df.loc is similar to df.iloc, with the exception that the stop value is inclusive:\n# Using .loc\nbsrn.loc[1434:1440]\n\n\n\n\n\n DATE H_m SWD_Wm2 STD_SWD DIR_Wm2 ‚Ä¶ SWU_Wm2 LWU_Wm2 T_degC RH P_hPa 1434 2019-10-01 23:54:00 2 -2.0 0.0 0.0 ‚Ä¶ 0 385 19.6 17.6 965 1435 2019-10-01 23:55:00 2 -2.0 0.0 0.0 ‚Ä¶ 0 385 19.5 17.5 965 1436 2019-10-01 23:56:00 2 -2.0 0.0 0.0 ‚Ä¶ 0 386 19.4 17.4 965 1437 2019-10-01 23:57:00 2 -2.0 0.0 0.0 ‚Ä¶ 0 386 19.1 17.5 965 1438 2019-10-01 23:58:00 2 -2.0 0.0 0.0 ‚Ä¶ 0 386 19.4 17.6 965 1439 2019-10-01 23:59:00 2 -2.0 0.0 0.0 ‚Ä¶ 0 386 19.3 17.5 965 1440 2019-10-02 00:00:00 2 -2.0 0.0 0.0 ‚Ä¶ 0 386 19.1 17.5 965\n\n\n\n[7 rows x 15 columns] \n\n\n\nColumn indexing\nIn addition to integer indexing with df.iloc, columns can be accessed in two ways: dot notation . or square brackets []. The former takes advantage of the fact that the columns are effectively ‚Äúattributes‚Äù of the DataFrame and returns a Series:\nbsrn.SWD_Wm2\n\n\n\n\n\n0       -3.0\n1       -3.0\n2       -3.0\n3       -3.0\n4       -3.0\n...\n44635   -2.0\n44636   -2.0\n44637   -2.0\n44638   -2.0\n44639   -2.0\nName: SWD_Wm2, Length: 44640, dtype: float64\n\n\n\n\nThe second way of extracting columns is to pass the column name as a string in square brackets, i.e.¬†df['col']:\nbsrn['SWD_Wm2']\n\n\n\n\n\n0       -3.0\n1       -3.0\n2       -3.0\n3       -3.0\n4       -3.0\n...\n44635   -2.0\n44636   -2.0\n44637   -2.0\n44638   -2.0\n44639   -2.0\nName: SWD_Wm2, Length: 44640, dtype: float64\n\n\n\n\nUsing single brackets, the result is a Series. However, using double brackets, it is possible to return the column as a DataFrame:\nbsrn[['SWD_Wm2']]\n\n\n\n\n\n SWD_Wm2 0 -3.0 1 -3.0 2 -3.0 3 -3.0 4 -3.0 ‚Ä¶ ‚Ä¶ 44635 -2.0 44636 -2.0 44637 -2.0 44638 -2.0 44639 -2.0\n\n\n\n[44640 rows x 1 columns] \n\nThis allows you to add additional columns, which you cannot do with a Series object. Furthermore, with the double bracket notation, a list is being passed to the index operator (outer brackets). Thus, it is possible to extract multiple columns by adding column names to the list:\nbsrn[['SWD_Wm2','LWD_Wm2']]\n\n\n\n\n\n SWD_Wm2 LWD_Wm2 0 -3.0 300.0 1 -3.0 300.0 2 -3.0 300.0 3 -3.0 300.0 4 -3.0 300.0 ‚Ä¶ ‚Ä¶ ‚Ä¶ 44635 -2.0 380.0 44636 -2.0 380.0 44637 -2.0 380.0 44638 -2.0 381.0 44639 -2.0 381.0\n\n\n\n[44640 rows x 2 columns] \n\nWhen accessing a single column, the choice between using dot notation and square brackets is more or less a matter of preference. However, there are occasions when the bracket notation proves particularly useful. For example, you could access each column in a DataFrame by iterating through df.columns, which returns an Index object containing the column names as str objects that can be directly passed to the index operator. Additionally, you may find it useful to use the double bracket syntax to return a DataFrame object, rather than a Series, which can only ever contain a single column of data."
  },
  {
    "objectID": "interactive_sessions/4-1_pandas.html#practice-3.",
    "href": "interactive_sessions/4-1_pandas.html#practice-3.",
    "title": "Session 4-1: Pandas üêº",
    "section": "üìö Practice 3.",
    "text": "üìö Practice 3.\n\n\nCreate a new DataFrame containing the first record for each day and the following columns: the timestamp of the record, incoming shortwave radiation, direct and diffuse radiation, and incoming longwave radiation. (Hint: the BSRN station collects data every minute).\n\n\nCreate a new Series containing the temperature values every hour at the top of the hour.\n\n\n\nDatetime objects\n\n\nLike the BSRN data we are working with in this session, many environmental datasets include timed records. Python has a few different libraries for dealing with timestamps, which are referred to as datetime objects. The standard datetime library is the primary way of manipulating dates and times in Python, but there are additional third-party packages that provide additional support. A few worth exploring are dateutil, an extension of the datetime library useful for parsing timestamps, and pytz, which provides a smooth way of tackling time zones.\nThough we will not review datetime objects in depth here, it is useful to understand the basics of how to deal with datetime objects in Python as you will no doubt encounter them in the future. For now, we will focus on a few pandas functions built on the datetime library to handle datetime objects.\nThe pd.date_range() function allows you to build a DatetimeIndex with a fixed frequency. This can be done by specifying a start date and an end date as follows:\npd.date_range('4/1/2017','4/30/2017')\n\n&gt;&gt;&gt; DatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04',\n                   '2017-01-05', '2017-01-06', '2017-01-07', '2017-01-08',\n                   '2017-01-09', '2017-01-10',\n                   ...\n                   '2020-12-22', '2020-12-23', '2020-12-24', '2020-12-25',\n                   '2020-12-26', '2020-12-27', '2020-12-28', '2020-12-29',\n                   '2020-12-30', '2020-12-31'],\n                  dtype='datetime64[ns]', length=1461, freq='D')\nBecause it was not specified otherwise, the frequency was set as the default, daily. To return a different frequency, we could use the freq parameter:\n# Specify start and end, minute-ly frequency\npd.date_range('1/1/2017','12/31/2020', freq='min')\n\n&gt;&gt;&gt; DatetimeIndex(['2017-01-01 00:00:00', '2017-01-01 00:01:00',\n                   '2017-01-01 00:02:00', '2017-01-01 00:03:00',\n                   '2017-01-01 00:04:00', '2017-01-01 00:05:00',\n                   '2017-01-01 00:06:00', '2017-01-01 00:07:00',\n                   '2017-01-01 00:08:00', '2017-01-01 00:09:00',\n                   ...\n                   '2020-12-30 23:51:00', '2020-12-30 23:52:00',\n                   '2020-12-30 23:53:00', '2020-12-30 23:54:00',\n                   '2020-12-30 23:55:00', '2020-12-30 23:56:00',\n                   '2020-12-30 23:57:00', '2020-12-30 23:58:00',\n                   '2020-12-30 23:59:00', '2020-12-31 00:00:00'],\n                  dtype='datetime64[ns]', length=2102401, freq='T')\n\n# Specify start and end, monthly frequency\npd.date_range('1/1/2017','12/31/2020', freq='M')\n\n&gt;&gt;&gt; DatetimeIndex(['2017-01-31', '2017-02-28', '2017-03-31', '2017-04-30',\n                   '2017-05-31', '2017-06-30', '2017-07-31', '2017-08-31',\n                   '2017-09-30', '2017-10-31', '2017-11-30', '2017-12-31',\n                   '2018-01-31', '2018-02-28', '2018-03-31', '2018-04-30',\n                   '2018-05-31', '2018-06-30', '2018-07-31', '2018-08-31',\n                   '2018-09-30', '2018-10-31', '2018-11-30', '2018-12-31',\n                   '2019-01-31', '2019-02-28', '2019-03-31', '2019-04-30',\n                   '2019-05-31', '2019-06-30', '2019-07-31', '2019-08-31',\n                   '2019-09-30', '2019-10-31', '2019-11-30', '2019-12-31',\n                   '2020-01-31', '2020-02-29', '2020-03-31', '2020-04-30',\n                   '2020-05-31', '2020-06-30', '2020-07-31', '2020-08-31',\n                   '2020-09-30', '2020-10-31', '2020-11-30', '2020-12-31'],\n                  dtype='datetime64[ns]', freq='M')\nThere are many other parameters for the pd.date_range() function, as well as other pandas functions. More useful to us, however, are the functions for dealing with existing timestamps, such as those in our bsrn DataFrame.\n\nParsing dates in pandas\nLet‚Äôs start by taking a look at bsrn.DATE, which contains the timestamps for each record of our BSRN data.\nbsrn.DATE\n\n\n\n\n\n0        2019-10-01 00:00:00\n1        2019-10-01 00:01:00\n2        2019-10-01 00:02:00\n3        2019-10-01 00:03:00\n4        2019-10-01 00:04:00\n...\n44635    2019-10-31 23:55:00\n44636    2019-10-31 23:56:00\n44637    2019-10-31 23:57:00\n44638    2019-10-31 23:58:00\n44639    2019-10-31 23:59:00\nName: DATE, Length: 44640, dtype: object\n\n\n\n\nWhile the values certainly resemble datetime objects, they are stored in pandas as ‚Äúobjects,‚Äù which basically means that pandas doesn‚Äôt recognize the data type ‚Äì¬†it doesn‚Äôt know how to handle them. Using the pd.to_datetime() function, we can convert this column to datetime objects:\npd.to_datetime(bsrn.DATE)\n\n\n\n\n\n0       2019-10-01 00:00:00\n1       2019-10-01 00:01:00\n2       2019-10-01 00:02:00\n3       2019-10-01 00:03:00\n4       2019-10-01 00:04:00\n...\n44635   2019-10-31 23:55:00\n44636   2019-10-31 23:56:00\n44637   2019-10-31 23:57:00\n44638   2019-10-31 23:58:00\n44639   2019-10-31 23:59:00\nName: DATE, Length: 44640, dtype: datetime64[ns]\n\n\n\n\nNotice that ostensibly nothing has changed, but the dtype is now a datetime object, making it much easier to manipulate not only this column, but the entire DataFrame. For instance, now that we‚Äôve told pandas that this column contains timestamps, we can set this column as the index using df.set_index().\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Convert bsrn.DATE column to datetime objects\nbsrn = pd.read_csv('../data/BSRN_GOB_2019-10.csv')\nbsrn['DATE'] = pd.to_datetime(bsrn.DATE)  # Note: overwriting a column like this is NOT recommended.\n# Set bsrn.DATE as the DataFrame index\nbsrn.set_index('DATE', inplace=True)\n\n\nAs noted in the comment in the cell above, reseting the values in a column as we did in the first line of code is generally not recommended, but in this case, since we knew exactly what the result would be, it‚Äôs acceptable. Also, notice the inplace=True argument passed to df.set_index(). This prevented us from having to copy the DataFrame to a new variable, instead performing the operation in-place.\nLet‚Äôs take a look at our DataFrame again:\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nbsrn.info()\n\n\nAs expected, the index has been changed to a DatetimeIndex, and there is no longer a 'DATE' column. Had we wanted to keep the timestamps as a column as well, we could have passed drop=False to df.set_index(), telling pandas not to drop (or delete) the 'DATE' column. We can look at the DatetimeIndex just as before using df.index.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nbsrn.describe()\n\n\nNow that we have a DatetimeIndex, we can access specific attributes of the datetime objects like the year, day, hour, etc. To do this, we add the desired time period using dot notation: df.index.attribute. For a full list of attributes, see the pd.DatetimeIndex documentation. For example:\n# Get the hour of each record\nbsrn.index.hour\n\n\n\n\n\nInt64Index([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n...\n23, 23, 23, 23, 23, 23, 23, 23, 23, 23],\ndtype='int64', name='DATE', length=44640)\n\n\n\n\nThe result is a pandas Index object with the same length as the original DataFrame. To return only the unique values, we use the Series.unique() function, which can be used on any Series object (including a column of a DataFrame):\n# Get the unique hour values\nbsrn.index.hour.unique()\n\n\n\n\n\nInt64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n17, 18, 19, 20, 21, 22, 23],\ndtype='int64', name='DATE')\n\n\n\n\n\nüêç &lt;b&gt;Method chaining.&lt;/b&gt;  This process of stringing multiple methods together in a single line of code is called &lt;b&gt;method chaining&lt;/b&gt;, a hallmark of object-oriented programming. Method chaining is a means of concatenating functions in order to quickly complete a series of data transformations. In pandas, we often use method chaining in aggregation processes to perfrom calculations on groups or selections of data. Methods are appended using dot notation to the end of a command. Any code that is expressed using method chaining could also be written using a series of commands (and vice versa). Method chaining is common in JavaScript, and while it is not widely used in Python, it is commonly applied in pandas.\n\nDealing with datetime objects can be tricky and often requires a bit of trial and error before the timestamps are in the desired format. If you know the format of your dataset and its timestamp records, you can parse the datetimes and set the index when reading in the data. For example, we could have imported our data as follows:\nbsrn = pd.read_csv('../data/BSRN_GOB_2019-10.csv',index_col=0,parse_dates=True)\nThis would have accomplished what we ultimately did in three lines in a single line of code. But remember, working with most raw datasets is rarely this straightforward ‚Äì¬†even the file we are using in this session was preprocessed to streamline the import process!\n\n\n\nA few useful operations\n\n\nNow that our DataFrame is a bit cleaner ‚Äì each of the columns contains a single, numeric data type ‚Äì we are ready to start working with our data. Next, we‚Äôll explore DataFrame reduction operations, how to add and delete data, and concatenation in pandas.\n\nDataFrame reduction\nMuch like NumPy, pandas has several useful methods for reducing data to a single statistic. These are intuitively named and include: df.mean(), df.median(), df.sum(), df.max(), df.min(), and df.std(). Unlike array reduction, however, these basic statistical methods in pandas operate column-wise, returning a Series containing the statistic for each column indexed by column name. For example:\n# Calculate median of each column\nbsrn.median()\n\n\n\n\n\nH_m          2.0\nSWD_Wm2     27.0\nSTD_SWD      0.3\nDIR_Wm2      0.0\nSTD_DIR      0.0\nDIF_Wm2     19.0\nSTD_DIF      0.1\nLWD_Wm2    340.0\nSTD_LWD      0.1\nSWU_Wm2     11.0\nLWU_Wm2    432.0\nT_degC      22.4\nRH          33.1\nP_hPa      965.0\ndtype: float64\n\n\n\n\nTo retrieve the value for just a single column, you can use indexing to call the column as a Series:\n# Calculate median incoming shortwave radiation\nbsrn.SWD_Wm2.median()\n\n &gt;&gt;&gt;  27.0\n\nFurthermore, while it is not apparent in this example, pandas default behaviour is to ignore NaN values when performing computations. This can be changed by passing skipna=False to the reduction method (e.g.¬†df.median(skipna=False)), though skipping NaNs is often quite useful!"
  },
  {
    "objectID": "interactive_sessions/4-1_pandas.html#practice-4.",
    "href": "interactive_sessions/4-1_pandas.html#practice-4.",
    "title": "Session 4-1: Pandas üêº",
    "section": "üìö  Practice 4. ",
    "text": "üìö  Practice 4. \nCalculate the mean incoming shortwave, outgoing shortwave, incoming longwave, and outgoing longwave radiation over the entire month.\n\nAdding data\nMuch like when we converted bsrn.DATE to datetime objects, a column can be added to a DataFrame using square bracket notation with a new column label as a string. The data for the new column can come in the form of a list, Series, or a single value:\ndf = pd.DataFrame([[25.8, 28.1, 16.2, 11.0],\n                   [17.9, 14.2, 18.8, 28.0],\n                   [23.6, 18.4, 29.9, 27.8],\n                   [23.6, 36.2, 22.1, 14.5]],\n                 columns=['A','B','C','D'])\n\n# Add a column from a list\ndf['E'] = [13.0, 40.1, 39.8, 28.2]\n\n# Add a column from a Series\ndf['F'] = pd.Series([18, 22, 30, 24])\n\n# Propagate a single value through all rows\ndf['G'] = 'blue'\n\ndf\n\n\n\n\n\n      A     B     C     D     E   F     G\n0  25.8  28.1  16.2  11.0  13.0  18  blue\n1  17.9  14.2  18.8  28.0  40.1  22  blue\n2  23.6  18.4  29.9  27.8  39.8  30  blue\n3  23.6  36.2  22.1  14.5  28.2  24  blue\n\n\n\n\nNew columns can also be added as the result of an arithmetic operation (e.g.¬†sum, product, etc.) performed on one or more existing columns:\n# Add a new column by converting values in df.A from ¬∞C to ¬∞F\ndf['A_degF'] = (df['A'] * (9/5)) + 32\n\n# Add a new column representing the difference between df.B and df.C\ndf['BC_diff'] = df.B - df.C\n\ndf\n\n\n\n\n\n      A     B     C     D     E   F     G  A_degF  BC_diff\n0  25.8  28.1  16.2  11.0  13.0  18  blue   78.44     11.9\n1  17.9  14.2  18.8  28.0  40.1  22  blue   64.22     -4.6\n2  23.6  18.4  29.9  27.8  39.8  30  blue   74.48    -11.5\n3  23.6  36.2  22.1  14.5  28.2  24  blue   74.48     14.1\n\n\n\n\nFinally, you can use a Boolean expression to add a column, which contains Boolean objects (True or False) based on the condition. For example:\n# Add a column with Booleans for values in df.D greater than or equal to 20.0\ndf['D_20plus'] = df.D &gt;= 20.0\n\ndf\n\n\n\n\n\n      A     B     C     D     E   F     G  A_degF  BC_diff  D_20plus\n0  25.8  28.1  16.2  11.0  13.0  18  blue   78.44     11.9     False\n1  17.9  14.2  18.8  28.0  40.1  22  blue   64.22     -4.6      True\n2  23.6  18.4  29.9  27.8  39.8  30  blue   74.48    -11.5      True\n3  23.6  36.2  22.1  14.5  28.2  24  blue   74.48     14.1     False\n\n\n\n\nThese conditional expressions can also be used to create Boolean masks, which allow you to ‚Äúmask‚Äù the values in the DataFrame that do not meet a condition, only extracting those that do. For example, let‚Äôs use a Boolean mask to apply an mathematical expression on only certain values in column 'D':\n# Subtract 20 from all values in dfD greater than or equal to 20\ndf['D_less20'] = df.D[df.D &gt;= 20.0] - 20.0\n\ndf\n\n\n\n\n\n      A     B     C     D     E   F     G  A_degF  BC_diff  D_20plus  D_less20\n0  25.8  28.1  16.2  11.0  13.0  18  blue   78.44     11.9     False       NaN\n1  17.9  14.2  18.8  28.0  40.1  22  blue   64.22     -4.6      True       8.0\n2  23.6  18.4  29.9  27.8  39.8  30  blue   74.48    -11.5      True       7.8\n3  23.6  36.2  22.1  14.5  28.2  24  blue   74.48     14.1     False       NaN\n\n\n\n\nAll values that do not meet the condition are hidden from the expression, leaving NaNs in the resulting column. Boolean masks come in quite handy in data analysis, as they allow you to extract certain rows from a DataFrame based on their values in one or more columns.\nFurthermore, in addition to simply adding columns, new columns can be inserted in a desired index position using df.insert() with arguments specifying the location, name, and values of the column:\n# Create list of seasons\nseasons = ['winter', 'spring', 'summer', 'fall']\n\n# Insert season as first column\ndf.insert(0, 'SEASON', seasons)\n\ndf\n\n\n\n\n\n   SEASON     A     B     C     D     E   F     G  A_degF  BC_diff  D_20plus  D_less20\n0  winter  25.8  28.1  16.2  11.0  13.0  18  blue   78.44     11.9     False       NaN\n1  spring  17.9  14.2  18.8  28.0  40.1  22  blue   64.22     -4.6      True       8.0\n2  summer  23.6  18.4  29.9  27.8  39.8  30  blue   74.48    -11.5      True       7.8\n3    fall  23.6  36.2  22.1  14.5  28.2  24  blue   74.48     14.1     False       NaN\n\n\n\n\n\n\nRemoving data\nUnlike adding new data columns, removing columns from a DataFrame should be done with caution. In fact, it‚Äôs not a bad idea to create a copy of your DataFrame before performing any operations. This will allow you to return to the original data as needed without having to re-import or re-initialize the DataFrame. If you do need to remove a column, you can use the del command:\n# Delete 'G' from df\ndel df['G']\n\ndf\n\n\n\n\n\n   SEASON     A     B     C     D     E   F  A_degF  BC_diff  D_20plus  D_less20\n0  winter  25.8  28.1  16.2  11.0  13.0  18   78.44     11.9     False       NaN\n1  spring  17.9  14.2  18.8  28.0  40.1  22   64.22     -4.6      True       8.0\n2  summer  23.6  18.4  29.9  27.8  39.8  30   74.48    -11.5      True       7.8\n3    fall  23.6  36.2  22.1  14.5  28.2  24   74.48     14.1     False       NaN\n\n\n\n\nNote that this is an in-place operation, meaning that the column is deleted from the original variable. Alternatively, you can use df.pop() to extract a column. This method allows a column values to be extracted (and deleted) from a DataFrame and assigned to a new variable:\n# Extract column 'F' from df as a new Series\ndf_F = df.pop('F')\n\ndf\n\n\n\n\n\n   SEASON     A     B     C     D     E  A_degF  BC_diff  D_20plus  D_less20\n0  winter  25.8  28.1  16.2  11.0  13.0   78.44     11.9     False       NaN\n1  spring  17.9  14.2  18.8  28.0  40.1   64.22     -4.6      True       8.0\n2  summer  23.6  18.4  29.9  27.8  39.8   74.48    -11.5      True       7.8\n3    fall  23.6  36.2  22.1  14.5  28.2   74.48     14.1     False       NaN\n\n\n\n\n\n\nApplying functions\nIn addition to manipulating individual columns, you can apply a function to an entire Series or DataFrame using the pandas function df.apply(). For example, consider our original DataFrame df, which consists of temperature values in ¬∞C:\ndf = pd.DataFrame([[25.8, 28.1, 16.2, 11.0],[17.9, 14.2, 18.8, 28.0],\n                   [23.6, 18.4, 29.9, 27.8],[23.6, 36.2, 22.1, 14.5]],\n                 columns=['A','B','C','D'])\ndf\n\n\n\n\n\n      A     B     C     D\n0  25.8  28.1  16.2  11.0\n1  17.9  14.2  18.8  28.0\n2  23.6  18.4  29.9  27.8\n3  23.6  36.2  22.1  14.5\n\n\n\n\nWe previously used arithmetic operators to convert column 'A' to ¬∞F, but we could also use a function. First, let‚Äôs define a function convert_CtoF to convert temperature values from Celsius to Fahrenheit:\ndef convert_CtoF(degC):\n    \"\"\" Converts a temperature to from Celsius to Fahrenheit\n    \n    Parameters\n    ----------\n        degC : float\n            Temperature value in ¬∞C\n       \n    Returns\n    -------\n        degF : float\n            Temperature value in ¬∞F\n    \"\"\"\n    \n    degF = (degC *(9./5)) + 32\n    \n    return degF\nUsing df.apply() we can use this function to convert values in column 'A' as follows:\ndf.A.apply(convert_CtoF)\n\n\n\n\n\n0    78.44\n1    64.22\n2    74.48\n3    74.48\nName: A, dtype: float64\n\n\n\n\nWhere this becomes especially useful is for operating on entire DataFrames. You have to be careful with this if your DataFrame contains multiple data types, but it works well when you need to perform an operation on an entire DataFrame. For example, we could convert all of the values in df by iterating through the columns, or, using df.apply(), we could acheive the same result in a single line of code:\ndf.apply(convert_CtoF)\n\n\n\n\n\n       A      B      C      D\n0  78.44  82.58  61.16  51.80\n1  64.22  57.56  65.84  82.40\n2  74.48  65.12  85.82  82.04\n3  74.48  97.16  71.78  58.10\n\n\n\n\ndf.apply(convert_CtoF)\n\n\n\n\n\n       A      B      C      D\n0  78.44  82.58  61.16  51.80\n1  64.22  57.56  65.84  82.40\n2  74.48  65.12  85.82  82.04\n3  74.48  97.16  71.78  58.10"
  },
  {
    "objectID": "interactive_sessions/4-1_pandas.html#practice-5.",
    "href": "interactive_sessions/4-1_pandas.html#practice-5.",
    "title": "Session 4-1: Pandas üêº",
    "section": "üìö  Practice 5. ",
    "text": "üìö  Practice 5. \n\n\nAdd a column ‚ÄòNET_SW‚Äô to bsrn with the net shortwave radiation.\n\n\nAdd a column ‚ÄòNET_LW‚Äô to bsrn with the net longwave radiation.\n\n\nAdd a column ‚ÄòNET_RAD‚Äô to bsrn with the net total radiation. Net radiation is given by the following equation:\n\nR^{}_{N} \\, = \\,  R^{\\, \\downarrow}_{SW} \\, - \\,  R^{\\, \\uparrow}_{SW} \\, + \\, R^{\\, \\downarrow}_{LW} \\, - \\,  R^{\\, \\uparrow}_{LW} where R^{\\, \\downarrow}_{SW} and R^{\\, \\uparrow}_{SW} are incoming and outgoing shortwave radiation, respectively, and R^{\\, \\downarrow}_{LW} and R^{\\, \\uparrow}_{LW} are incoming and outgoing longwave radiation, respectively.\n\nCreate a new DataFrame with the day of the month and daily mean values of shortwave incoming, shortwave outgoing, longwave incoming, longwave outgoing radiation, and net total radiation. (Hint: use masking!).\n\n\n\nCombining DataFrames\nThere are several ways to combine data from multiple Series or DataFrames into a single object in pandas. These functions include pd.append(), pd.join(), and pd.merge(). We will focus on the general pd.concat() function, which is the most versatile way to concatenate pandas objects. To learn more about these other functions, refer to the pandas documentation or see  Chapter 3 of the  Python Data Science Handbook.\nLet‚Äôs start by considering the simplest case of two DataFrames with identical columns:\ndf1 = pd.DataFrame([['Los Angeles', 34.0522, -118.2437],\n                    ['Bamako', 12.6392, 8.0029],\n                    ['Johannesburg', -26.2041, 28.0473],\n                    ['Cairo', 30.0444, 31.2357]],\n                  columns=['CITY', 'LAT', 'LONG'])\n\ndf2 = pd.DataFrame([['Cape Town', -33.9249, 18.4241],\n                    ['Kyoto', 35.0116, 135.7681],\n                    ['London', 51.5074, -0.1278],\n                    ['Cochabamba', -17.4140, -66.1653]],\n                  columns=['CITY', 'LAT', 'LONG'])\nUsing pd.concat([df1,df2]), we can combine the two DataFrames into one. Notice that we must pass the DataFrames as a list, because pd.concat() requires an iterable object as its input.\n# Concatenate df1 and df2\ncity_coords = pd.concat([df1,df2])\n\ncity_coords\n\n\n\n\n\n           CITY      LAT      LONG\n0   Los Angeles  34.0522 -118.2437\n1        Bamako  12.6392    8.0029\n2  Johannesburg -26.2041   28.0473\n3         Cairo  30.0444   31.2357\n0     Cape Town -33.9249   18.4241\n1         Kyoto  35.0116  135.7681\n2        London  51.5074   -0.1278\n3    Cochabamba -17.4140  -66.1653\n\n\n\n\nBy default, pandas concatenates along the row axis, appending the values in df2 to df1 as new rows. However, notice that the original index values have been retained. Since these index labels do not contain useful information, it would be best to reset the index before proceeding. This can be done in one of two ways. First, we could have passed ignore_index=True to the pd.concat() function, telling pandas to ignore the index labels. Since we have already created a new variable, however, let‚Äôs use a more general method: df.reset_index().\n# Reset index in-place and delete old index\ncity_coords.reset_index(inplace=True, drop=True)\n\ncity_coords\n\n\n\n\n\n           CITY      LAT      LONG\n0   Los Angeles  34.0522 -118.2437\n1        Bamako  12.6392    8.0029\n2  Johannesburg -26.2041   28.0473\n3         Cairo  30.0444   31.2357\n4     Cape Town -33.9249   18.4241\n5         Kyoto  35.0116  135.7681\n6        London  51.5074   -0.1278\n7    Cochabamba -17.4140  -66.1653\n\n\n\n\nBy passing the optional inplace and drop parameters, we ensured that pandas would reset the index in-place (the default is to return a new DataFrame) and drop the old index (the default behaviour is to add the former index as a column).\nNow let‚Äôs consider the case of concatenating two DataFrames whose columns do not match. In this case, pandas will keep source rows and columns separate in the concatenated DataFrame, filling empty cells with NaN values:\ndf3 = pd.DataFrame([['USA', 87],['Mali', 350],['South Africa', 1753],['Egypt', 23],\n                    ['South Africa', 25],['Japan', 47],['UK', 11],['Bolivia', 2558]],\n                  columns=['COUNTRY', 'ELEV'])\n\n# Concatenate cities1 and df3\npd.concat([city_coords,df3])\n\n\n\n\n\n           CITY      LAT      LONG       COUNTRY    ELEV\n0   Los Angeles  34.0522 -118.2437           NaN     NaN\n1        Bamako  12.6392    8.0029           NaN     NaN\n2  Johannesburg -26.2041   28.0473           NaN     NaN\n3         Cairo  30.0444   31.2357           NaN     NaN\n4     Cape Town -33.9249   18.4241           NaN     NaN\n5         Kyoto  35.0116  135.7681           NaN     NaN\n6        London  51.5074   -0.1278           NaN     NaN\n7    Cochabamba -17.4140  -66.1653           NaN     NaN\n0           NaN      NaN       NaN           USA    87.0\n1           NaN      NaN       NaN          Mali   350.0\n2           NaN      NaN       NaN  South Africa  1753.0\n3           NaN      NaN       NaN         Egypt    23.0\n4           NaN      NaN       NaN  South Africa    25.0\n5           NaN      NaN       NaN         Japan    47.0\n6           NaN      NaN       NaN            UK    11.0\n7           NaN      NaN       NaN       Bolivia  2558.0\n\n\n\n\nInstead, we must pass axis=1 to the function to specify that we want to add the data in df3 as columns to the new DataFrame:\n# Concatenate along column axis\ncities = pd.concat([city_coords,df3], axis=1)\n\ncities\n\n\n\n\n\n           CITY      LAT      LONG       COUNTRY  ELEV\n0   Los Angeles  34.0522 -118.2437           USA    87\n1        Bamako  12.6392    8.0029          Mali   350\n2  Johannesburg -26.2041   28.0473  South Africa  1753\n3         Cairo  30.0444   31.2357         Egypt    23\n4     Cape Town -33.9249   18.4241  South Africa    25\n5         Kyoto  35.0116  135.7681         Japan    47\n6        London  51.5074   -0.1278            UK    11\n7    Cochabamba -17.4140  -66.1653       Bolivia  2558"
  },
  {
    "objectID": "interactive_sessions/4-1_pandas.html#practice-6.",
    "href": "interactive_sessions/4-1_pandas.html#practice-6.",
    "title": "Session 4-1: Pandas üêº",
    "section": "üìö  Practice 6. ",
    "text": "üìö  Practice 6. \n\n\nConcatenate df1 and df2 into a new DataFrame with all 9 rivers.\n\n\nCreate a new DataFrame rivers with the discharge, mouth, source, and continent information and add this to your DataFrame from (a) to produce a DataFrame with all of the data in the table below.\n\n\n\n\n\n\nRiver\n\n\nLength (\\text{km})\n\n\nDrainage area (\\text{km}^2)\n\n\nDischarge (\\text{m}^3/\\text{s})\n\n\nMouth\n\n\nSource\n\n\nContinent\n\n\n\n\nAmazon\n\n\n6400\n\n\n7,050,000\n\n\n209,000\n\n\nAtlantic Ocean\n\n\nRio Mantaro\n\n\nSouth America\n\n\n\n\nCongo\n\n\n4371\n\n\n4,014,500\n\n\n41,200\n\n\nAtlantic Ocean\n\n\nLualaba River\n\n\nAfrica\n\n\n\n\nYangtze\n\n\n6418\n\n\n1,808,500\n\n\n30,166\n\n\nEast China Sea\n\n\nJianggendiru Glacier\n\n\nAsia\n\n\n\n\nMississippi\n\n\n3730\n\n\n3,202,230\n\n\n16,792\n\n\nGulf of Mexico\n\n\nLake Itasca\n\n\nNorth America\n\n\n\n\nZambezi\n\n\n2574\n\n\n1,331,000\n\n\n3,400\n\n\nIndian Ocean\n\n\nMiombo Woodlands\n\n\nAfrica\n\n\n\n\nMekong\n\n\n4023\n\n\n811,000\n\n\n16,000\n\n\nSouth China Sea\n\n\nLasagongma Spring\n\n\nAsia\n\n\n\n\nMurray\n\n\n2508\n\n\n1,0614,69\n\n\n767\n\n\nSouthern Ocean\n\n\nAustralian Alps\n\n\nOceania\n\n\n\n\nRh√¥ne\n\n\n813\n\n\n98,000\n\n\n1,710\n\n\nMediterranean Sea\n\n\nRh√¥ne Glacier\n\n\nEurope\n\n\n\n\nCubango\n\n\n1056\n\n\n530,000\n\n\n475\n\n\nOkavango Delta\n\n\nBi√© Plateau\n\n\nAfrica\n\n\n\n\n\nData export\n\n\nWhile you will most likely use pandas DataFrames to manipulate data, perform statistical analyses, and visualize results within Python, you may encounter scenarios where it is useful to ‚Äúsave‚Äù a DataFrame with which you‚Äôve been working. Exporting data from pandas is analogous to importing it.\nLet‚Äôs take the example of the cities DataFrame we created in the last example. Now that we‚Äôve compiled GPS coordinates of various cities, let‚Äôs say we wanted to load these data into a GIS software application. We could export this DataFrame using df.to_csv() specifying the file name with the full file path as follows:\ncities.to_csv('./exports/cities.csv')"
  },
  {
    "objectID": "interactive_sessions/4-1_pandas.html#practice-7.",
    "href": "interactive_sessions/4-1_pandas.html#practice-7.",
    "title": "Session 4-1: Pandas üêº",
    "section": "üìö  Practice 7. ",
    "text": "üìö  Practice 7. \nUsing the example above, export your rivers DataFrame to a CSV file in a local data folder. Make sure you have created a data folder in your local copy of the course repository first!\n\nCongratulations!\nYou made it to the end of your first journey with Pandas. You deserve a warm, fuzzy reward‚Ä¶\n\n\nCode\nfrom IPython.display import YouTubeVideo\nfrom random import choice\nids=['sGF6bOi1NfA','Z98ZxYFsIWo', 'l73rmrLTHQc', 'D7xWXk5T3-g']\nYouTubeVideo(id=choice(ids),width=600,height=300)\n\n\n\n        \n        \n\n\n\n\n\n\nCode\n# IGNORE THIS CELL\nfrom IPython.core.display import HTML\ndef css_styling():\n    styles = open(\"./styles/exercises.css\", \"r\").read()\n    return HTML(styles)\ncss_styling()"
  },
  {
    "objectID": "interactive_sessions/0-1_ready_set_python.html",
    "href": "interactive_sessions/0-1_ready_set_python.html",
    "title": "Interactive Session 0-1: Getting ready to Python",
    "section": "",
    "text": "üè† Course Home | ‚û°Ô∏è Next Session |"
  },
  {
    "objectID": "interactive_sessions/0-1_ready_set_python.html#general-plan",
    "href": "interactive_sessions/0-1_ready_set_python.html#general-plan",
    "title": "Interactive Session 0-1: Getting ready to Python",
    "section": "General Plan",
    "text": "General Plan\nThere are many ways to set up your local machine to run maintainable python data science code. In addition to the usual need to build your code using version control (e.g.¬†GitHub), any strategy for local computation requires three critical components:\n\nA system for managing computing environments to ensure that code runs in a consistent environment.\nA system for managing python packages to ensure that code runs with consistent dependencies.\nA system for managing python code, which is usually an integrated development environment (IDE) in which editing and running code occur interactively.\n\nIn the past, these three components were often managed separately, but in recent years, there has been a trend towards integrating these components into a single system. For example, the RStudio IDE is a single system that manages all three components for R code. Similarly, the Data Spell IDE is a single system that manages all three components for python code.\nIn this class, we will use a combination of tools to manage these three components. We will use conda to manage computing environments and python packages, and we will use Visual Studio Code as our IDE.\nThese are both very popular tools in the python data science community, and they are both free and open source. However, there are many other options for managing computing environments, python packages, and code execution. We will discuss some of these options below, or you can skip ahead to the instructions for setting up your local machine."
  },
  {
    "objectID": "interactive_sessions/0-1_ready_set_python.html#managing-computing-environments-libraries-and-dependencies",
    "href": "interactive_sessions/0-1_ready_set_python.html#managing-computing-environments-libraries-and-dependencies",
    "title": "Interactive Session 0-1: Getting ready to Python",
    "section": "Managing Computing Environments, Libraries, and Dependencies",
    "text": "Managing Computing Environments, Libraries, and Dependencies\nThere are many options for managing computing environments. These days, a common method is to use containers, in which an entire computational system (including processes, memory, disk space) is spun up as an isolated service on your local (or remote) machine. Tools such as docker or python-specific shiv allow for isolated packaging and execution of python programs. Generally, these are better-suited for deployment of code on remote servers, but they can be used locally too. In this class, we‚Äôre not going to use containers. Instead we will use a more traditional approach to managing computing environments and packages.\nMore traditional approaches to managing computing environments and/or packages and dependencies include a suite of diverse tools. Some of these focus only on managing computing environments, while others focus on managing python packages. Some are designed to work with python only (e.g venv, conda), while others are designed to work with any programming language (e.g virtualenv). A few of the most popular options in each category are listed below.\n\nPackage management tools for python\npip is the standard package management tool for python. It is included with the standard python distribution, and it is the most widely used package management tool for python. However, it does not manage computing environments, so it is not as widely used as other tools below.\npipx is a tool developed by Brett Cannon that is designed to manage python packages. It has the advantage of being able to install any package hosted on PyPI, as well as packages hosted on github and even packages you‚Äôve made on your local machine. However, it does not manage computing environments, and it does not manage package dependencies.\n\n\nEnvironment management tools for python\nvirtualenv is a is a tool to create isolated Python environments. Since Python 3.3, a subset of it has been integrated into the standard library under the venv. While venv is sufficient to create virtual environments, it does not manage package dependencies, so it is also not as widely used as other tools below.\n\n\nTools that manage both environments and packages\nconda is a tool developed by Anaconda that is designed to manage computing environments. However, it also allows you to install packages, and even install binaries of packages directly without the need for local compilation. It also manages package dependencies, ensuring libraries are inter-operable.\nmamba is a new tool developed by the QuantStack team that is designed to be a drop-in replacement for conda. It allows you to create new environments, install packages, and even install binaries of packages directly without the need for local compilation. It also manages package dependencies, ensuring libraries are inter-operable. It is designed to be faster than conda and to use fewer resources. It is also designed to be more compatible with pip than conda. However, it is not as widely used as conda and it is not as well-supported by IDEs.\npoetry is another new tool developed by the Python Packaging Authority team that is designed to manage computing environments and python packages. Advantages of poetry include a simplified approach to dependency management and a simplified approach to building and packaging your own code for distribution. Disadvantages include a lack of support for conda environments and a lack of support for pip packages that are not hosted on PyPI (although this may change in the future)."
  },
  {
    "objectID": "interactive_sessions/0-1_ready_set_python.html#instructions-installing-configuring-conda",
    "href": "interactive_sessions/0-1_ready_set_python.html#instructions-installing-configuring-conda",
    "title": "Interactive Session 0-1: Getting ready to Python",
    "section": "Instructions: Installing & configuring conda",
    "text": "Instructions: Installing & configuring conda"
  },
  {
    "objectID": "interactive_sessions/0-1_ready_set_python.html#instruction-launching-jupyter-and-testing-your-environment",
    "href": "interactive_sessions/0-1_ready_set_python.html#instruction-launching-jupyter-and-testing-your-environment",
    "title": "Interactive Session 0-1: Getting ready to Python",
    "section": "Instruction: Launching Jupyter and testing your environment",
    "text": "Instruction: Launching Jupyter and testing your environment"
  },
  {
    "objectID": "interactive_sessions/0-1_ready_set_python.html#instructions-installing-and-confuguring-vscode",
    "href": "interactive_sessions/0-1_ready_set_python.html#instructions-installing-and-confuguring-vscode",
    "title": "Interactive Session 0-1: Getting ready to Python",
    "section": "Instructions: Installing and confuguring VSCode",
    "text": "Instructions: Installing and confuguring VSCode"
  },
  {
    "objectID": "interactive_sessions/99_hello_world.html",
    "href": "interactive_sessions/99_hello_world.html",
    "title": "Interactive Session 0-2: Hello, World!",
    "section": "",
    "text": "Python is a general-purpose, open-source programming language with diverse capabilities. While it is often used for high-level applications and complex tasks, Python‚Äôs object-oriented structure and clean syntax make it straightforward to learn.\nIn addition to Python scripts (.py files), many data scientists use Jupyter Notebooks (.ipynb files) for experimentation and visualization.\nNotebooks allow for rapid code evaluation and ease of use. For this reason, sessions have been designed in Jupyter Notebook format, though you are encouraged to explore Python scripts as well.\nThis first session will briefly introduce a few basic Python features."
  },
  {
    "objectID": "interactive_sessions/99_hello_world.html#introduction-to-python",
    "href": "interactive_sessions/99_hello_world.html#introduction-to-python",
    "title": "Interactive Session 0-2: Hello, World!",
    "section": "Introduction to Python",
    "text": "Introduction to Python\n\n\n\nPrint statements\nThe most basic function in Python is the print() function, which simply prints out a line.\nprint( 'This is a print statement.' )\n\n &gt;&gt;&gt;  This is a print statement.\n\n\n‚ñ∂Ô∏è Run the cell below.\n\n\n\nCode\nprint( 'This is a print statement.' )\n\n\nThis is a print statement.\n\n\n\nüìö Practice 1. In the following cell, use the print() command to print the line ‚ÄúHello, World!‚Äù)\n\n\n\n\n\nUser input\nThe input() command is an important Python function that can be used in a program to prompt for user input."
  },
  {
    "objectID": "interactive_sessions/99_hello_world.html#try-it.",
    "href": "interactive_sessions/99_hello_world.html#try-it.",
    "title": "Interactive Session 0-2: Hello, World!",
    "section": "‚úèÔ∏è Try it.",
    "text": "‚úèÔ∏è Try it.\nRun the following cell to see how the input() function works, inputing your first name when prompted.\n\n\nCode\nname = input( 'What is your first name? ' )\nprint(name)\n\n\n\n\n\n\n\n\nComments\nIn programming, a comment is a piece of text that the computer ignores when running the program. Comments are usually used at the top of a program to provide a description of the program, name and information of the author, dependencies, etc., as well as throughout the script to explain lines of code. Commenting is a critical element of proper coding etiquette.\nThere are two types of comments: in-line comments and block comments. In-line comments are used to provide context at the end of a line of code, while block comments come before a ‚Äúblock‚Äù or chunk of code.\n\nüêç &lt;b&gt;Note.&lt;/b&gt;\nIn Python, there are two ways of writing comments. For in-line comments and most short comments, the &lt;span style=\"font-weight:bold\"&gt; # &lt;/span&gt; character is used. For large block comments, such as those at the beginning of a file or function, three quotations &lt;span style=\"font-weight:bold\"&gt;  '''  &lt;/span&gt; are used to enclose comments. Both single and double quotations are interpreted the same, as long as they are not used together.\n\nThe following example is a function (a type of Python object we will cover later on) that uses both methods of commenting. As you can see, without the comments, it would be difficult to understand what the function does, what parameters it takes, and what the intermediate steps are to reach the end.\ndef homo_affine_matrix(p, p_prime):\n    '''\n    Finds the unique homogeneous affine transformation that \n    transforms a set of 3 points from one coordinate system \n    to that of a second set of points in 3-D space\n    \n    Parameters\n    ----------\n    p : array\n        original set of points as a row vector: ((p1, p2, p3))\n    p_prime : array\n        Transformed points as a row vector: ((p1_prime, p2_prime, p3_prime))\n    \n    Returns\n    -------\n    array\n        4 x 3 affine transformation matrix.\n        \n    Source: Adapted by B. Morgan from https://math.stackexchange.com/a/222170 (robjohn)\n    '''\n    \n    # Construct intermediate matrix\n    Q       = p[1:]       - p[0]\n    Q_prime = p_prime[1:] - p_prime[0]\n\n    # Calculate rotation matrix\n    R = np.dot(np.linalg.inv(np.row_stack((Q, np.cross(*Q)))),\n               np.row_stack((Q_prime, np.cross(*Q_prime))))\n\n    # Calculate translation vector\n    t = p_prime[0] - np.dot(p[0], R)\n\n    # Calculate affine transformation matrix\n    return np.column_stack((np.row_stack((R, t)),\n                            (0, 0, 0, 1)))\n \n\n‚ñ∂Ô∏è Run the cell below.\n\n\n\nCode\n# This comment will not be read by the computer.\n2 + 2 \n\n\n4"
  },
  {
    "objectID": "interactive_sessions/conda_setup.html#installing-conda",
    "href": "interactive_sessions/conda_setup.html#installing-conda",
    "title": "conda installation and usage",
    "section": "Installing conda",
    "text": "Installing conda\n\nInstalling Miniconda on Windows:\n\nDownload Miniconda Installer:\n\nGo to the Miniconda download page.\nDownload the Miniconda installer for Windows. Make sure to choose the appropriate version (Python 3.x) based on your preference.\n\nRun the Installer:\n\nLocate the downloaded installer and double-click on it to start the installation process.\nFollow the on-screen instructions. Here are some recommendations for the installation:\n\nChoose ‚ÄúJust Me‚Äù unless you‚Äôre sure about the ‚ÄúAll Users‚Äù option.\nUse the default installation location unless you have a specific need to change it.\nIt‚Äôs recommended to check the box ‚ÄúAdd Miniconda to my PATH environment variable‚Äù for easier command-line use. However, be aware that this can interfere with other Python installations.\n\n\nVerify Installation:\n\nOpen the Command Prompt (you can search for cmd in the Windows search bar).\nType conda --version and press Enter. This should display the version of conda installed, confirming that the installation was successful.\n\n\n\n\nInstalling Miniconda on MacOS:\n\nDownload Miniconda Installer:\n\nGo to the Miniconda download page.\nDownload the Miniconda installer for MacOS. Make sure to choose the appropriate version (Python 3.x) based on your preference.\n\nRun the Installer:\n\nOpen a terminal.\nNavigate to the directory where the installer was downloaded.\nMake the installer executable with the command:\nchmod +x Miniconda3-latest-MacOSX-x86_64.sh\nStart the installation process with:\n./Miniconda3-latest-MacOSX-x86_64.sh\nFollow the on-screen instructions to complete the installation.\n\nVerify Installation:\n\nIn the terminal, type:\nconda --version\nThis should display the version of conda installed, confirming that the installation was successful.\n\n\n\n\nInstalling Miniconda on Linux:\n\nDownload Miniconda Installer:\n\nGo to the Miniconda download page.\nDownload the Miniconda installer for Linux. Make sure to choose the appropriate version (Python 3.x) based on your preference.\n\nRun the Installer:\n\nOpen a terminal.\nNavigate to the directory where the installer was downloaded.\nMake the installer executable with the command:\nchmod +x Miniconda3-latest-Linux-x86_64.sh\nStart the installation process with:\n./Miniconda3-latest-Linux-x86_64.sh\nFollow the on-screen instructions to complete the installation.\n\nVerify Installation:\n\nIn the terminal, type:\nconda --version\nThis should display the version of conda installed, confirming that the installation was successful."
  },
  {
    "objectID": "interactive_sessions/conda_setup.html#initializing-conda-for-your-shell",
    "href": "interactive_sessions/conda_setup.html#initializing-conda-for-your-shell",
    "title": "conda installation and usage",
    "section": "Initializing Conda for Your Shell:",
    "text": "Initializing Conda for Your Shell:\nOnce you‚Äôve identified your shell, you can initialize conda for it:\nconda init &lt;shell_name&gt;\nReplace &lt;shell_name&gt; with the name of the shell (bash, zsh, cmd, powershell, etc.)\nNote: If for any reason conda is not recognized as a command, it‚Äôs possible the conda binary path isn‚Äôt in your system‚Äôs PATH variable. You‚Äôll need to add it manually, and the process will depend on which OS and shell you‚Äôre using.\n\nUpdating conda\nYou won‚Äôt need to do this very often, but it‚Äôs a good idea to update conda periodically to ensure you have the latest version.\n\nWindows:\n\nOpen the a PowerShell from the Start Menu.\nRun the following command to update conda: bash      conda update conda\n\nMacOS/Linux:\n\nOpen the Terminal.\nRun the following command to update conda: bash      conda update conda"
  },
  {
    "objectID": "interactive_sessions/conda_setup.html#creating-a-new-conda-environment",
    "href": "interactive_sessions/conda_setup.html#creating-a-new-conda-environment",
    "title": "conda installation and usage",
    "section": "Creating a new conda environment",
    "text": "Creating a new conda environment\nConda can create new environments using a markup language specification called yaml. We will use an environment file created for our course to create an eds217_2023 environment on your local machine.\nYou can create a new environment using the following command:\n    conda create -n my_cool_environment\nThis will create a new environment named my_cool_environment\nOften you will want to specify the python that you‚Äôd like the environment to use. You do that by adding a python=&lt;version&gt;. Here is a command that would create an environment that had the python 3.10 interpreter installed:\n    conda create -n my_py310_environment python=3.10\nFinally, you probably noticed that conda prompts you for permission before installing new packages. Sometimes you need to pay attention to these prompts, especially when you are adding packages to very complex environments. But on creation, you may want to speed things up. The --yes flag allows you to bypass approval and go straight to installation:\n    conda create --yes -n my_py310_environment python=3.10"
  },
  {
    "objectID": "interactive_sessions/conda_setup.html#activating-an-environment",
    "href": "interactive_sessions/conda_setup.html#activating-an-environment",
    "title": "conda installation and usage",
    "section": "Activating an environment",
    "text": "Activating an environment\nYou can activate an environment using the following command:\n    conda activate my_cool_environment"
  },
  {
    "objectID": "interactive_sessions/conda_setup.html#deactivating-an-environment",
    "href": "interactive_sessions/conda_setup.html#deactivating-an-environment",
    "title": "conda installation and usage",
    "section": "Deactivating an environment",
    "text": "Deactivating an environment\nYou can deactivate an environment using the following command:\n    conda deactivate\nThere is no need to deactivate an environment before activating a different environment."
  },
  {
    "objectID": "interactive_sessions/conda_setup.html#removing-an-environment-generally-you-wont-want-to-do-this",
    "href": "interactive_sessions/conda_setup.html#removing-an-environment-generally-you-wont-want-to-do-this",
    "title": "conda installation and usage",
    "section": "Removing an environment (generally, you won‚Äôt want to do this!)",
    "text": "Removing an environment (generally, you won‚Äôt want to do this!)\nYou can remove an environment using the following command:\n    conda env remove -n my_cool_environment"
  },
  {
    "objectID": "interactive_sessions/conda_setup.html#listing-all-available-environments",
    "href": "interactive_sessions/conda_setup.html#listing-all-available-environments",
    "title": "conda installation and usage",
    "section": "Listing all available environments",
    "text": "Listing all available environments\nEventually you are going to have a lot of different environments, and you may not remember all of them, or the exact name of the one you are looking to activate. In this case, conda env list (or conda info --envs) is your friend.\n    conda env list\nThis will display a list of all the conda environments you have created. The currently active environment (if any) will be marked with an asterisk (*) next to its name."
  },
  {
    "objectID": "interactive_sessions/conda_setup.html#prerequisites",
    "href": "interactive_sessions/conda_setup.html#prerequisites",
    "title": "conda installation and usage",
    "section": "Prerequisites:",
    "text": "Prerequisites:\n\nEnsure you have already installed conda (see above)."
  },
  {
    "objectID": "interactive_sessions/conda_setup.html#instructions",
    "href": "interactive_sessions/conda_setup.html#instructions",
    "title": "conda installation and usage",
    "section": "Instructions:",
    "text": "Instructions:\n\n1. Fork the eds217_2023 Repository:\nGo to the eds217_2023 repository and click the ‚ÄúFork‚Äù button in the upper right corner of the page. This will create a copy of the repository in your GitHub account.\n\n\n1. Clone Your Forked Repository:\nOpen a terminal (Command Prompt on Windows, Terminal on MacOS/Linux) and navigate to the directory where you want to clone the repository.\ngit clone https://github.com/YOUR_USERNAME/eds217_2023.git\ncd eds217_2023\nReplace YOUR_USERNAME with your GitHub username.\n\n\n2. Create the Conda Environment:\n\nWindows:\nconda env create -f environment.yml\nconda activate eds217_2023\n\n\nMacOS/Linux:\nconda env create -f environment.yml\nsource activate eds217_2023\n\n\n\n3. Set Up the eds217_2023 Environment as an ipykernel:\nIn order to make sure our python environment is used when running code within an interactive editor such as a Jupyter notebook or in an IDE, you need ensure that the correct kernel is used to run our code. A python kernel is a program that runs and introspects the user‚Äôs code. Interactive tools such as Jupyter and IDEs use the kernel to execute code when you run a cell in the editor. `ipykernel`` is a tool that allows you to use specific python kernels when running code interactively within Jupyter notebooks and IDEs.\nAfter activating the eds217_2023 environment, run the following command:\npython -m ipykernel install --user --name eds217_2023 --display-name \"Python (eds217_2023)\"\nThis will allow you to select the eds217_2023 environment as a kernel in Jupyter or in your IDE, which we will install next."
  },
  {
    "objectID": "interactive_sessions/0-2_hello_data_science.html",
    "href": "interactive_sessions/0-2_hello_data_science.html",
    "title": "Interactive Session 0-2: ‚ÄúHello, Python Data Science‚Äù",
    "section": "",
    "text": "‚¨ÖÔ∏è Previous Session | üè† Course Home | ‚û°Ô∏è Next Session|"
  },
  {
    "objectID": "interactive_sessions/0-2_hello_data_science.html#setup",
    "href": "interactive_sessions/0-2_hello_data_science.html#setup",
    "title": "Interactive Session 0-2: ‚ÄúHello, Python Data Science‚Äù",
    "section": "Setup",
    "text": "Setup\n\nFork and clone this repo\nCheck out the project structure & files"
  },
  {
    "objectID": "interactive_sessions/0-2_hello_data_science.html#data-science-101",
    "href": "interactive_sessions/0-2_hello_data_science.html#data-science-101",
    "title": "Interactive Session 0-2: ‚ÄúHello, Python Data Science‚Äù",
    "section": "Data Science 101",
    "text": "Data Science 101\n\nWe‚Äôll run through each line in the toolik_airtemp_summary.ipynb, which is translated from your first EDS221 assignment. This will allow us to understand where what it‚Äôs doing, and see for the first time, some of the most important tools in our python toolkit.\nSimilarly, we‚Äôll create a new notebook from scratch & work through one more example to read in a different dataset.\n\nOur second example will be to:\n\nRead in a different set of data from a long-term water quality study at Elkhorn Slough, CA.\nExplore the imported data\nClean up the dataset names\nCreate a plot of water quality variables at varying sites, do some customization\nExport the graph to /figs"
  },
  {
    "objectID": "interactive_sessions/0-2_hello_data_science.html#data-sources",
    "href": "interactive_sessions/0-2_hello_data_science.html#data-sources",
    "title": "Interactive Session 0-2: ‚ÄúHello, Python Data Science‚Äù",
    "section": "Data sources",
    "text": "Data sources\nAll datasets are collected and provided by scientists with the Toolik Station Long Term Ecological Research (LTER) site, Alaska.\nToolik Station Meteorological Data: toolik_weather.csv Shaver, G. 2019. A multi-year DAILY weather file for the Toolik Field Station at Toolik Lake, AK starting 1988 to present. ver 4. Environmental Data Initiative. https://doi.org/10.6073/pasta/ce0f300cdf87ec002909012abefd9c5c (Accessed 2021-08-08).\nToolik Lake Chlorophyll: toolik_chlorophyll.csv Miller, M. 2014. Chlorophyll A, and primary productivity of Toolik lake , Arctic LTER 1975 to 1988, Toolik Filed Station, Alaska. ver 5. Environmental Data Initiative. https://doi.org/10.6073/pasta/6738024bf0174f73b3f74486f43d1059 (Accessed 2021-08-08).\nToolik fish: toolik_fish.csv Budy, P., C. Luecke, and M. McDonald. 2020. Fish captures in lakes of the Arctic LTER region Toolik Field Station Alaska from 1986 to present. ver 6. Environmental Data Initiative. https://doi.org/10.6073/pasta/d0a9358f783339821b82510eb8c61b45 (Accessed 2021-08-08).\nElkhorn Slough Water Quality Data: elkhorn_slough_water_quality.csv Arora, B., Venkatesh, S., Dwivedi, D., Vezhapperambu, S., & Ramesh, M. (2022). Water Quality Parameters at the Elkhorn Slough National Estuarine Research Reserve (ESNERR). ESS-DIVE. doi:10.15485/1875297, version: ess-dive-941fcc36279c83f-20230407T151617854348.\nhttps://knb.ecoinformatics.org/view/doi%3A10.15485%2F1875297"
  },
  {
    "objectID": "interactive_sessions/1-2_lists.html",
    "href": "interactive_sessions/1-2_lists.html",
    "title": "Session 1-2: Lists + Indexing",
    "section": "",
    "text": "‚¨ÖÔ∏è Previous Session | üè† Course Home | ‚û°Ô∏è Next Session |\nPython has four collection data types, the most common of which is the list. This session introduces lists and a few of the important list operations. We will also cover indexing, a key feature of programming."
  },
  {
    "objectID": "interactive_sessions/1-2_lists.html#lists",
    "href": "interactive_sessions/1-2_lists.html#lists",
    "title": "Session 1-2: Lists + Indexing",
    "section": "Lists",
    "text": "Lists\nA list is a Python object used to contain multiple values. Lists are ordered and changeable. They are defined as follows:\nnum_list = [4, 23, 654, 2, 0, -12, 4391]\nstr_list = ['energy', 'water', 'carbon']\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Define list variables\nnum_list = [4, 23, 654, 2, 0, -12, 4391]\nstr_list = ['energy', 'water', 'carbon']\n\n\nWhile you can create lists containing mixed data types, this is not usually recommended.\nThe len() command returns the length of the list.\nlen(str_list)\n&gt;&gt;&gt; 3\nThe min() and max() commands are used to find the minimum and maximum values in a list. For a list of strings, this corresponds to the alphabetically first and last elements.\nmin(str_list)\n&gt;&gt;&gt; 'carbon'\n\nmax(str_list)\n&gt;&gt;&gt; 'water'\n\n‚úèÔ∏è &lt;b&gt; Try it. &lt;/b&gt; \nUse the &lt;code&gt;len()&lt;/code&gt;, &lt;code&gt;min()&lt;/code&gt;, and &lt;code&gt;max()&lt;/code&gt; commands to find the length, minimum, and maximum of &lt;code&gt;num_list&lt;/code&gt;.\n\n\n\nCode\n# Find the length of num_list\n\n# Minimum value of num_list\n\n# Maximum value of num_list\n\n\n\n\n\nIndexing\nThe index is used to reference a value in an iterable object by its position. To access an element in a list by its index, use square brackets [].\n\nüêç &lt;b&gt;Note.&lt;/b&gt; Python is zero-indexed. This means that the first element in the list is 0, the second is 1, and so on. The last element in a list with $n$ elements is $n - $1.\n\nnum_list[2]\n&gt;&gt;&gt; 654\nYou can also access an element based on its position from the end of the list.\nnum_list[-2]\n&gt;&gt;&gt; -12\n\n‚úèÔ∏è &lt;b&gt; Try it. &lt;/b&gt; \nFind the 2nd element in &lt;code&gt;str_list&lt;/code&gt; in two different ways. Remember that Python is zero-indexed!\n\n\n\nCode\n# Option 1\n\n# Option 2\n\n\nAccessing a range of values in a list is called slicing. A slice specifies a start and an endpoint, generating a new list based on the indices. The indices are separated by a :.\n\nüêç &lt;b&gt;Note.&lt;/b&gt;  The endpoint index in the slice is &lt;i&gt;exclusive&lt;/i&gt;. To slice to the end of the list, omit an endpoint.\n\nnum_list[2:6]\n&gt;&gt;&gt; [654, 2, 0, -12]\n\nnum_list[0:4]   \n&gt;&gt;&gt; [4, 23, 654, 2]\n\nnum_list[:4]    \n&gt;&gt;&gt; [4, 23, 654, 2]\n\nnum_list[-6:-1] \n&gt;&gt;&gt; [23, 654, 2, 0,-12]\nIt is also possible to specify a step size, i.e.¬†[start:stop:step]. A step size of 1 would select every element, 2 would select every other element, etc.\nnum_list[0:4:2]  \n&gt;&gt;&gt; [4, 654]\n\nnum_list[::2]    \n&gt;&gt;&gt;[4, 654, 0, 4391]\nA step of -1 returns the list in reverse.\nnum_list[::-1]\n&gt;&gt;&gt; [4391, -12, 0, 2, 654, 23, 4]\n\nüìö  &lt;b&gt; Practice 1. &lt;/b&gt; \nDefine a new list of &lt;b&gt;floats&lt;/b&gt; with &lt;b&gt;8 elements&lt;/b&gt; called &lt;code&gt;my_list&lt;/code&gt;. \n\nFind the 5th element in your list.\nCreate a new list containing every other value in your original list.\nUsing slicing and two different methods of indexing, remove the first and last values in your list.\n\n\n\n\nCode\n# Define a new list called my_list.\n\n# Find the 5th element.\n\n# New list with every other value.\n\n# Remove first and last values. \n\n\nLike lists, strings can also be indexed using the same notation. This can be useful for many applications, such as selecting files in a certain folder for import based on their names or extension.\nword_str = 'antidisestablishmentarianism'\n\nword_str[14]\n&gt;&gt;&gt; 's'\n\nword_str[::3]\n&gt;&gt;&gt; 'aistlhnrnm'\n\nüìö  &lt;b&gt; Practice 2. &lt;/b&gt; \nUse indexing to extract the second letter of the third element ('a') in &lt;code&gt;str_list&lt;/code&gt;.\n\n\n\n\n\nList Operations\nElements can be added to a list using the command list.append().\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\ncolors = ['red', 'blue', 'green', 'black', 'white']\ncolors.append('pink')\nprint(colors)\n\n\nYou can add an element to a list in a specific position using the command list.insert().\ncolors.insert(4, 'purple')\nprint(colors)\n&gt;&gt;&gt; ['red', 'blue', 'green', 'black', 'purple', 'white', 'pink']\n\n‚úèÔ∏è &lt;b&gt; Try it. &lt;/b&gt; \nAdd &lt;code&gt;'purple'&lt;/code&gt; to the list &lt;code&gt;colors&lt;/code&gt; between &lt;code&gt;'green'&lt;/code&gt; and &lt;code&gt;'black'&lt;/code&gt;.\n\nThere are multiple ways to remove elements from a list. The commands list.pop() and del remove elements based on indices.\ncolors.pop()       # removes the last element\ncolors.pop(2)      # removes the third element\ndel colors[2]      # removes the third element\ndel colors[2:4]    # removes the third and fourth elements\nThe command list.remove() removes an element based on its value.\ncolors.remove('red')\nprint(colors)\n&gt;&gt;&gt; ['blue', 'green', 'black', 'purple', 'white', 'pink']\n\n‚úèÔ∏è &lt;b&gt; Try it. &lt;/b&gt; \nRemove &lt;code&gt;'pink'&lt;/code&gt; and &lt;code&gt;'purple'&lt;/code&gt; from &lt;code&gt;colors&lt;/code&gt;, using &lt;code&gt;del&lt;/code&gt; for one of the strings and &lt;code&gt;list.remove()&lt;/code&gt; for the other.\n\nYou can sort the elements in a list (numerically or alphabetically) in two ways. The first uses the command list.sort().\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nrand_list = [5.1 , 3.42 , 3.333 , 100.4 , 0.5 , 26.0 , 7.44 , 5.8 , 39.0]\nrand_list.sort()\nprint(rand_list)\n\n\nSetting reverse=True within this command sorts the list in reverse order:\nrand_list = [5.1 , 3.42 , 3.333 , 100.4 , 0.5 , 26.0 , 7.44 , 5.8 , 39.0]\nrand_list.sort(reverse=True)\nprint(rand_list)\n&gt;&gt;&gt; [100.4, 39.0, 26.0, 7.44, 5.8, 5.1, 3.42, 3.333, 0.5]\nSo far, all of the list commands we‚Äôve used have been in-place operators. This means that they perform the operation to the variable in place without requiring a new variable to be assigned. By contrast, standard operators do not change the original list variable. A new variable must be set in order to retain the operation.\n\n‚úèÔ∏è &lt;b&gt; Try it. &lt;/b&gt; \nVerify that &lt;code&gt;rand_list&lt;/code&gt; was, in fact, sorted in place by using the &lt;code&gt;min()&lt;/code&gt; and &lt;code&gt;max()&lt;/code&gt; functions to determine the minmum and maximum values in the list and printing the first and last values in the list.\n\n\n\nCode\n# Print the min and max values in rand_list.\n\n# Print the first and last values in rand_list.\n\n\nThe other method of sorting a list is to use the sorted() command, which does not change the original list. Instead, the sorted list must be assigned to a new variable.\nrand_list = [5.1 , 3.42 , 3.333 , 100.4 , 0.5 , 26.0 , 7.44 , 5.8 , 39.0]\nsorted_list = sorted(rand_list)\nprint(rand_list[0])\nprint(sorted_list[0])\n&gt;&gt;&gt; 5.1\n    0.5\nTo avoid changing the original variable when using an in-place operator, it is wise to create a copy. There are multiple ways to create copies of lists, but it is important to know the difference between a true copy and a view. A view of a list can be created as follows:\nstr_list = ['energy', 'water', 'carbon']\nstr_list_view = str_list\nAny in-place operation performed on str_list_view will also be applied to str_list. To avoid this, create a copy of str_list using any of the following methods:\nstr_list_copy = str_list.copy()\n# or\nstr_list_copy = str_list[:]\n# or\nstr_list_copy = list(str_list)\n\nüìö  &lt;b&gt; Practice 3. &lt;/b&gt; \nCreate a copy of &lt;code&gt;my_list&lt;/code&gt;, which you assigned above. \n\nUsing indexing or list operators, remove the first and last elements of your copied list.\nSort both the original list and the copied list in reverse order.\nUse the len() function and a boolean operator to determine which list is longer.\n\n\n\n\nCode\n# Create a copy of mylist.\n\n# Sort both lists from largest to smallest.\n\n# Determine which list is longer.\n\n\nIn addition to adding single elements to a list using list.append() or list.insert(), multiple elements can be added to a list at the same time by adding multiple lists together.\nrainbow  = ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet']\nshades = ['coral', 'chartreuse', 'cyan', 'navy']\nprint( rainbow + shades )\n&gt;&gt;&gt; ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet', 'coral', 'chartreuse', 'cyan', 'navy']\n\nüìö  &lt;b&gt; Practice 4. &lt;/b&gt; \nAdd &lt;code&gt;rand_list&lt;/code&gt; and &lt;code&gt;my_list&lt;/code&gt; together in a new list called &lt;code&gt;float_list&lt;/code&gt;. Print the result.\n\nSingle lists can be repeated by multiplying by an integer.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nstr_list2 = str_list * 2\nnum_list4 = num_list * 4\nprint( str_list2 )\nprint( num_list4 )\n\n\n\n\nGenerating sequential lists\nSequential lists are valuable tools, particularly for iteration, which we will explore in the next session. The range() function is used to create an iterable object based on the size of an integer argument.\nrange(4)\n&gt;&gt;&gt; range(0, 4)\nTo construct a sequential list from the range() object, use the list() function.\nlist(range(4))\n&gt;&gt;&gt; [0, 1, 2, 3]\nUsing multiple integer arguments, the range() function can be used to generate sequential lists between two bounds: range(start, stop [, step]).\n\nüêç &lt;b&gt;Note.&lt;/b&gt; \nLike indexing, all Python functions using &lt;span style=\"font-style: italic\"&gt; start &lt;/span&gt; and &lt;span style=\"font-style: italic\"&gt; stop &lt;/span&gt; arguments, the &lt;span style=\"font-style: italic\"&gt; stop &lt;/span&gt; value is &lt;span style=\"font-weight: bold\"&gt; exclusive &lt;/span&gt;.\n\nrange_10 = list(range(1,11))\nodds_10 = list(range(1,11,2))\nprint(range_10)\nprint(odds_10)\n&gt;&gt;&gt; [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n    [1, 3, 5, 7, 9]\n\nüìö  &lt;b&gt; Practice 5. &lt;/b&gt; \nUse the &lt;code&gt;range()&lt;/code&gt; function to construct a list of all hundreds (e.g. 100, 200, etc.) between 0 and 1000, inclusive.\n\n\n\nCode\n# Construct a list of hundreds from 0 to 1000\n\n# Print your list"
  },
  {
    "objectID": "interactive_sessions/2-1_control_flow_statements.html",
    "href": "interactive_sessions/2-1_control_flow_statements.html",
    "title": "Session 2-1: Control Flow Statements",
    "section": "",
    "text": "‚¨ÖÔ∏è Previous Session | üè† Course Home | ‚û°Ô∏è Next Session |\nThe control flow of a program refers to the order in which its code is executed. In Python, control flow is regulated by conditional statements, loops, and functions. This session will cover if statements, for loops, and while loops; functions will be covered in a later session."
  },
  {
    "objectID": "interactive_sessions/2-1_control_flow_statements.html#try-it.",
    "href": "interactive_sessions/2-1_control_flow_statements.html#try-it.",
    "title": "Session 2-1: Control Flow Statements",
    "section": "‚úèÔ∏è Try it.",
    "text": "‚úèÔ∏è Try it.\nRun the following cell, changing the value of x to be negative or positive to demonstrate how the if statement works.\n\n\nCode\n# Define x\nx = 4\n# Evaluate condition\nif x &gt; 0:\n    print('x is a positive number.')\n\n\nThe if statement can be used in conjunction with the else command to instruct the program to do something different if the condition is not met.\ny = -3218\nif y &gt; 0:\n    print('y is a positive number.')\nelse:\n    print('y is not a positive number.')\n\n &gt;&gt;&gt;  y is not a positive number.\n\nTo evaluate multiple conditions, add the elif command after the if statement. Infinite elif statements can be included.\ny = -3218\nif y &gt; 0:\n    print('y is a positive number.')\nelif y == 0:\n    print('y = 0')\nelse:\n    print('y is a negative number.')\n\n &gt;&gt;&gt;  y is a negative number."
  },
  {
    "objectID": "interactive_sessions/2-1_control_flow_statements.html#try-it.-1",
    "href": "interactive_sessions/2-1_control_flow_statements.html#try-it.-1",
    "title": "Session 2-1: Control Flow Statements",
    "section": "‚úèÔ∏è Try it.",
    "text": "‚úèÔ∏è Try it.\n\n\nCode\n# Experiment with if, elif, and else statements.\n\n\n\n\n\n Operators \n\nThere are several operators that can be used within if statements to evaluate more complex conditions. The in operator is used to check if an object exists within an iterable object container, such as a list.\naminoacids = ['histidine', 'isoleucine', 'leucine', 'lysine', 'methionine', \n              'phenylalanine', 'threonine', 'tryptophan', 'valine', 'alanine', \n              'arginine', 'asparagine', 'aspartate', 'cysteine', 'glutamate', \n              'glutamine', 'glycine', 'proline', 'serine', 'tyrosine']\n\norg_compound = 'cysteine'\n\nif org_compound in aminoacids:\n    print(org_compound.capitalize() + ' is an amino acid.' )\nelse:\n    print(org_compound.capitalize() + ' is not an amino acid.')\n\n &gt;&gt;&gt;  Cysteine is an amino acid.\n\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# List of amino acids\naminoacids = ['histidine', 'isoleucine', 'leucine', 'lysine', 'methionine', \n              'phenylalanine', 'threonine', 'tryptophan', 'valine', 'alanine', \n              'arginine', 'asparagine', 'aspartate', 'cysteine', 'glutamate', \n              'glutamine', 'glycine', 'proline', 'serine', 'tyrosine']\n\norg_compound = 'adenine'\nif org_compound in aminoacids:\n    print(org_compound.capitalize() + ' is an amino acid.' )\nelse:\n    print(org_compound.capitalize() + ' is not an amino acid.')\n\n\nThe and and or operators can be used to build more complex if statements based on multiple expressions. The and operator is used to specify that multiple conditions must be satisfied.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nessential = ['histidine', 'lysine', 'threonine', 'tryptophan', \n             'valine', 'methionine', 'leucine', 'phenylalanine']\naromatic = ['phenylalanine', 'tryptophan', 'tyrosine']\n\naa = 'tyrosine'\n\nif aa in aromatic and aa in essential:\n    print(aa.capitalize() + ' is an essential, aromatic amino acid.')\nelif aa in essential:\n    print(aa.capitalize() + ' is an essential, non-aromatic amino acid.')\nelif aa in aromatic:\n    print(aa.capitalize() + ' is a non-essential, aromatic amino acid.')\nelif aa in aminoacids:\n    print(aa.capitalize() + 'is a non-essential amino acid.')\nelse:\n    print(aa.capitalize() + ' is not an amino acid.')\n\n\nThe or operator is used to write an if statement where one of multiple conditions must be met.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\ncharged = ['arginine', 'lysine', 'aspartate', 'glutamate']\npolar_un = ['glutamine', 'asparagine', 'histidine', 'serine', 'threonine', 'tyrosine', 'cysteine']\n\nif aa in charged or polar_un:\n    print(aa.capitalize() + ' is a polar amino acid.')\nelif aa in aminoacids:\n    print(aa.capitalize() + ' is a non-polar amino acid.')\nelse:\n    print(aa.capitalize() + ' is not an amino acid.')\n\n\n\n Nested if statements \n\nif statements can build on one another to perform specific actions based on each condition by nesting if statements. For example, the previous cell could be re-written as follows:\nif aa in aminoacids:\n    if aa in charged or polar_un:\n        print(aa.capitalize() + ' is a polar amino acid.')\n    else:\n        print(aa.capitalize() + ' is a non-polar amino acid.')\nelse:\n    print(aa.capitalize() + ' is not an amino acid.')    \nWhile this is a trivial example, nested if statements can be quite useful when inside a program as they allow the program to skip to the end if the first condition is not satisfied."
  },
  {
    "objectID": "interactive_sessions/2-1_control_flow_statements.html#try-it.-2",
    "href": "interactive_sessions/2-1_control_flow_statements.html#try-it.-2",
    "title": "Session 2-1: Control Flow Statements",
    "section": "‚úèÔ∏è Try it.",
    "text": "‚úèÔ∏è Try it.\n\n\nCode\n# Experiment with nested if statements."
  },
  {
    "objectID": "interactive_sessions/2-1_control_flow_statements.html#loops",
    "href": "interactive_sessions/2-1_control_flow_statements.html#loops",
    "title": "Session 2-1: Control Flow Statements",
    "section": "Loops",
    "text": "Loops\n\n\n\nfor loops\nfor loops are the most commonly used type of loop and are extremely useful. for loops are used to iterate or loop through any object capable of returning its members one at time, i.e.¬†an iterable object.\n\nüêç &lt;b&gt;Note.&lt;/b&gt;  The &lt;b&gt;iterator&lt;/b&gt; in a &lt;code&gt;for&lt;/code&gt; loop is a temporary variable used to store each value in the iterable object. The iterator is defined in the &lt;code&gt;for&lt;/code&gt; loop syntax as follows:\nfor &lt;ITERATOR&gt; in &lt;ITERABLE&gt;:\n    do something to iterator\nThe name of the iterator should reflect the nature of the list. i is commonly used as an enumerator or counter variable and should be avoided for other uses.\n\nA generic counter for loop can be generated using the range() function.\nfor i in range(4):\n    print(i + 1)\n    \n&gt;&gt;&gt; 1\n    2\n    3\n    4\nfor loops are commonly used to iterate through lists. As with the generic range() for loop, the iterator is assigned the next value from the list at the end of the indented code block.\nfor aa in aminoacids:\n    print(aa)\n    \n&gt;&gt;&gt; histidine\n    isoleucine\n    leucine\n    lysine\n    methionine\n    phenylalanine\n    threonine\n    tryptophan\n    valine\n    alanine\n    arginine\n    asparagine\n    aspartate\n    cysteine\n    glutamate\n    glutamine\n    glycine\n    proline\n    serine\n    tyrosine\n\n Built-in functions \n\nThe enumerate() function can be used in a for loop to keep track of the index of the iterator. This can be useful for keeping track of the number of iterations completed, accessing other elements in the same list based on their relative index (e.g.¬†the value immediately following the current iterator value), or accessing elements in another list based on the iterator‚Äôs index.\ngases = ['N2', 'O2', 'Ar', 'H2O', 'CO2']\n\nfor i,gas in enumerate(gases):\n    rank = i + 1\n    print('The #%d most abundant gas in the atmosphere is %s.' %(rank,gas))\n\n &gt;&gt;&gt;  The #1 most abundant gas in the atmosphere is N2.\n\n\nThe #2 most abundant gas in the atmosphere is O2.\n\n\nThe #3 most abundant gas in the atmosphere is Ar.\n\n\nThe #4 most abundant gas in the atmosphere is H2O.\n\n\nThe #5 most abundant gas in the atmosphere is CO2.\n\n\n\nCode\n# Define list of atomic masses of 6 most abundant elements\natomic_mass = [12.011, 1.00784, 14.0067, 15.999, 30.97376, 32.065]  # g/mol\n\n# Iterate through org_elements + print the atomic mass of each element\n\n\nTo iterate through multiple lists at the same time without indexing, use the zip() function.\ngas_frac = [0.78084, 0.209476, 0.00934, 0.0025, 0.000314]\ngas_molar_mass = [14.0067, 15.999, 39.948, 18.01528, 44.01]\n\nmass_atmosphere = 5.148e21   # grams\nn_a = 6.022e23    # Avogadro's number\n\nfor frac,mol_mass in zip(gas_frac,gas_molar_mass):\n    mass = frac * mass_atmosphere\n    molecules = (mass / mol_mass ) * n_a\n    print(molecules)\n\n&gt;&gt;&gt; 1.7282458205744395e+44\n    4.0590156271366957e+43\n    7.248215956743767e+41\n    4.302078013774973e+41\n    2.211859664621677e+40\nThe zip() function is nearly always used when each element of one list corresponds to an element in the same index position in another list. Therefore, it should mostly be used with two or more lists of the same length. If the lists do not have the same length, however, the number of iterations of the for loop will match the length of the shortest list.\nThe enumerate() and zip() functions can also be used together, if necessary, to keep track of the index position of iterators.\nfor i,(frac,mol_mass) in enumerate(zip(gas_frac,gas_molar_mass)):\n    gas = gases[i]\n    mass = frac * mass_atmosphere\n    molecules = (mass / mol_mass ) * n_a\n    print('There are ' + '{:.2e}'.format(molecules) + ' molecules of %s in the atmosphere.' % (gas))\n\n &gt;&gt;&gt;  There are 1.73e+44 molecules of N2 in the atmosphere.\n\n\nThere are 4.06e+43 molecules of O2 in the atmosphere.\n\n\nThere are 7.25e+41 molecules of Ar in the atmosphere.\n\n\nThere are 4.30e+41 molecules of H2O in the atmosphere.\n\n\nThere are 2.21e+40 molecules of CO2 in the atmosphere.\n\n\n Nested for loops \n\nLike if statements, for loops can be nested to perform an operation multiple times for each iterator in the overall loop.\nfor aa in aminoacids:\n    vowels = 0\n    for letter in aa:\n        if letter in ['a', 'e', 'i', 'o', 'u']:\n            vowels = vowels + 1\n    print(aa.capitalize() + ' has ' + str(vowels) + ' vowels.')\n    \n&gt;&gt;&gt; Histidine has 4 vowels.\n    Isoleucine has 6 vowels.\n    Leucine has 4 vowels.\n    Lysine has 2 vowels.\n    Methionine has 5 vowels.\n    Phenylalanine has 5 vowels.\n    Threonine has 4 vowels.\n    Tryptophan has 2 vowels.\n    Valine has 3 vowels.\n    Alanine has 4 vowels.\n    Arginine has 4 vowels.\n    Asparagine has 5 vowels.\n    Aspartate has 4 vowels.\n    Cysteine has 3 vowels.\n    Glutamate has 4 vowels.\n    Glutamine has 4 vowels.\n    Glycine has 2 vowels.\n    Proline has 3 vowels.\n    Serine has 3 vowels.\n    Tyrosine has 3 vowels.\n\n\n\n\nwhile loops\nwhile loops are used to repeatedly execute a block of code while a given condition is satisfied. The indented code block will be contiuously executed until the condition becomes False.\nx = 0\nwhile x &lt; 4:\n    x = x + 1\n    print(x)\n&gt;&gt;&gt; 1\n    2\n    3\n    4\nwhile loops are often used for user input, which allows the program to ‚Äústall,‚Äù repeatedly prompting the user until an acceptable answer is input.\n\n‚úèÔ∏è Try it. Run the following cell to see how a while loop works, inputing various answers that do not satisfy the condition before inputing the value that will end the loop.\n\n\n\nCode\n# Define a blank string variable\nuser_txt = \"\"\n# Wait for a specific answer, repeatedly asking for input until the condition is satisfied.\nwhile user_txt != \"42\":\n    user_txt = input(\"What is the answer to life, the universe, and everything? \")\n\n\n\nüêç &lt;b&gt;Note.&lt;/b&gt; &lt;code&gt;while&lt;/code&gt; loops can be infinite, so they should be used very judiciously. For example, imagine if instead of &lt;code&gt;x = x + 1&lt;/code&gt; in the previous example, the code block executed in the &lt;code&gt;while&lt;/code&gt; loop were &lt;code&gt;x = x - 1&lt;/code&gt;:\nx = 0\nwhile x &lt; 4:\n    x = x - 1\n    print(x)\n&gt;&gt;&gt; -1\n    -2\n    -3\n    -4\n    -5\n    -6\n    ... # and so on\nThis loop would never stop executing because the condition can never be met.\n\n\n\n\n\nEscaping loops\nSometimes it is necessary to terminate a loop iteration or the loop itself. The break and continue statements are used to escape loops.\nbreak statements in Python are used to escape an entire loop based on a condition. In nested loops, a break will only exit out of one level.\n\n\n\nbreak_chart\n\n\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nwhile True:\n    print(\"What is the answer to life, the universe, and everything? \")\n    option = input(\"   Your answer: \")\n    if option.isdigit():\n        if int(option) == 42:\n            print('You have solved the ultimate question.')\n            break\n        else:\n            print(\"Nope. Try again...\\n\")\n    else:\n        print(\"Nope. Try again...\\n\")\n\n\ncontinue statements are used to skip the remainder of the loop for a given iteration.\n\n\n\ncontinue_chart\n\n\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nfor x in [47, 22.5342, 'four hundred eighty-two', 0, 72104, -932.14, 6, -23, 'eleven']:\n    # Check to see if value is a string\n    if type(x) == str:\n        # If so, skip to next iteration\n        continue\n    # Otherwise, divide by 2 and print\n    print(x/2)\n\n\n\n\n\n\nList comprehensions\nA list comprehension is a quick, concise way to generate a list. They are generally used to condense the generation of a list from a simple expression into one line of code.\nFor example, say you wanted a list of all the squares from 0 and 100. As we learned in Session 1-2, this could be accomplished as follows:\nsquares = []\n\nfor x in range(11):\n    squares.append(x**2)\n\nprint(squares)\n&gt;&gt;&gt; squares = [0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\nUsing a list comprehension, the same list could be generated in a single line of code as follows:\nsquares = [x**2 for x in range(11)]\n\nprint(squares)\n&gt;&gt;&gt; squares = [0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\nList comprehensions are often combined with if statements. For example, the following line of code creates a list of all even numbers from 0 to 100.\nevens = [x for x in range(101) if x%2 == 0]\n\nprint(evens)\n&gt;&gt;&gt; evens = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, \n               22, 24, 26, 28, 30, 32, 34, 36, 38, 40,\n               42, 44, 46, 48, 50, 52, 54, 56, 58, 60, \n               62, 64, 66, 68, 70, 72, 74, 76, 78, 80, \n               82, 84, 86, 88, 90, 92, 94, 96, 98]\n\n‚úèÔ∏è Try it. Change the code above to create a list of all even squares from 0 to 100."
  },
  {
    "objectID": "interactive_sessions/enumerate_and_zip.html#first-lets-make-three-short-lists-to-use-in-our-example",
    "href": "interactive_sessions/enumerate_and_zip.html#first-lets-make-three-short-lists-to-use-in-our-example",
    "title": "Enumerate and Zip",
    "section": "1. First, let‚Äôs make three short lists to use in our example:",
    "text": "1. First, let‚Äôs make three short lists to use in our example:\n\n\nCode\nlist_1 = ['a', 'b', 'c', 'd']\nlist_2 = [1, 2, 3, 4]\nlist_3 = ['i', 'j', 'k', 'l']"
  },
  {
    "objectID": "interactive_sessions/enumerate_and_zip.html#now-lets-see-what-happens-when-we-put-the-first-list-into-a-for-loop-using-enumerate",
    "href": "interactive_sessions/enumerate_and_zip.html#now-lets-see-what-happens-when-we-put-the-first-list-into-a-for-loop-using-enumerate",
    "title": "Enumerate and Zip",
    "section": "2. Now, let‚Äôs see what happens when we put the first list into a for loop using enumerate",
    "text": "2. Now, let‚Äôs see what happens when we put the first list into a for loop using enumerate\n\n\nCode\nfor stuff in enumerate(list_1):\n    print(stuff)\n\n\n\nü§î stuff contained two items each time the loop ran. The first item is the index into list_1 (the current position of the loop in the list) and the second item is the value of list_1 at that position."
  },
  {
    "objectID": "interactive_sessions/enumerate_and_zip.html#lets-alter-our-code-to-assign-each-of-these-items-separately-in-the-for-loop",
    "href": "interactive_sessions/enumerate_and_zip.html#lets-alter-our-code-to-assign-each-of-these-items-separately-in-the-for-loop",
    "title": "Enumerate and Zip",
    "section": "3. Let‚Äôs alter our code to assign each of these items separately in the for loop:",
    "text": "3. Let‚Äôs alter our code to assign each of these items separately in the for loop:\n\n\nCode\nfor i,value in enumerate(list_1):\n    print(i,value)\n\n\n\nüß† : Now that we have an ‚Äúindex‚Äù variable (i), we could grab the value of any item in any other list at the same position!"
  },
  {
    "objectID": "interactive_sessions/enumerate_and_zip.html#lets-grab-the-values-of-list_1-and-list_2-as-well.",
    "href": "interactive_sessions/enumerate_and_zip.html#lets-grab-the-values-of-list_1-and-list_2-as-well.",
    "title": "Enumerate and Zip",
    "section": "4. Let‚Äôs grab the values of list_1 and list_2 as well.",
    "text": "4. Let‚Äôs grab the values of list_1 and list_2 as well.\n\n\nCode\nfor i,value in enumerate(list_1):\n    print(i,value,list_2[i],list_3[i])"
  },
  {
    "objectID": "interactive_sessions/enumerate_and_zip.html#finally-instead-of-using-the-index-to-grab-items-outside-the-loop-we-can-use-zip-to-loop-through-all-of-these-lists-at-the-same-time",
    "href": "interactive_sessions/enumerate_and_zip.html#finally-instead-of-using-the-index-to-grab-items-outside-the-loop-we-can-use-zip-to-loop-through-all-of-these-lists-at-the-same-time",
    "title": "Enumerate and Zip",
    "section": "üß† üß† Finally, instead of using the index to ‚Äúgrab‚Äù items outside the loop, we can use zip to loop through all of these lists at the same time!",
    "text": "üß† üß† Finally, instead of using the index to ‚Äúgrab‚Äù items outside the loop, we can use zip to loop through all of these lists at the same time!\n\n\nCode\nfor stuff in zip(list_1, list_2, list_3):\n    print(stuff)\n\n\n\n\nCode\ndef f(x):\n    return x + 'stuff'\n\n# new_list = []\n# for value in list_1:\n#     new_list.append(f(value))\n    \n\nnew_list = [f(value) for value in list_1]\nprint(new_list)\n\n\n\n\nCode\nfor v in zip(list_1): #, list_2, list_3):\n    print(v)\n\n\n\n\n\nth-4165028533.jpeg"
  },
  {
    "objectID": "interactive_sessions/99_scipy.html",
    "href": "interactive_sessions/99_scipy.html",
    "title": "Probability and Stochastic Processes with Scipy",
    "section": "",
    "text": "Rainfall in many dryland ecosystems exhibits pronounced variability from year to year. Because rainfall occurs intermittently and with a low degree of predictability, we often are forced to characterize the occurrence and amount of rainfall in terms of probability.\nIn this session, we will investigate the probabilistic nature of rainfall at various locations in Kenya. We will use these data to explore the concepts of probability and conditional probability, as well as probability density functions and cumulative density functions. We will use these data to develop a stochastic model of rainfall that can be used to generate novel time series of rainfall and develop inference into the likelihood of various daily, monthly, seasonal, and annual rainfall totals."
  },
  {
    "objectID": "interactive_sessions/99_scipy.html#determine-yearly-rainfall-totals.",
    "href": "interactive_sessions/99_scipy.html#determine-yearly-rainfall-totals.",
    "title": "Probability and Stochastic Processes with Scipy",
    "section": "Determine yearly rainfall totals.",
    "text": "Determine yearly rainfall totals.\nThe most common ‚Äì but not always the most useful ‚Äì interval of rainfall characterization is annual. Let‚Äôs see what years we have available to investigate annual rainfall, using the pd.unique() function.\n\n‚úèÔ∏è &lt;b&gt; Try it. &lt;/b&gt; \nUse the &lt;code&gt;pd.unique()&lt;/code&gt; function to get a list of the unique years contained in the &lt;code&gt;Year&lt;/code&gt; column of our dataframe, &lt;code&gt;df&lt;/code&gt;. Assign this list of years to a new array called &lt;code&gt;all_years&lt;/code&gt;\n\nIf all goes well, you should get results that look like this:\n\n&gt; print(all_years)\n\n[1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 1984 1985\n 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999\n 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013\n 2014 2015 2016 2017]\nIt looks like we have almost five decades of rainfall data. The first thing we might want to do with this data is look at yearly summaries.\n\n\n\nüìö  &lt;b&gt; Practice 1. (The following steps can be combined into a single line of code)&lt;/b&gt; \n\n\nDetermine the annual rainfall totals for each year.\n\n\nUse method chaining combined with the df.groupby() function to get the sum of every year‚Äôs rainfall.\n\n\nUse the argument min_count in the sum() command to force pandas to only include sums of rainfall for years with at least 350 days of data.\n\n\nAssign these totals to a new Series variable called yearly_rainfall_total.\n\n\n\nIf all goes correctly, you should get results that look like this:\n&gt; yearly_rainfall_total.head()\n\nYear\n1972    681.228\n1973        NaN\n1974    378.968\n1975    791.464\n1976    363.220\nName: Rainfall (mm), dtype: float64"
  },
  {
    "objectID": "interactive_sessions/99_scipy.html#quantifying-rainfall-variability",
    "href": "interactive_sessions/99_scipy.html#quantifying-rainfall-variability",
    "title": "Probability and Stochastic Processes with Scipy",
    "section": "Quantifying rainfall variability",
    "text": "Quantifying rainfall variability\nWe can use the coefficient of variation to describe rainfall variability. The coefficient of variation, CV, of a group of numbers is the ratio of the standard deviation, \\sigma, to the mean, \\mu. We can calculate the population CV of a sample, \\widehat{CV}, as the ratio of standard deviation of the sample, s, and the sample mean, \\bar{x}:\n \\widehat{CV} = \\frac{s}{\\bar{x}} \n\n‚úèÔ∏è &lt;b&gt; Try it. &lt;/b&gt;    \nCreate a histogram of yearly rainfall amounts using the hist() command that is a built-in method of any pandas.Series.   Note: Your variable, yearly_rainfall_total should be a pandas.Series object.\n\nWe have a very pronounced mode, with a large degree of variation. Let‚Äôs calculate the \\widehat{CV}.\n\n‚úèÔ∏è &lt;b&gt; Try it. &lt;/b&gt;    \nUse the formula above to calculate the \\widehat{CV} for the data in yearly_rainfall_total.\n\n\\widehat{CV} has some problems dealing with small sample sizes (n) and tends to be biased low. We can create an unbiased estimator, \\widehat{CV}^{*} using the following function{^1}:\n \\widehat{CV}^{*} = \\left(1 + \\frac{1}{4n}\\right)\\widehat{CV} \n{^1} technically, \\widehat{CV}^{*} is only valid for normally-distributed data, but we will use it anyway as an example.\n\nüìö  &lt;b&gt; Practice 2.&lt;/b&gt;\nCreate a function that calculates the $\\widehat{CV}^{*}$ for a set of data and use the function to determine the $\\widehat{CV}^{*}$ value for our yearly rainfall data."
  },
  {
    "objectID": "interactive_sessions/99_scipy.html#examining-daily-rainfall-probabilities-and-amounts",
    "href": "interactive_sessions/99_scipy.html#examining-daily-rainfall-probabilities-and-amounts",
    "title": "Probability and Stochastic Processes with Scipy",
    "section": "Examining daily rainfall probabilities and amounts",
    "text": "Examining daily rainfall probabilities and amounts\nAs we see from the analysis of yearly and monthly rainfall, the climatology of this location is characterized by a fairly high degree of variability. The same is true of many tropical drylands.\n\n1. What is the chance that it will rain?\nIn order to get at the rainfall process itself, let‚Äôs look at the distribution of rainfall events. We can find all the days of rainfall by simply filtering our DataFrame to find days when rain was greater than zero. To do this, we can use the loc() function.\nThe following line of code returns a new dataframe that contains only the rows where 'Rainfall (mm)' is greater than zero:\nrainy_days = df.loc[ (df['Rainfall (mm)'] &gt; 0) ]\n\nüìö  &lt;b&gt; Practice 3.&lt;/b&gt;\nCalculate the overall probability of daily rainfall.\n&lt;ol class=\"alpha\"&gt;\n    &lt;li&gt; Create a &lt;code&gt;DataFrame&lt;/code&gt; containing only rainy days (when rainfall is greater than zero) called &lt;code&gt;rainy_days&lt;/code&gt;&lt;/li&gt;\n    &lt;li&gt; Create a &lt;code&gt;DataFrame&lt;/code&gt; consisting of all observation days (when rainfall is not equal to &lt;code&gt;NaN&lt;/code&gt;) called &lt;code&gt;all_days&lt;/code&gt;&lt;/li&gt;\n    &lt;li&gt; Use the ratio of the length of &lt;code&gt;rainy_days&lt;/code&gt; and &lt;code&gt;all_days&lt;/code&gt; to determine the probability of rainfall and save this as a new variable called &lt;code&gt;prob_rain&lt;/code&gt;.&lt;/li&gt;\n&lt;/ol&gt;\n\nIf all goes well, then you should get the following value for prob_rain:\n\n&gt; print(prob_rain)\n0.13211859564472098\n\n\n\n\nMonthly Rainfall Totals and Probabilities\nOver the entire time period, the probability of rainfall was about 13%, which means that it rains - on average - a little less than once per week (approximately every 7.7 days). However, most regions have strong seasonality in rainfall. Our data comes from a site that is located very close to the equator in central Kenya. This means that the movement of the ITCZ across the tropics causes changes in the likelihood of rainfall from month to month. We should therefore look at monthly rainfall probabilies rather than simply the annual average.\n\nüìö  &lt;b&gt; Practice 4.&lt;/b&gt;\n&lt;ol class=\"alpha\"&gt;\n    &lt;li&gt; Determine the probability of rainfall for each month. Add these values to a list called &lt;code&gt;lambda_by_month&lt;/code&gt;. Hint: You can combine test criteria using logical operators (i.e. &lt;code&gt;&&lt;/code&gt; and &lt;code&gt;|&lt;/code&gt;).\n    &lt;li&gt;Create a bar graph of the monthly rainfall probabilites. Label your plot appropriately.\n&lt;/ol&gt;\n\n\n\n2. What is the amount that it will rain?\nWe‚Äôve already subsetted all of our data for days with rain and stored this in the variable rainy_days. Let‚Äôs look at the distribution of rain amounts:\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nrainy_days['Rainfall (mm)'].hist(bins=50)\n\n\nAs opposed to the annual rainfall distribution, the distribution of daily storm totals (or daily rainfall) has a much clearer distribution. This pattern of daily rainfall ‚Äì a positively skewed distribution with a short-tail ‚Äì is very consistent with what we see across tropical drylands, and even more broadly across any location where rainfall is dominated by convective processes. We might ask what the average storm total is‚Ä¶\n\n‚úèÔ∏è &lt;b&gt; Try it. &lt;/b&gt;    \nCalculate the average rainfall amount on rainy days over the data record. Save this quantity as a new variable called &lt;code&gt;avg_rainfall_depth&lt;/code&gt;."
  },
  {
    "objectID": "interactive_sessions/99_scipy.html#fitting-rainfall-depths-to-an-exponential-distribution",
    "href": "interactive_sessions/99_scipy.html#fitting-rainfall-depths-to-an-exponential-distribution",
    "title": "Probability and Stochastic Processes with Scipy",
    "section": "Fitting rainfall depths to an exponential distribution",
    "text": "Fitting rainfall depths to an exponential distribution\nA simple rainfall model makes two assumptions; (1) that events arrive according to a Poisson process, and (2) that rainfall depths are distributed according to an exponential distribution. These two assumptions are accompanied by the need for two rainfall parameters; (1) the probability of rainfall events, \\lambda_r, and the average depth of rainfall events \\alpha. We‚Äôve already seen how to estimate both of these parameters from rainfall data, so here we are going to focus on testing the appropriateness of the model assumptions during the growing season for Laikipia, Kenya.\n\nStep 1. Fit the distribution\nTo fit the distribution, we are going to use some more functions from python‚Äôs suite of numerical analysis. In this case we are going to use some functions from scipy. The scipy.stats module has a large suite of distribution functions pre-defined, which we can use to develop a fit for our data. Using any of these distributions for fitting our data is very easy. The distribution we are most interested in is the exponential distribution, which is called expon in the stats module.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nimport scipy.stats as st\n\ndistribution = st.expon\ndata = rainy_days['Rainfall (mm)']\nparams = distribution.fit(data, loc=0) # Force the distribution to be built off of zero\n\nprint(params)\n\narg = params[:-2]\nloc = params[-2]\nscale = params[-1]\n\n\n\n\nStep 2. Calculate fitted PDF and error with fit in distribution\nTo test the fit of our distribution, we can compare the empirical histogram to that predicted by our model. To do this, we first use our data to generate the empirical histogram. In this exampkle, we break the data into 30 bins, and we generate a histrogram of density rather than counts. This allows for an easier comparison between our empirical data and the fitted probability distribution function. Here are the steps:\n\nGenerate a histogram, from the data. Save the bin locations in x and the density of values in y\nShift the x bin locations generated from the histogram to the center of bins.\nCalculate the value of the fitted pdf(x) for each of the bins in x.\nDetermine the residual sum of the squares, SS_{error}, and total sum of squares, SS_{yy}, according to:\n\n SS_{error} = \\sum_{i=1}^{n} \\left(y_i - f(x_i)\\right)^2   SS_{yy} = \\sum_{i=1}^{n} \\left(y_i - \\bar{y}\\right)^2 \n\nCalculate the r^2 of the fit, according to\n\n r^2 = 1- \\frac{SS_{error}}{SS_{yy}} \n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Step 1. Generate a density histogram of the data \ny, x = np.histogram(data, bins=30, density=True)\n\n# Step 2. Shift the x bin locations to the center of bins.\nx = (x + np.roll(x, -1))[:-1] / 2.0\n\n# Step 3. Calculate the values of pdx(x) for all x.\npdf = distribution.pdf(x, loc=loc, scale=scale, *arg)\n\n# Step 4. Determine the residual and total sum of the squares.\nss_error = np.sum(np.power(y - pdf, 2.0))\nss_yy = np.sum(np.power(y - y.mean(), 2.0))\n\nr_2 = 1 - ( ss_error / ss_yy )\nprint(r_2)\n\n\nThese results suggest that an exponential distribution is a really good fit for our observed data on rainfall amounts."
  },
  {
    "objectID": "interactive_sessions/99_scipy.html#modeling-rainfall",
    "href": "interactive_sessions/99_scipy.html#modeling-rainfall",
    "title": "Probability and Stochastic Processes with Scipy",
    "section": "Modeling rainfall",
    "text": "Modeling rainfall\nLet‚Äôs make a very simple model of rainfall that assumes a Poisson process (i.e.¬†a memoryless process). We just saw that this approach is probably too simple, and later we will look at how we could improve the model. But it helps to start with the most simple approach and then add complexity as needed.\nOur model will be built with the following assumptions:\n\nThe probability of an ‚Äúevent‚Äù (i.e.¬†rainfall) occuring on any given day is given by the parameter \\lambda_r, which has units day^{-1}\nThe total depth of rainfall on days with rain is a random variable, drawn from an exponential distribution with mean, \\alpha, which has units of mm.\n\nThis type of model is a ‚Äúmarked Poisson process‚Äù and is a special case of a Poisson process on a number line (in our case, the number line is time), where each event is characterized by a random ‚Äúmark‚Äù that is independent of the event.\n\nStep 1. Simulating Poisson events\nWe have a variety of means to simulate a Poisson process, and all of them will require the use of a random number generator. The numpy package has lots of different builtin functions to generate pseudo-random numbers, and we can use one of these.\nTo simulate the likelihood of a Poisson event, we can draw a sample from a uniform distribution in [0,1] and compare that to our probability of an event. If the value we draw is less than or equal to our probability, then an event occurs, otherwise no event occurs. The probability density function, f(x) of a uniform distribution sampled over the half open interval from a to b, [a,b) is given by\n\nf(x) = \\frac{1}{b - a}\n\nWe can sample from this distribution using the np.random.uniform() function:\n\ns = np.random.uniform(low=0,high=1)\nThe np.random.uniform() function also takes the optional argument size, which specifies the number of random samples to return. This allows for the generation of large lists of random samples without using a for loop or list comprehension approaches.\n# Returns an array of 2 random draws.\ns = np.random.uniform(low=0, high=1, size=2) \n\n# Returns an array of 2 rows each with 10 random draws\ns_mat = np.random.uniform(low=0, high=1, size=[2, 10])\n\n‚úèÔ∏è &lt;b&gt; Try it. &lt;/b&gt;    \nUsing a $\\lambda_r$ value equal to your calculated value of &lt;code&gt;prob_rain&lt;/code&gt; and assuming that the length of a growing season, $T_{seas}$, is 160 days, simulate a season of rainfall \"events\", where the value of a day is &lt;code&gt;1&lt;/code&gt; if rainfall occurs, and &lt;code&gt;0&lt;/code&gt; if not.&lt;/div&gt;\n\n\nüí° You can use the pd.astype() command to force Boolean (True/False) values into integers (1,0)\n\n\n\nStep 2. Adding marks to events\nOur approach for creating marks (i.e.¬†daily rainfall depths) is to sample them randomly from an exponential distribution with mean \\alpha. The exponential probability density function f(x) for x\\gt0 as a function of \\alpha is expressed as\n\nf(x) = \\frac{1}{\\alpha} \\exp\\left(-\\frac{x}{\\alpha}\\right),\n\nJust as with sampling from a uniform distribution (and as with most everything in python/pandas), there‚Äôs a function for that. In this case, we are going to use the‚Ä¶ you guessed it‚Ä¶ np.random.exponential() function. It is used similarly to the np.random.uniform() function, but with a single parameter to describe the distribution:\ns = np.random.exponential(scale=11.5) \nThe scale parameter is the mean of the distribution, which in our simulations will be \\alpha (mm), the average rainfall depth on days with rain. Just as with np.random.uniform(), we can generate lists or arrays of samples from the exponential distribution.\n\ns = np.random.exponential(scale=11.5, size=2) # Returns an array of 2 random draws.\n\ns_mat = np.random.uniform(scale=11.5, size=[2, 10]) # Returns an array of 2 rows each with 10 random draws.\n\n‚úèÔ∏è &lt;b&gt; Try it. &lt;/b&gt;    \nUse the average storm depth (&lt;code&gt;avg_rainfall_depth&lt;/code&gt;), probability of rainfall you calculated earlier, and the same $T_{seas}$ as before, generate a season of rainfall.&lt;/div&gt;\n\nüí° The product of 0 and any rainfall amount is still 0. That means you don‚Äôt need to worry about which days have rain and which don‚Äôt when multiplying a list of rainfall_amounts and a binary list of rain_days.\n\n\n\nSimulating multiple years of rainfall\n\nüìö  &lt;b&gt; Practice 5.&lt;/b&gt;\nUsing your &lt;code&gt;prob_rain&lt;/code&gt; and your &lt;code&gt;avg_rainfall_depth&lt;/code&gt;:\n&lt;ol class=\"alpha\"&gt;\n    &lt;li&gt; Generate 100 years of rainfall.&lt;/li&gt;\n    &lt;li&gt; Plot a histogram of the annual totals.&lt;/li&gt;\n    &lt;li&gt;Calculate the $\\widehat{CV}$ of seasonal rainfall.&lt;/li&gt;\n&lt;/ol&gt;\n\n\n\nCode"
  },
  {
    "objectID": "interactive_sessions/99_scipy.html#conditional-probabilities-given-that-today-was-rainy-will-it-rain-tomorrow",
    "href": "interactive_sessions/99_scipy.html#conditional-probabilities-given-that-today-was-rainy-will-it-rain-tomorrow",
    "title": "Probability and Stochastic Processes with Scipy",
    "section": "Conditional Probabilities: Given that today was rainy, will it rain tomorrow?",
    "text": "Conditional Probabilities: Given that today was rainy, will it rain tomorrow?\nFinally, we want to look to see if we can predict rainfall. The easiest way to think about predictability is to ask whether or not knowing what happened today significantly alters our expectation about what will happen tomorrow. There are two options:\n\nIf every day is random and unpredictable, then rainfall tomorrow will be independent of what happened today.\nIf what happens today affects tomorrow, then we should see dependency between today and tomorrow‚Äôs rainfall.\n\nWe can test the independence of rainfall likelihood by testing to see if rainfall on day t affects the probability of rainfall on day t+1.\nFormally, we can write the probability that rainfall, R, on some day t,is greater than zero as P[R_t&gt;0]. If $ P[R_t&gt;0] = P[R_t&gt;0 | R_{t-1} &gt;0] $, then we can say that P[R_t] and P[R_{t-1}] are independent.\nFirst, we use the index property of rainy_days, which is just a list of all the index values (or rows) in our original data, df, where rainfall was greater than zero. Then we increment the index and save all these rows from the original data into a new variable, days_after_rain.\n\nüìö  &lt;b&gt; Practice 6.&lt;/b&gt;\n&lt;ol class=\"alpha\"&gt;\n    &lt;li&gt; Make a list of the index locations of &lt;code&gt;rainy_days&lt;/code&gt; using the &lt;code&gt;index&lt;/code&gt; attribute of &lt;code&gt;rainy_days&lt;/code&gt;.&lt;/li&gt;\n    &lt;li&gt; Create a new dataframe called &lt;code&gt;days_after_rain&lt;/code&gt; that contains only the days after rainy days. You will need to use our original &lt;code&gt;df&lt;/code&gt; dataframe to get this new dataframe.&lt;/li&gt;\n    &lt;li&gt;Calculate the probability of rainfall following rainy days using your new &lt;code&gt;days_after_rain&lt;/code&gt; variable. Call this new probability &lt;code&gt;prob_rain_after_rain&lt;/code&gt;&lt;/li&gt;\n    &lt;li&gt;Compare the probability of rainfall following rainy days (&lt;code&gt;prob_rain_after_rain&lt;/code&gt;) to the overall probability of rainfall (&lt;code&gt;prob_rain&lt;/code&gt;). Based on these values, is an assumption of independence in the rainfall process valid?&lt;/li&gt;\n&lt;/ol&gt;"
  },
  {
    "objectID": "interactive_sessions/99_scipy.html#simulating-annual-rainfall-with-monthly-values",
    "href": "interactive_sessions/99_scipy.html#simulating-annual-rainfall-with-monthly-values",
    "title": "Probability and Stochastic Processes with Scipy",
    "section": "Simulating annual rainfall with monthly values",
    "text": "Simulating annual rainfall with monthly values\nOne improvement we can make to our model is to have monthly values for our rainfall probabilities. To get these monthly \\lambda_r values, we need to determine the probability of rainfall for each month by dividing the number of rainy days per month by the total number of observations in each month, which we did in Practice 4. For now, we assume stationarity in the monthly values, which means that we are assuming that the values of \\lambda_r in each month are the same through out the entire record (i.e.¬†Jan 1938 has the same properties as Jan 2008).\n\nüí° It‚Äôs worth thinking about how you could test our stationarity assumption. If you have an idea of how to do so, go ahead and give it a shot!\n\nTo improve our simulation of annual rainfall, we are going to use our monthly values to specify daily values of \\lambda_r using the monthly values we just calculated. The use of a variable \\lambda value in a Poisson process creates what is known as an ‚Äúinhomogenous Poisson process‚Äù (or, alternatively, ‚Äúnonhomogeneous‚Äù‚Ä¶ unfortunatetly, there isn‚Äôt much homogeneity in what we call it!). These types of processes allow the properties of the process to change in space and time. Our implementation ‚Äì using monthly values ‚Äì is a little clunky, and we‚Äôd prefer to have the \\lambda values change more smoothly throughout the year. However, we probably don‚Äôt have sufficient data to allow for this, even if we could accomodate the more complicated coding it would require.\nIn order to generate our nonhomogenous process, we will first generate a daily array of month numbers for the year 2018. This is really easy in python using datetime + timedelta (which we need to import).\n    from datetime import timedelta, datetime\n    datetimes = np.arange(\n        datetime(2018,1,1), datetime(2018,12,31),\n        timedelta(days=1)).astype(datetime)\n    month_value_by_day = np.array([datetime.month for datetime in datetimes])\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nfrom datetime import timedelta, datetime\ndatetimes = np.arange(datetime(2018,1,1), datetime(2019,1,1), timedelta(days=1)).astype(datetime)\nmonth_value_by_day = np.array([datetime.month for datetime in datetimes])\n\n\nNow we have a 12-element np.array of \\lambda_r values, organized by month number, which we‚Äôve stored in lambda_by_month and we have a list of 365 days that contains the month number for each day, which we‚Äôve stored in month_value_by_day. We can use these two variables to get a new variable that contains the correct \\lambda value for each day.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\ndaily_lambda_values = np.array([lambda_by_month[i-1] for i in month_value_by_day])\n\n\nWith daily values of \\lambda_r, we only need to follow the same cookbook we used to make the stationary simulation.\n    simulated_rainy_days = (\n        np.random.uniform(\n            low=0, high=1, size=len(daily_lambda_values)\n        ) &lt;= daily_lambda_values).astype(int)\n    simulated_rainfall_values = np.random.exponential(\n        scale=alpha, size=len(daily_lambda_values)\n    )\n\n‚úèÔ∏è &lt;b&gt; Try it. &lt;/b&gt;    \nUse the code above to create a new daily rainfall simulation using our daily values of $\\lambda_r$. Plot the daily rainfall data. You can easily plot your rainfall data using the &lt;code&gt;datetimes&lt;/code&gt; object we already created as the basis of our x-axis.&lt;/div&gt;"
  },
  {
    "objectID": "interactive_sessions/99_functions_objects_classes.html",
    "href": "interactive_sessions/99_functions_objects_classes.html",
    "title": "Session 1-5: Functions & Classes",
    "section": "",
    "text": "In order to effecitvely use Python, we need to learn how to use functions and classes.\nFunctions are a fundamental part of almost any programming language, and Python is no different. We have already been introduced to many of the excellent builtin functions that are part of the standard Python library. However, as you develop your own analyses, you will need to be able to author your own functions. These functions will allow you to create stand-alone pieces of code that perform consistent operations on input data.\nClasses are even more versatile than functions. They are present in almost all object-oriented programming languages and provide a means to bundle functions and data together. Defining a new class creates a new type of object (recall that objects are the fundamental building block of Python), allowing new instances of that object type to be made."
  },
  {
    "objectID": "interactive_sessions/99_functions_objects_classes.html#functions",
    "href": "interactive_sessions/99_functions_objects_classes.html#functions",
    "title": "Session 1-5: Functions & Classes",
    "section": "Functions",
    "text": "Functions\nA function is simply a set of instructions that you wish to use repeatedly on varying data. Sometimes a function is used to group a complex set of instructions that allows you to compartmentalize your code in ways that improve its readability. There are three types of functions in Python: builtin functions, user-defined functions, and anonymous functions. We have already seen many builtin functions, and you will meet many more in the coming weeks. This session focuses on user-defined and anonymous functions, both of which are important for advanced data analysis.\n\nUser-defined Functions (UDFs)\nA UDF is created using some very specific syntax. First, a function is delcared using the def keyword. The name of the function - and any arguments it takes - follows the def keyword, followed by a :. The combination of def and : is a similar construction to other control statements in Python that you‚Äôve already seen, such as for + :, and if + :. The code block below is the simplest possible function.\ndef my_function():\n    pass\nThe pass keyword means ‚Äúdo nothing‚Äù. Therefore, we have defined a function that: (1) does not take any arguments; (2) does nothing, and then (3) returns nothing.\n\nüêç &lt;b&gt;Note.&lt;/b&gt; Technically, a function that lacks a &lt;code&gt;return&lt;/code&gt; statement will still return a value. But the value it returns is &lt;a href=\"https://docs.python.org/3/c-api/none.html\"&gt;&lt;code&gt;None&lt;/code&gt;&lt;/a&gt;, which is python-ese for nothing. Just like the concept of &lt;a href=https://www.amazon.com/Zero-Biography-Dangerous-Charles-Seife/dp/0140296476&gt;`0`&lt;/a&gt;, the concept of &lt;code&gt;None&lt;/code&gt; will turn out to be quite useful!\n\nThe next function below still does not take any arguments and it still doesn‚Äôt do anything. It does, however, include a return statement. As the note above indicates, the return statement isn‚Äôt necessary in Python functions; they will just automatically return None if you don‚Äôt specify otherwise. However, using the return statement is required if you ever want to work with any output from your functions.\ndef my_function():\n    return True\nNext, we can take a look at an example of a function that takes an argument (a) and returns a value (also a)‚Ä¶ but it still doesn‚Äôt actually do anything!\ndef my_function(a):\n    return a\nHopefully your functions will be more useful than the ones above that do nothing. However, we‚Äôve introduced these three ‚Äúdo-nothing functions‚Äù in order to highlight three important aspects of all functions. The ability to (1) pass an argument into a function, (2) transform data within - or based on - the value of a function argument, and (3) return some new data or result based on those manipulations. These three factors combine to make functions extremely useful. Finally the code below provides an example of a function that has all three components and does something that should be quite familiar to you at this point.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\ndef my_function(msg):\n    print(msg)\n\nmy_function(\"Hello World\")\n\n\n\n\nCode\n# Define a simple function\ndef convert_F_to_C(temp_F):\n    temp_C = (temp_F-32)*5./9.\n    return temp_C\n\n\nThe function above takes a Temperature in Fahrenheit and converts it to Celsius. It then returns this new value."
  },
  {
    "objectID": "interactive_sessions/99_functions_objects_classes.html#try-it.",
    "href": "interactive_sessions/99_functions_objects_classes.html#try-it.",
    "title": "Session 1-5: Functions & Classes",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nCall the convert_F_to_C() function with the value 98.6 (¬∞F). It should return 37.0.\n\n\nCode\nconvert_F_to_C(98.6)\n\n\n\nFunction Arguments & Parameters\nWhen we define a function, we specify the parameters the function requires. The definition of convert_F_to_C contains a single parameter, temp_F. When we call a function, we supply arguments to the function which are then mapped to the function parameters.\nProviding the argument 98.6 to the function maps this number to the temp_F parameter. So wherever temp_F appears in the function, 98.6 is used instead.\nWhat happens if we call a function without supplying arguments for the parameters?\n\nconvert_F_to_C() # Uh-oh... we didn't provide an argument for the temp_F parameter.\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-3-60e83d2545ed&gt; in &lt;module&gt;\n----&gt; 1 convert_F_to_C()\n\nTypeError: convert_F_to_C() missing 1 required positional argument: 'temp_F'\nIn the error above, we see that not providing a required argument raises a TypeError, and provides some additional detail regarding what went wrong. In this case, we are told that we are mssing one required positional argument, which needs to be assigned to the temp_F parameter.\n\nSpecifying default parameters\nWhen defining a function, it is possible to set a default value for any parameter. This is done by assigning the parameter the default value right inside the parameter list:\ndef convert_F_to_C(temp_F=0):\n    temp_C = (temp_F-32)*5./9.\n    return temp_C\n\nüêç &lt;b&gt;Note.&lt;/b&gt; Functions are no different than variables (or any other object in Python). Therefore, any function you create in a notebook can be re-defined by simply editing the function and re-running the cell!"
  },
  {
    "objectID": "interactive_sessions/99_functions_objects_classes.html#try-it.-1",
    "href": "interactive_sessions/99_functions_objects_classes.html#try-it.-1",
    "title": "Session 1-5: Functions & Classes",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nRe-define the convert_F_to_C() function so that the default value of temp_F is 0. Test what happens if you call this re-defined function without an argument.\n\n\nCode\ndef convert_F_to_C(temp_F):\n    temp_C = (temp_F-32)*5./9.\n    return temp_C\n\nconvert_F_to_C()\n\n\n\nRequired arguments\nArguments that are included in the parameter list and do not have default values are called required arguments. In the convert_temp_to_C function below, both temp and unit are required parameters.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\ndef convert_temp_to_C(temp, unit='F'):\n    \"\"\" Converts a temperature to Celsius\n    \n    Parameters\n    ----------\n        temp : float\n            Temperature value to convert\n        unit : str\n            temp units ('K' or 'F')\n       \n    Returns\n    -------\n        temp_C : float\n            The value of temp converted to Celsius\n            \n    \"\"\"\n    if unit.capitalize() == 'K':\n        temp_C = temp - 273.15\n    elif unit.capitalize() == 'F':\n        temp_C = (temp-32)*5/9\n    \n    return temp_C\n\n\nYou can also see that we have added some comments to the function right below the definition. These commments are known as Docstrings, and they are critical to allow users to understand what your program is doing.\nThe use of \"\"\" to begin and end the docstrings signifies we are writing a multi-line comment (as opposed to # which specifys a single line comment in Python).\nThe first line of a function‚Äôs docstrings should always be a short description of what the function does. The rest of the docstrings should specify the required arguments that the function needs and what values it returns, if any.\n\nüêç &lt;b&gt;The ABC of Python:&lt;/b&gt; Always. Be. Commenting.\n\n\nüêç &lt;b&gt;Note.&lt;/b&gt; You can see the function definition and &lt;b&gt;docstrings&lt;/b&gt; for any function (if they exist) by using the builtin &lt;code&gt;help()&lt;/code&gt; function."
  },
  {
    "objectID": "interactive_sessions/99_functions_objects_classes.html#try-it.-2",
    "href": "interactive_sessions/99_functions_objects_classes.html#try-it.-2",
    "title": "Session 1-5: Functions & Classes",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nUse the help() function to see the function definition and docstrings for the convert_temp_to_C function.\n\nüìö  &lt;b&gt; Practice 1. &lt;/b&gt; \nCreate a new function, &lt;code&gt;convert_temp_to_K&lt;/code&gt; that converts a temperature to Kelvin from either Fahrenheit or Celsius, depending on user-supplied arguments. \n\n\nKeyword arguments\nIn both convert_temp_to_C and convert_temp_to_K, the order of parameters is very important. If we tried calling the convert_temp_to_C function like this: convert_temp_to_C('F', 212.0) we would get an error! The reason we get an error is because the function definition assumes that the first argument should be mapped to the first parameter temp and the second argument should be mapped to the second parameter unit. That‚Äôs why the TypeError refered to temp_Fas a required positional argument in the section above.\nThis assumption that arguments be mapped to parameters in a specific order can make working with complicated functions that have many parameters almost impossible. For this reason, Python provides the ability to pass keyword arguments to functions.\nRather than making assumptions about how arguments map to parameters based on their order, keyword arguments specify exactly how arguments are mapped to parameters within a function. This is done by assigning an argument to a specific parameter within the function call.\nFor example, instead of writing convert_temp_to_C('F', 212.0), we can instead call the same function using convert_temp_to_C(unit='F', temp=212.0).\n\nüêç Note. Python doesn‚Äôt require you to change anything about a function‚Äôs definition to take advantage of keyword arguments. For this reason, it‚Äôs good practice to use keyword arguments whenever possible."
  },
  {
    "objectID": "interactive_sessions/99_functions_objects_classes.html#try-it.-3",
    "href": "interactive_sessions/99_functions_objects_classes.html#try-it.-3",
    "title": "Session 1-5: Functions & Classes",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nCall the convert_temp_to_K function that you created using keyword arguments.\nAn important consideration when using keyword arguments is that all keyword arguments must all follow any positional arguments that are passed to a function.\nIn other words, if you are calling a function with a mix of positional arguments and keyword arguments, the positional arguments need to all be listed first.\nSo, using our simple temperature conversion function as an example, convert_temp_to_C(212.0, unit='F') is valid, but convert_temp_to_C(temp=212.0, 'F') is not.\n\nAnonymous (lambda) functions\nSometimes we may just want to create a very simple function without having to go through all the trouble of using def to define the function, writing docstrings and adding a return statement.\nFor these ‚Äúone-liners‚Äù Python has the concept of anonymous functions. Instead of using def, these functions are declared using lambda notation, so they are often refered to as lambda functions.\nBecause they are meant to be simple, a lambda function declaration is always contained in a single line:\nQ = lambda T: 5.67e-8 * T**4\nThe function above calculates the Energy Flux, Q [W/m ^2], for a blackbody object at a specified temperature, T [Kelvin], assuming an emissivity of 1:\nQ = 5.67x10^{-8} \\times T^4\nWe can use this lambda function just like any other function:\nQ(50+273.15)\n&gt;&gt;&gt; 618.3006455416394\n\nüìö  &lt;b&gt; Practice 2. &lt;/b&gt; \nCreate a &lt;code&gt;lambda&lt;/code&gt; function in the cell below that converts a Celsius temperature to Kelvin.&lt;/div&gt;\n\n\n\nDocumenting Functions\nAs we saw above, the use of docstrings can greatly improve your ability to understand what a program requires in terms of arguments and what the function returns.\nThere is no standard for docstrings, but there are some best practices. A good docstring should contain:\n\nA brief description of what the function does.\nA more detailed explanation of how the functions works, if necessary.\nInformation on any arguments - both required and optional - that may be passed into the function.\nInformation on any values that the function returns.\nInformation on any parameter default values.\nInformation on any exceptions that the function raises.\nAny information about side effects the function may cause, or restrictions on when the function can be used.\n\nThe last couple of items on the list above aren‚Äôt very common, but the first four are essential components of all function docstrings. While it is fine to develop your own docstring style, here‚Äôs another example of what a docstring should look like:\ndef Q(T, epsilon=1, unit='C'):\n    \"\"\" Calculates energy emitted by an object with temperature T\n\n    Uses the Stefan-Bolzmann Law to calculate total radiative \n    emmittance in W/m^2 based on temperature and emissivity:\n    \n    Q = epsilon * sigma * T**4\n    \n    where sigma is the Stefan-Boltzmann constant (5.67e-8 W/m^2/K^4),\n    epsilon is the emissitivity (0-1), and T is temperature in Kelvin.\n\n    Parameters\n    ----------\n        T: float\n            Temperature of object\n        epsilon: float, optional\n            emissivity of object [0-1] (default is 1)\n        unit: str, optional\n            units of T, either 'F', 'C', or 'K' (default is 'C')\n    \n    Returns\n    -------\n        Q: float\n            Energy emitted by object [W/m^2]\n    \"\"\"\n\n    # Set Stefan-Boltzmann constant:\n    SIGMA = 5.67e-8 # W/m2/K^4                    \n                \n    # If T is in Fahrenheit, convert to C:\n    if unit == 'F':\n        T = (T - 32) * (5./9.)\n        unit = 'C' # Re-assign unit to C\n        \n    # If T is in Celsius, convert to Kelvin\n    if unit == 'C':\n        T = T + 273.15\n    \n    # Calculate Q and return the value\n    Q = epsilon * SIGMA * T**4\n    return Q"
  },
  {
    "objectID": "interactive_sessions/99_functions_objects_classes.html#practice-3.",
    "href": "interactive_sessions/99_functions_objects_classes.html#practice-3.",
    "title": "Session 1-5: Functions & Classes",
    "section": "üìö  Practice 3. ",
    "text": "üìö  Practice 3. \nWrite a complete set of docstrings for your function, convert_temp_to_K. Check to make sure they work using the help() function."
  },
  {
    "objectID": "interactive_sessions/99_functions_objects_classes.html#classes",
    "href": "interactive_sessions/99_functions_objects_classes.html#classes",
    "title": "Session 1-5: Functions & Classes",
    "section": "Classes",
    "text": "Classes\nClasses are Python objects that contain both attributes (i.e.¬†data) and methods (i.e.¬†functions). Classes are the essence of any object-oriented programming (OOP) language. A Class is created using the class keyword:\n\nclass Temperature:\n    value = 74.0\n    unit = 'F'\n    \nThe above code defines a new class called Temperature. It then assigns two attributes to the class, value and unit. This simple class has no methods. We will get to those next!\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nclass Temperature:\n    value = 74.0\n    unit = 'F'\n\n\n\nCreating an Instance of a Class\nWe create an instance of the class by calling it (like we call a function) and assigning the output of the call to a new variable:\nmy_temp = Temperature()\n\n\nAccessing Class Data\nClass objects are mutable, so it‚Äôs possible to change the data in a class. We can access ‚Äì and alter ‚Äì the attributes of a class using . notation:\nprint(my_temp.value) # Use . notation to access attributes of a class.\n&gt;&gt;&gt; 74.0\n\nmy_temp.value = 83.2  # Assign a new value to this instance of Temperature.\nprint(my_temp.value)  # Check to see if the value has been changed...\n&gt;&gt;&gt; 83.2\n\nüìö  Practice 4.  Create an instance of the Temperature class and print out the string: ‚ÄúThe temperature is 74 ¬∞F‚Äù.\n\n\n\nClass Initialization\nIn the example above, we saw how to create an instance of a Temperature. But you probably noticed that every instance of our class will have the same temperature and unit: 74.0 degress F.\nWhile it‚Äôs possible to change these values later (in python parlance, we‚Äôd say ‚Äúafter the object is instanced‚Äù), it‚Äôd be better if we could initialize our class instances with the values we want. To do this, we will create our first class method, the __init__ method.\n\n\n__init__() method\nThe __init__() method is a special function that we create for classes that tells python how to create a new instance of the class. This function is included as part of a class‚Äôs definition, and usually should be the very first method that appears in the class.\n\nüêç &lt;b&gt;Note.&lt;/b&gt; Python uses the \"double underscore + name + double underscore\" syntax (&lt;code&gt;__&lt;/code&gt; + &lt;code&gt;init&lt;/code&gt; + &lt;code&gt;__&lt;/code&gt;) to define a suite of \"magic\" functions. The exact way to pronounce these strange functions isn't settled, but most people use \"dunder\" to refer to the double underscore, so &lt;code&gt;__init__()&lt;/code&gt; is referred to as the \"dunder init dunder\" function, or \"dunder init\" for short.\n\nThe __init__() function is the initializer method for a Class. It gets passed whatever arguments are provided when a class is created. For example, if we called T = Temperature(10, 'F'), the __init__() method would automatically be passed these two arguments.\nWe define the __init__ method just like any other function, with one difference: It always includes self as its first argument:\n\nclass Temperature:\n    \n    def __init__(self, value=74.0, unit='F'):\n        self.value = value\n        self.unit = unit\n\n\nWhy self?\nIt‚Äôs not at all obvious why we need to add an extra parameter (which, by convention is always called self ) to the initializer method. Even weirder is the fact that we need to add this extra parameter, self to every class method! To beginning Pythonistas, the concept of self is deeply strange.\nHowever, there is a fairly straight-foward reason for its existence.\nPython functions ‚Äì like functions in most programming languages ‚Äì can only manipulate data that exist within the function itself. Speaking generally, Python functions aren‚Äôt supposed to manipulate data that they haven‚Äôt been passed via argument. The consequences of this ‚Äúseparation of namespaces‚Äù is that ‚Äì paradoxically ‚Äì a class method can‚Äôt operate on class data unless the class data itself is passed into the method!\nBy convention, we use the self parameter to allow us to work with the properties of a class within our class methods. If we didn‚Äôt include this extra parameter, the functions that we write inside of a class wouldn‚Äôt even be able to access the attributes of the class that they were inside of!\nEssentially, you can think of self as a placeholder for a class instance. In fact, Python automatically passes a class instance into any class function whenever that class function is called. The class instance argument is inserted by Python before any other arguments. Therefore, we include the self parameter at the beginning of every class method (remember methods and functions are the same thing), knowing that Python will pass a class instance into our function first, and all of the attributes of that instance will get assigned to the self parameter.\nThere are many, many other explanations out there about the need for the self parameter in class methods.\nBecause the use of self in Python is such a confusing concept, there have also been formal proposals to get rid of self and warnings about its misuse in popular culture.\nThere is even a blog post about the necessity self from the creator of self‚Ä¶ himself.\n\nüêç &lt;b&gt;Bottom Line.&lt;/b&gt; Whenever you write a class function, you will need to include &lt;code&gt;self&lt;/code&gt; as the first positional parameter in the function definition. In addition, whenever you want to access class attributes within a class method, you will need to use &lt;code&gt;self&lt;/code&gt; as the object that contains the data.\n\n\nUsing the __init__() function\nWe almost never call the __init__() function directly. Instead, the Class constructor function calls it for us. We already saw that a class instance is created using the constructor function like my_temp = Temperature(). With our new __init__() function defined for the Temperature class, we can now create Temperature instances with any data we want:\nT = Temperature(83.2, 'F')\nprint(T.value)\n&gt;&gt;&gt; 83.2"
  },
  {
    "objectID": "interactive_sessions/99_functions_objects_classes.html#try-it.-4",
    "href": "interactive_sessions/99_functions_objects_classes.html#try-it.-4",
    "title": "Session 1-5: Functions & Classes",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nCopy the example code above to create an new Temperature class that contains an init method that sets the value and units. Create a few new Temperature instances with different values and units.\n\nUser Defined Class Methods\nWe often want to create our own methods that allow us to manipulate and work with class data. For example, now that we have a Temperature class, we might want to create a method that allows us to get the value of temperature in any unit. We can add this functionality by defining a class method. The class method is just the same as every other Python function, except, like __init__(), it has self as its first argument:\n\nclass Temperature:\n    \n    def __init__(self, value=74.0, unit='F'):\n        self.value = value\n        self.unit = unit\n        self.temp_K = self.get_K()\n\n    def get_K(self):\n        \n        unit = self.unit\n        T = self.value\n\n        # If T is in Fahrenheit, convert to C:\n        if unit == 'F':\n            T = (T - 32) * (5./9.)\n            unit = 'C' # Re-assign unit to C\n        \n        # If T is in Celsius, convert to Kelvin\n        if unit == 'C':\n            T = T + 273.15\n\n        return T\n    \nIn this example, we‚Äôve added a class function called get_K that always returns the Temperature object‚Äôs value in degrees Kelvin. You will notice that we can even use this function inside the __init__() function. This means that after creating an instance of an object, we will end up with the temp_K property of the object set for us automatically.\nWe can also run any class methods by calling it directly. Just as we used . notation to access class data, we use . notation to access class methods:\n\nmy_temp = Temperature(50,'C')\nmy_temp.get_K()\n&gt;&gt;&gt; 323.15"
  },
  {
    "objectID": "interactive_sessions/99_functions_objects_classes.html#try-it.-5",
    "href": "interactive_sessions/99_functions_objects_classes.html#try-it.-5",
    "title": "Session 1-5: Functions & Classes",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nCopy the example code above to create an new Temperature class that contains the get_K method, which is used to set the value of temp_K during intialization. Create a few new Temperature instances with different values and units and test to make sure temp_K is being set correctly.\n\nüìö  &lt;b&gt; Practice 5. &lt;/b&gt; \nUse the cell below to create a new &lt;code&gt;Temperature&lt;/code&gt; class that contains a user-defined class function of your own design. This function can do anything you want. It doesn't need to be fancy or even useful, just make sure you test your function by creating an instance of &lt;code&gt;Temperature&lt;/code&gt; class and running the function.\n\n\nTwo additional - and very useful - Class ‚ÄúMagic‚Äù Methods\nIn addition to the __init__() function, there are some other useful ‚Äúmagic‚Äù class methods. The first, __repr__ is a function that allows you to define how a Class object represents itself. For example, check out this example for a string (str) variable:\n\nSIGMA = 5.67e-8 # Create a variable that contains the Stefan-Boltzmann constant\n\nSIGMA               # What happens if you just execute a line that contains the variable? \n&gt;&gt;&gt; 5.67e-08\nYou see that when the variable SIGMA is invoked, the Python interpreter returns 5.67e-08, which is not exactly what you wrote when you assigned SIGMA. That‚Äôs because what is happening ‚Äúbehind the scenes‚Äù is that Python is calling the __repr__() function for the object SIGMA:\nSIGMA.__repr__()\n&gt;&gt;&gt; 5.67e-08\nThere is a similar magic function, __str__() that is called whenever print() is invoked on an object:\nprint(SIGMA)\n&gt;&gt;&gt; 5.67e-08\n\nSIGMA.__str__()\n&gt;&gt;&gt; 5.67e-08\nIn the case of float variables (type(SIGMA) is float), __repr__() and __str__() return the same thing. But they don‚Äôt have to. Look at this example, using a datetime object, which is part of the standard Python library and the primary object for dealing with time/date information in Python:\nfrom datetime import datetime\n\ncurrent_time = datetime.now()\n\ncurrent_time\n&gt;&gt;&gt; datetime.datetime(2020, 4, 17, 12, 50, 52, 778357) # Your time will be different!\n\nprint(current_time)\n&gt;&gt;&gt; 2020-04-17 12:51:38.750213\nWe see that the representation of a datetime object (created by the class‚Äôs __repr__() function) is different than the class‚Äôs __str__() function.\nYou can create these functions inside a class in the same way you created the __init__() function:\nclass Temperature:\n    \n    def __init__(self, value=74.0, unit='F'):\n        self.value = value\n        self.unit = unit\n\n    def __repr__(self):\n        return \"Temperature({value},¬∞{unit})\".format(value=self.value,unit=self.unit)\n\n    def __str__(self):\n        return \"The temperature is {value} ¬∞{unit}\".format(value=self.value, unit=self.unit)\n\n‚úèÔ∏è &lt;b&gt; Try it. &lt;/b&gt;  \nUse the example above to define a new &lt;code&gt;Temperature&lt;/code&gt; class that adds &lt;code&gt;__repr__&lt;/code&gt; and &lt;code&gt;__str__&lt;/code&gt; methods. Test out the &lt;code&gt;__str__()&lt;/code&gt; method by creating an instance and using the &lt;code&gt;print()&lt;/code&gt; command. \n\n\n\nDocumenting Classes\nJust like functions, classes should contain docstrings. The format and content of docstrings is similar to a function, but there is a need for even more description. This is because the docstrings should include information about all of the attributes of the class as well as any class methods that are defined. So for a simple Weather class we might have something like this:\nclass Weather:\n    \"\"\"\n    A class used to represent the weather\n\n    Attributes\n    ----------\n    \n    temperature : float\n        air temperature, in deg-C\n        \n    relative_humidity : float\n        relative humidity, in %\n    \n    pressure : float\n        air pressure, in kPa\n    \n    Methods\n    -------\n    \n    sat_vap_pressure()\n        returns the saturation vapor pressure for the current weather condition\n        \n    \"\"\"\n    \n    def __init__(self, temp, RH, P):\n        \"\"\" \n        Parameters\n        ----------\n        temp : float\n            air temperature, in ¬∞C\n        \n        RH : float\n            relative humidity, in %\n        \n        P : float\n            pressure, in kPa\n        \"\"\"\n        \n        self.temperature = temp\n        self.relative_humidity = RH\n        self.pressure = P \n        \n    def sat_vap_pressure(self):\n        \"\"\" Determines Saturation Vapor Pressure\n        \n        Uses the Tetens equation to estimate saturation vapor pressure (svp) given air Temp.\n        \n        P = 0.61078 * exp((17.27 * T)/(T + 237.3))\n        \n        where P is svp in kPa and T is air temperature in ¬∞C.\n        \n        \n        Returns:\n        --------\n        \n        P, saturation vapor pressure, in kPa\n        \n        \"\"\"\n        from math import exp\n        \n        P = 0.61078 * exp(17.27*self.C)/(self.T + 237.3)\n        \n        return P\n\nüêç ABC!\nNotice how there are more docstrings in this definition than there is code! This is because the concept of abstraction ‚Äì creating classes and functions that represent general concepts and methods ‚Äì requires a high degree of documentation in order for the abstractions to be used correctly.\nThe same code written in a notebook cell (without abstraction) would be easier to read and require much less documentation.\n\n\nüìö  &lt;b&gt; Practice 6. &lt;/b&gt; \nUse the cell below to create a final version of your &lt;code&gt;Temperature&lt;/code&gt; class that includes docstrings. Check to see if your docstrings are working using the &lt;code&gt;help()&lt;/code&gt; function."
  },
  {
    "objectID": "interactive_sessions/99_functions_objects_classes.html#loading-functions-and-classes",
    "href": "interactive_sessions/99_functions_objects_classes.html#loading-functions-and-classes",
    "title": "Session 1-5: Functions & Classes",
    "section": "Loading functions and classes",
    "text": "Loading functions and classes\nIn the last practice cell, you probably noticed that your class definition is getting pretty large. While it‚Äôs nice to be able to edit this code easily in your notebook, once you have settled on a function or class definition, it is often helpful to move the definitions out of your notebook and just load them when you need them. Python uses the import function to load objects from external libraries and files.\n\nMoving your class definition to a new file"
  },
  {
    "objectID": "interactive_sessions/99_functions_objects_classes.html#practice-7.",
    "href": "interactive_sessions/99_functions_objects_classes.html#practice-7.",
    "title": "Session 1-5: Functions & Classes",
    "section": "üìö  Practice 7. ",
    "text": "üìö  Practice 7. \nFollow the directions below to save your final Temperature class definition into a new file called temperature.py\n\nGo to the JupyterLab File menu and click New -&gt; Text File\n\n\n\nCopy the entire Temperature class definition you created during Practice 5 and paste it into the text file.\nRename the text file temperature.py. You can do this easily by right-clicking on the filename in the file‚Äôs tab (see the image below)\n\n\n\nüêç &lt;b&gt;Note:&lt;/b&gt;Make sure you save your file with the &lt;code&gt;.py&lt;/code&gt; extension and not &lt;code&gt;.txt&lt;/code&gt;. You will know if you saved it correctly if the file appears with python code formating as in the image below:\n\n\n\nSave the new file. This will create a file called temperature.py in your current directory.\n\n\nImporting your class from the temperature.py file.\nAssuming you were able to save your file correctly, and that your Temperature class definition doesn‚Äôt have any errors in it, you can import the Temperature class into your notebook like this:\n\nfrom temperature import Temperature\n\nT = Temperature(98.6, unit='F')"
  },
  {
    "objectID": "interactive_sessions/99_functions_objects_classes.html#try-it.-6",
    "href": "interactive_sessions/99_functions_objects_classes.html#try-it.-6",
    "title": "Session 1-5: Functions & Classes",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nLoad your Temperature class from the temperature.py file using the import command. Make some new Temperature instances to ensure that your class loaded correctly and works okay."
  },
  {
    "objectID": "interactive_sessions/vscode_setup.html#prerequisites",
    "href": "interactive_sessions/vscode_setup.html#prerequisites",
    "title": "Installing and configuring VSCode for Python development",
    "section": "Prerequisites:",
    "text": "Prerequisites:\n\nEnsure you have Visual Studio Code installed.\nInstall the Python extension for VSCode from the Visual Studio Code Marketplace.\nFollow the previous instructions to set up the eds217_2023 environment on your machine."
  },
  {
    "objectID": "interactive_sessions/vscode_setup.html#instructions",
    "href": "interactive_sessions/vscode_setup.html#instructions",
    "title": "Installing and configuring VSCode for Python development",
    "section": "Instructions:",
    "text": "Instructions:\n\n1. Open the eds217_2023 Directory in VSCode:\nLaunch VSCode, then go to File &gt; Open Folder (or File &gt; Open on MacOS) and select the eds217_2023 directory that you cloned from GitHub.\n\n\n2. Selecting the Python Interpreter:\nOnce the folder is open in VSCode:\n\nClick on the Python version in the corner of the VSCode window.\nA list of available Python interpreters will appear at the top. Look for the one that corresponds to the eds217_2023 environment. It should look something like this: Python (eds217_2023).\nClick on it to select it. This ensures that any Python file or Jupyter notebook you open in VSCode will use the eds217_2023 environment.\n\n\n\n3. Verify the Jupyter Kernel:\nIf you‚Äôre working with Jupyter notebooks in VSCode:\n\nOpen or create a new Jupyter notebook (.ipynb file).\nIn the top-right corner of the notebook editor, you‚Äôll see the kernel that the notebook is using. Ensure it says Python (eds217_2023). If not, click on it and select Python (eds217_2023) from the dropdown list.\n\n\n\n4. Troubleshooting:\nIf you don‚Äôt see the eds217_2023 environment in the list of available interpreters or kernels:\n\nEnsure you‚Äôve activated the environment in your terminal and installed the Jupyter kernel as described in the previous instructions.\nRestart VSCode.\n\nThere are a ton of great tutorials that can help you make the most of VSode. Here are some of our favorites:\n\n\nCode\nfrom IPython.display import YouTubeVideo\n\ntutorial_ids = [\"h1sAzPojKMg\",\"E9U-EBG8jVk\", \"zulGMYg0v6U\"]\nfor tutorial in tutorial_ids:\n    display(YouTubeVideo(tutorial))"
  },
  {
    "objectID": "interactive_sessions/Setting_up_local.html#general-plan",
    "href": "interactive_sessions/Setting_up_local.html#general-plan",
    "title": "Setting up your local Python",
    "section": "General Plan",
    "text": "General Plan\nThere are many ways to set up your local machine to run maintanable python data science code. In addition to the usual need to build your code within a repository, any strategy for local computation depends on three critical components that must work well together: Computational isolation, Functional consistency, Developer efficiency.\nIn practice, these three components necessitate three fundamental and inter-related progamming tools:\n\nA tool for managing computing environments\nA tool for managing python packages\nAn tool for managing python code, which is usually an integrated development environment (IDE) in which editing and running code occur interactively.\n\nWe are about to install tools that will meet these three needs on your computer and ensure a successful start on your python journey. I‚Äôve included details about some of the tools we will not use, and you‚Äôll see them mentioned (and discussed) across the Pythonverse. Don‚Äôt be afraid to try new tools!"
  },
  {
    "objectID": "interactive_sessions/Setting_up_local.html#managing-computing-environments",
    "href": "interactive_sessions/Setting_up_local.html#managing-computing-environments",
    "title": "Setting up your local Python",
    "section": "1. Managing Computing Environments",
    "text": "1. Managing Computing Environments\nThere are many options for managing computing environments.\nThese days, a common method is to use containers, in which an entire computational system (including processes, memory, disk space) is spun up as an isolated service on your local (or remote). Systems such as docker or python-specific shiv allow for isolated packaging and execution of python programs.\nA more common approach on local machines is to use the condaenvironment management system. Conda is developed by Anaconda, and is the most widely used environment management system in python data science. Other local solutions include venv, which is part of the main python distribution.\nIn this course, we will use conda for our environment management. We will be able to access conda through the terminal/command line, within a terminal inside our IDE, or even inside our notebooks themselves."
  },
  {
    "objectID": "interactive_sessions/Setting_up_local.html#managing-python-packages",
    "href": "interactive_sessions/Setting_up_local.html#managing-python-packages",
    "title": "Setting up your local Python",
    "section": "2. Managing Python Packages",
    "text": "2. Managing Python Packages\nOnce a computing environment is created in conda, there is a need to manage the individual packages within the environment. Once again, there are many ways to accomplish this task in python.\nFirstly, conda itself is a sophisticated package management system! The conda progra, is able to download and install almost any python package and often it is possible to install binaries of packages directly without the need for local compilation. conda also manages package dependencies, ensuring libraries are inter-operable.\nIn addition to conda, there is the Package Installer for Python pip. pip can install any package hosted on the Python Package Index (PyPI), as well as packages hosted on github and even packages you‚Äôve made on your local machine.\nRecently, a new entry to package management, poetry has started to become popular. poetry focuses on making package management - and especially dependency management between and amongst packages - much easier. It also simplifies building and packaging your own code for distribution.\nIn this class, we will use conda for our package management. Because conda also includes pip, we get all the advantages of conda (faster installs, easy integration with IDEs) without losing the broad capabilities of pip. In a few years, we might be using poetry, but for now, conda is still the most common tool you will see in professional python development shops, so it‚Äôs the one you should know best.\nConda can create new environments using a markup language specification called yaml. We will use an environment file created for our course to create an eds217_2023 environment on your local machine."
  },
  {
    "objectID": "interactive_sessions/Setting_up_local.html#managing-code-and-execution-ides",
    "href": "interactive_sessions/Setting_up_local.html#managing-code-and-execution-ides",
    "title": "Setting up your local Python",
    "section": "3. Managing code and execution (IDEs)",
    "text": "3. Managing code and execution (IDEs)\nFinally, we come to the tool you will use most when coding on your local computer ‚Äì the Integrated Development Environment, or IDE. The possibilities for IDEs is even more expansive than for either of the other tools. Common python IDEs include:\n\nVisual Studio Code\nJupyter Notebooks\nJupyter Labs\nSpyder\nPyCharm\nData Spell\nAtom\nRStudio\n\nAny of these could work well for a python data science workflow, but definitely some have more features focused on data science than others. For example, PyCharm is more focused on software engineering, but a new IDE called Data Spell, by the same company (Jet Brains), is squarely centered on data science workflows. That tool is too new (and niche) to spend a lot of time on in this course.\nThe best IDE is usually the one you are most familiar with. For that reason, RStudio isn‚Äôt a terrible choice for an IDE. Although you will only be able to write code in quarto (.qmd) files, RStudio could be sufficient for many of your python needs.\nHowever, if you‚Äôre going to develop python code professionally, you‚Äôre not going to see many teams using RStudio. The reason is because most python data science is developed in native python formats (.py files) or in Jupyter Notebook formats (.ipynb). RStudio can‚Äôt edit either of these formats. So it‚Äôs best to use an IDE that is designed to efficiently parse these files and allow you to execute code directly.\nWe will use both .py files and .ipybd files in this class. For notebooks, you could always edit them in a browser running Jupyter or JupyterLab However, it‚Äôs probably best to learn to use a non-browser IDE if possible, because that will provide more opportunities for customization and removes a layer of complication required when executing your code.\nToday we will look at both RStudio (for .qmd files) and Visual Studio Code (VSCode).\nHaving covered all the bases, we need to go ahead and get to work. Here are the steps for getting our class environment working on your machine, and within your IDE."
  },
  {
    "objectID": "interactive_sessions/jupyter_setup.html#open-a-terminal",
    "href": "interactive_sessions/jupyter_setup.html#open-a-terminal",
    "title": "jupyter setup for EDS 217",
    "section": "Open a terminal",
    "text": "Open a terminal\nWindows: Open PowerShell or Command Prompt\nMac/Linux: Open Terminal"
  },
  {
    "objectID": "interactive_sessions/jupyter_setup.html#activate-the-eds217_2023-environment",
    "href": "interactive_sessions/jupyter_setup.html#activate-the-eds217_2023-environment",
    "title": "jupyter setup for EDS 217",
    "section": "Activate the EDS217_2023 environment",
    "text": "Activate the EDS217_2023 environment\nconda activate eds217_2023"
  },
  {
    "objectID": "interactive_sessions/jupyter_setup.html#change-directory-to-the-root-of-the-class-repository",
    "href": "interactive_sessions/jupyter_setup.html#change-directory-to-the-root-of-the-class-repository",
    "title": "jupyter setup for EDS 217",
    "section": "Change directory to the root of the class repository",
    "text": "Change directory to the root of the class repository\ncd path/to/eds217_2023\nNote: Your path will be different depending on where you cloned your repo in the previous step.\nYou can find the path by right-clicking on the folder in the file explorer and selecting ‚ÄúProperties‚Äù (Windows) or ‚ÄúGet Info‚Äù (Mac)."
  },
  {
    "objectID": "interactive_sessions/jupyter_setup.html#launch-the-server",
    "href": "interactive_sessions/jupyter_setup.html#launch-the-server",
    "title": "jupyter setup for EDS 217",
    "section": "Launch the server",
    "text": "Launch the server\njupyter notebook\nA browser window should open automatically. If it doesn‚Äôt, you can copy the URL from the terminal and paste it into your browser."
  },
  {
    "objectID": "interactive_sessions/jupyter_setup.html#creating-a-new-notebook",
    "href": "interactive_sessions/jupyter_setup.html#creating-a-new-notebook",
    "title": "jupyter setup for EDS 217",
    "section": "Creating a new notebook",
    "text": "Creating a new notebook\nYou can create a new notebook by clicking the ‚ÄúNew‚Äù button in the upper right corner of the file browser and selecting ‚Äúnotebook‚Äù. This will create a new notebook in the current directory. The default name is Untitled.ipynb. We can change that later, so for now, just open the file by clicking on it."
  },
  {
    "objectID": "interactive_sessions/jupyter_setup.html#selecting-a-kernel",
    "href": "interactive_sessions/jupyter_setup.html#selecting-a-kernel",
    "title": "jupyter setup for EDS 217",
    "section": "Selecting a kernel",
    "text": "Selecting a kernel\nA kernel is the ‚Äúcomputational engine‚Äù that runs your notebook. It is the thing that actually executes the code you write. When you open a new notebook, you will be prompted to select a kernel. Select the eds217_2023 kernel. You can set our class environment as the default kernel by clicking the ‚ÄúAlways start the preferred kernel‚Äù button."
  },
  {
    "objectID": "interactive_sessions/jupyter_setup.html#the-notebook-interface",
    "href": "interactive_sessions/jupyter_setup.html#the-notebook-interface",
    "title": "jupyter setup for EDS 217",
    "section": "The notebook interface",
    "text": "The notebook interface\nIf everything has gone to plan, you should now be seeing a browser window that looks a lot like this:\n\n\n\nnotebook interface\n\n\nLet‚Äôs take a quick tour of the interface together.\n\nThe title bar\nThe title bar is the row of text at the top of the notebook. It contains the name of the notebook as well as the time since the last save (or ‚ÄúCheckpoint‚Äù). You can change the name of the notebook by clicking on the name and typing a new one.\nChange the title of your notebook to test_environment\n\n\nThe toolbar\nThe toolbar is the row of buttons at the top of the notebook. It contains buttons for saving the notebook, adding cells, running cells, and more. We‚Äôll go over these in more detail later. For now, the important point to notice is the text at the far right of the toolbar. It should say eds217_2023. This is the name of the kernel that is running your notebook. If you click on it, you can change the kernel (but don‚Äôt do that now!)"
  },
  {
    "objectID": "interactive_sessions/jupyter_setup.html#saving-a-notebook",
    "href": "interactive_sessions/jupyter_setup.html#saving-a-notebook",
    "title": "jupyter setup for EDS 217",
    "section": "Saving a notebook",
    "text": "Saving a notebook\nYou save a notebook by clicking the ‚ÄúSave‚Äù button in the upper left corner of the toolbar (it looks like a floppy disk). Do this now. You will see the ‚ÄúLast Checkpoint‚Äù update:\n\n\n\nlast checkpoint"
  },
  {
    "objectID": "interactive_sessions/jupyter_setup.html#working-with-cells",
    "href": "interactive_sessions/jupyter_setup.html#working-with-cells",
    "title": "jupyter setup for EDS 217",
    "section": "Working with cells",
    "text": "Working with cells\nThere are two cell types in a jupyter notebook: code cells and markdown cells. Code cells are for writing code, and markdown cells are for writing text. You can change the type of a cell by clicking on the dropdown menu in the toolbar and selecting the type you want.\nThe notebook has been initialized with a single cell. You can tell it‚Äôs a code cell because it has [ ]: to the left of it. We want a markdown cell instead, so we can add a title to this notebook and describe what it‚Äôs for.\nChange the cell type to markdown by clicking the dropdown menu in the toolbar and selecting ‚ÄúMarkdown‚Äù\nYou can now type text into the cell. Try typing a title for the notebook and a short description of what it‚Äôs for. You can use markdown syntax to format the text. If you need a refresher on markdown syntax, you can find one here.\n\n\n\nmarkdown cell"
  },
  {
    "objectID": "interactive_sessions/jupyter_setup.html#running-a-cell",
    "href": "interactive_sessions/jupyter_setup.html#running-a-cell",
    "title": "jupyter setup for EDS 217",
    "section": "Running a cell",
    "text": "Running a cell\nOnce you‚Äôve written your text, you can render it by running the cell. You can do this by clicking the ‚ÄúRun‚Äù button in the toolbar, or by pressing Shift+Enter. Do this now.\n\n\n\nrendered markdown cell\n\n\nNote: You can always edit the cell again by double-clicking on it. Re-rendering the cell by running it again will update the cell output."
  },
  {
    "objectID": "interactive_sessions/jupyter_setup.html#adding-a-cell-to-a-notebook",
    "href": "interactive_sessions/jupyter_setup.html#adding-a-cell-to-a-notebook",
    "title": "jupyter setup for EDS 217",
    "section": "Adding a cell to a notebook",
    "text": "Adding a cell to a notebook\nYou can add a cell to a notebook by clicking the ‚Äú+‚Äù button in the toolbar. This will add a cell below the currently selected cell.\nYou can also add a cell above +[] or below []+ the currently selected cell by clicking the buttons in the cell‚Äôs toolbar on the far right. These buttons have keyboard shortcuts (A) and (‚ÄòB‚Äô).\nPress B to add a cell below the currently selected cell\nThe default new cell type is code."
  },
  {
    "objectID": "interactive_sessions/jupyter_setup.html#deleting-a-cell",
    "href": "interactive_sessions/jupyter_setup.html#deleting-a-cell",
    "title": "jupyter setup for EDS 217",
    "section": "Deleting a cell",
    "text": "Deleting a cell\nYou can delete a cell by clicking the scissors button in the toolbar. This will delete the currently selected cell. There is also a trashcan icon in the cell toolbar that will delete the cell. Finally, you can also use the keyboard shortcut D,D (press D twice).\nPractice adding, deleting, and changing the type of cells until you are comfortable with the process"
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html",
    "href": "interactive_sessions/2-2_structured_data.html",
    "title": "Session 2-2: Structured Data in Python",
    "section": "",
    "text": "‚¨ÖÔ∏è Previous Session | üè† Course Home | ‚û°Ô∏è Next Session |\nProbably the easiest mental model for thinking about structured data is a spreadsheet. You are all familiar with Excel spreadsheets, with their numbered rows and lettered columns. In the spreadsheet, data is often ‚Äústructured‚Äù so that each row is an entry, and each column is perhaps a variable recorded for that entry.\nEnvironmental data map really well to this model, especially data collected over time. A spreadsheet of weather observations would have a new row for each observation period, and then the columns would be the meteorological data that was collected at that time. As we move into our data science tools, we will see that a major aspect of these tools is creating efficient ways of working with structured data.\nThat being said, Python has a number of built-in data types that can be used to organize data in a structured manner. These basic data types containing structured data are all lumped into a single broad category called collections. Within these collection data types, some of the data types are sequences, which means that the items in the collection have a deterministic ordering. You‚Äôve already used the two most common sequence data types: the string and the list. A string is simply an ordered collection of characters, while a list data type structures a collection of anything into an ordered series of elements that can be referenced by their position in the list.\nIn this lesson, we are going to learn about an additional sequence, called a tuple, as well as two collection data types called sets (set) and dictionaries (dict)."
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html#dictionaries",
    "href": "interactive_sessions/2-2_structured_data.html#dictionaries",
    "title": "Session 2-2: Structured Data in Python",
    "section": "1. Dictionaries ",
    "text": "1. Dictionaries \n\nTLDR: Dictionaries are a very common collection type that allows data to be organized using a key:value framework. Because of the similarity between key:value pairs and many data structures (e.g.¬†‚Äúlookup tables‚Äù), you will see Dictionaries quite a bit when working in python\n\nThe first collection we will look at today is the dictionary, or dict. This is one of the most powerful data structures in python. It is a mutable, unordered collection, which means that it can be altered, but elements within the structure cannot be referenced by their position and they cannot be sorted.\nYou can create a dictionary using the {}, providing both a key and a value, which are separated by a :.\nenvironmental_disciplines = {\n    'ecology':'The relationships between organisms and their environments.',\n    'hydrology':'The properties, distribution & effects of water on the surface, subsurface, & atmosphere.',\n    'geology':'The origin, history, and structure of the earth.',\n    'meteorology':'The phenomena of the atmosphere, especially weather and weather conditions.'\n    }\n\nüêç &lt;b&gt;Note.&lt;/b&gt; The use of whitespace and indentation is important in python. In the example above, the dictionary entries are indented relative to the brackets &lt;code&gt;{&lt;/code&gt; and &lt;code&gt;}&lt;/code&gt;. In addition, there is no space between the &lt;code&gt;'key'&lt;/code&gt;, the &lt;code&gt;:&lt;/code&gt;, and the &lt;code&gt;'value'&lt;/code&gt; for each entry. Finally, notice that there is a &lt;code&gt;,&lt;/code&gt; following each dictionary entry. This pattern is the same as all of the other &lt;i&gt;collection&lt;/i&gt; data types we've seen so far, including &lt;b&gt;list&lt;/b&gt;, &lt;b&gt;set&lt;/b&gt;, and &lt;b&gt;tuple&lt;/b&gt;.\n\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nenvironmental_disciplines = {\n    'ecology':'The relationships between organisms and their environments.',\n    'hydrology':'The properties, distribution & effects of water on the surface, subsurface, & atmosphere.',\n    'geology':'The origin, history, and structure of the earth.',\n    'meteorology':'The phenomena of the atmosphere, especially weather and weather conditions.'\n}\n\n\n\nAccessing elements in Dictionaries \nAccess an element in a dictionary is easy if you know what you are looking for. For example, if I want to know the definition of ecology, I can simply retireve the value of this defition using the key as my index into the dictionary:\nenvironmental_disciplines['ecology']\n&gt;&gt;&gt; 'The relationships between organisms and their environments.'"
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html#try-it.",
    "href": "interactive_sessions/2-2_structured_data.html#try-it.",
    "title": "Session 2-2: Structured Data in Python",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nTry accessing the some of the definitions in the environmental_disciplines dictionary.\n\n\nCode\nenvironmental_disciplines['ecology']\n\n\n'The relationships between organisms and their environments.'\n\n\nBecause dictionaries are mutable, it is easy to add additional entries. This is done using the following notation:\n    environmental_discplines['geomorphology'] =  'The evolution and configuration of landforms.'"
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html#try-it.-1",
    "href": "interactive_sessions/2-2_structured_data.html#try-it.-1",
    "title": "Session 2-2: Structured Data in Python",
    "section": "‚úèÔ∏è Try it.",
    "text": "‚úèÔ∏è Try it.\nBiogeochemistry is defined as ‚Äúthe chemical, physical, geological, and biological processes and reactions that govern the composition of the natural environment.‚Äù Add this discpline to the dictionary environmental_disciplines.\n\n\nCode\nenvironmental_disciplines['biogeochemistry'] = 'the chemical, physical, geological, and biological processes and reactions that govern the composition of the natural environment.'\nprint(environmental_disciplines['biogeochemistry'])\n\n\nthe chemical, physical, geological, and biological processes and reactions that govern the composition of the natural environment.\n\n\n\nAccessing dictionary keys and values \nEvery dictionary has builtin methods to retrieve its keys and values. These functions are called, appropriately, keys() and values()\n\ndisciplines = environmental_disciplines.keys()\nprint(disciplines)\n&gt;&gt;&gt; dict_keys(['ecology', 'hydrology', 'geology', 'meteorology', 'biogeochemistry'])\n\ndefinitions = environmental_disciplines.values()\nprint(definitions)\n&gt;&gt;&gt; dict_values(\n    ['The relationships between organisms and their environments.', \n     'The properties, distribution & effects of water on the surface, subsurface, & atmosphere.', \n     'The origin, history, and structure of the earth.', \n     'The phenomena of the atmosphere, especially weather and weather conditions.', \n     'The chemical, physical, geological, and biological processes and reactions that govern the composition of the natural environment.'])\n\nüêç Note. The keys() and values() functions return a dict_key object and dict_values object, respectively. Each of these objects contains a list of either the keys or values. You can force the result of the keys() or values() function into a list by wrapping either one in a list() command.\nFor example: key_list = list(environmental_disciplines.keys()) will return a list of the keys in environmental_disciplines\n\n\n\nCode\nfor key, value in environmental_disciplines.items():\n    print(key, value)\n    environmental_disciplines[key] = value.capitalize()\n\n\necology The relationships between organisms and their environments.\nhydrology The properties, distribution & effects of water on the surface, subsurface, & atmosphere.\ngeology The origin, history, and structure of the earth.\nmeteorology The phenomena of the atmosphere, especially weather and weather conditions.\nbiogeochemistry the chemical, physical, geological, and biological processes and reactions that govern the composition of the natural environment.\n\n\n\n\nLooping through Dictionaries \nPython has an efficient way to loop through all the keys and values of a dictionary at the same time. The items() method returns a tuple containing a (key, value) for each element in a dictionary. In practice this means that we can loop through a dictionary in the following way:\nmy_dict = {'name': 'Homer Simpson',\n           'occupation': 'Nuclear Engineer',\n           'address': '742 Evergreen Terrace',\n           'city': 'Springfield',\n           'state': ' ? '\n          }\n\nfor key, value in my_dict.items():\n    print(f\"{key.capitalize()}: {value}.\")\n\n\n&gt;&gt;&gt; Name: Homer Simpson.\n    Occupation: Nuclear Engineer.\n    Address: 742 Evergreen Terrace.\n    City: Springfield.\n    State:  ? ."
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html#try-it.-2",
    "href": "interactive_sessions/2-2_structured_data.html#try-it.-2",
    "title": "Session 2-2: Structured Data in Python",
    "section": "‚úèÔ∏è Try it.",
    "text": "‚úèÔ∏è Try it.\nLoop through the environmental_disciplines dictionary and print out a sentence providing the definition of each subject (e.g.¬†‚ÄúEcology is the study of‚Ä¶.‚Äù).\n\nAccessing un-assigned elements in Dictionaries \nAttempting to retrieve an element of a dictionary that doesn‚Äôt exist is the same as requesting an index of a list that doesn‚Äôt exist - Python will raise an Exception. For example, if I attempt to retrieve the definition of a field that hasn‚Äôt been defined, then I get an error.\nenvironmental_disciplines['xenohydrology']\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\n&lt;ipython-input-46-d4d91bf18209&gt; in &lt;module&gt;\n----&gt; 1 environmental_disciplines['xenohydrology']\n\nKeyError: 'xenohydrology'\nWhile it‚Äôs easy to determine what indicies are in a list using the len() command, it‚Äôs sometimes hard to know what elements are in a dict (but we‚Äôll learn how soon!). Regardless, to avoid getting an error when requesting an element from a dict, you can use the get() function. The get() function will return None if the element doesn‚Äôt exist:\nunknown_definition = environmental_disciplines.get('xenohydrology')\nprint(unknown_definition)\n&gt;&gt;&gt; None\nThe get() function will also allow you to pass an additional argument. This additional argument specifies a ‚Äúdefault‚Äù value which will be returned for any undefined elements:\nenvironmental_disciplines.get('xenohydrology', 'Discipline not defined.')\n&gt;&gt;&gt; 'Discipline not defined.'\nlist_of_disciplines = ['climatology', 'ecology', 'meteorology', 'geology', 'biogeochemistry']"
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html#try-it.-3",
    "href": "interactive_sessions/2-2_structured_data.html#try-it.-3",
    "title": "Session 2-2: Structured Data in Python",
    "section": "‚úèÔ∏è Try it.",
    "text": "‚úèÔ∏è Try it.\nUsing the list of discplines given above, write a for loop that either prints the definition of the discipline, or prints ‚ÄòDiscipline not defined.‚Äô"
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html#sets",
    "href": "interactive_sessions/2-2_structured_data.html#sets",
    "title": "Session 2-2: Structured Data in Python",
    "section": "2. Sets ",
    "text": "2. Sets \n\nTLDR: Sets are useful for comparing groups of items to determine their overlap or their differences. Sometimes used in data science, but rarely when working with large datasets.\n\nAs opposed to a list or tuple, a set is not a sequence. Although a set is iterable (like the sequences you‚Äôve already met), a set is an unordered collection data type, which means it is not a sequence. However, a set is mutable, which means - like a list - it can be modified after being created. Finally - and most uniquely - a set has no duplicate elements. In this sense, a set in python is very much like a mathematical set.\nWe‚Äôve seen that a list is implemented using [], while a tuple is implemented using ().\nA set is implemented using {}:\nnum_set = {1, 3, 6, 10, 15, 21, 28}\nstr_set = {'hydrology', 'ecology', 'geology', 'climatology'}\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Define set variables\nnum_set = {1, 3, 6, 10, 15, 21, 28}\nstr_set = {'hydrology', 'ecology', 'geology', 'climatology'}\n\n\nAs with all other collections, you can also create a set using the set() function:\nnum_set = set([1, 3, 6, 10, 15, 21, 28, 45, 45, 45, 45, 45])"
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html#try-it.-4",
    "href": "interactive_sessions/2-2_structured_data.html#try-it.-4",
    "title": "Session 2-2: Structured Data in Python",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nUse the set() function to create a set containing the first four prime numbers.\n\n\nCode\nnum_set = set([1, 3, 6, 10, 15, 3, 10, 21, 28])\nprint(num_set)\n\n\n{1, 3, 6, 10, 15, 21, 28}\n\n\n\nMutability \nSets are mutable.\nTo remove an element from a set, use the discard() method:\n\nstr_set.discard('ecology')\n    \nTo add an element from to set, use the add() method:\n\nstr_set.add('oceanography')\n    \nTo add multiple elements to a set at the same time, use the update() method. The items to add should be contained in a list.\n\nstr_set.update(['oceanography', 'microbiology'])"
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html#try-it.-5",
    "href": "interactive_sessions/2-2_structured_data.html#try-it.-5",
    "title": "Session 2-2: Structured Data in Python",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nAdd ‚Äòbiogeochemistry‚Äô and ‚Äòmeteorology‚Äô to str_set and then remove ‚Äòecology‚Äô.\nMany of the same functions that worked on list and tuple also work for a set.\nlen(str_set)\n&gt;&gt;&gt; 4\nThe min() and max() commands can also be used to find the minimum and maximum values in a tuple. For a tuple of strings, this corresponds to the alphabetically first and last elements.\nmin(str_set)\n&gt;&gt;&gt; 'climatology'\n\nmax(str_tuple)\n&gt;&gt;&gt; 'oceanography'"
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html#try-it.-6",
    "href": "interactive_sessions/2-2_structured_data.html#try-it.-6",
    "title": "Session 2-2: Structured Data in Python",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nUse the len(), min(), and max() commands to find the length, minimum, and maximum of num_set.\n\nMixed Data Types in Collections and Sequences \nAs a reminder, it‚Äôs usually a good idea to make sure your sets are all of the same basic data type. The reason is because Python doesn‚Äôt know how to compare the magnitude of different data types.\nWhich is larger: ecology, or the number 3? Python doesn‚Äôt know the answer, and neither do I. If you try to use functions like max or min on a mixed data type set you will get a TypeError exception.\n\nmixed_set = {3, 4, 'ecology', 'biology'}\nmax(mixed_set)\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-49-a4da84ba3cd4&gt; in &lt;module&gt;\n      1 mixed_set = {3, 4, 'ecology', 'biology'}\n----&gt; 2 max(mixed_set)\n\nTypeError: '&gt;' not supported between instances of 'int' and 'str'\n\n\nSet Methods \nGiven their similarity to mathematical sets, there are some specific functions that allow us to compare and combine the contents of different sets.\n\nUnion\nA union of sets contains all the items that are in any of the sets.\nThe union of sets A and B is defined as $ A B $.\n\n\n\nimage\n\n\nodds = {1, 3, 5, 7, 9, 11, 13, 15}\nevens = {2, 4, 6, 8, 10, 12, 14, 16}\n\nintegers = odds.union(evens)\nprint(integers)\n\n&gt;&gt;&gt; {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16}\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nknows_python = {'Bart', 'Maggie', 'Homer', 'Lisa', 'Professor Frink', 'Nelson'}\nknows_R = {'Homer', 'Nelson', 'Lisa', 'Marge', 'Ralph', 'Milhouse', 'Ms. Krabappel'}\n\n\n\n\nIntersection\nAn intersection of sets contains all the items that are found in all of the sets.\nThe intersection of sets A and B is defined as $ A B $.\n\n\n\nimage\n\n\n\nsquares = {4, 9, 16, 25, 36, 49}\nmultiples_of_nine = {9, 18, 27, 36, 45}\n\nsquares_divisible_by_nine = squares.intersection(multiples_of_nine)\nprint(squares_divisible_by_nine)\n\n&gt;&gt;&gt; {9, 36}"
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html#try-it.-7",
    "href": "interactive_sessions/2-2_structured_data.html#try-it.-7",
    "title": "Session 2-2: Structured Data in Python",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nUse the intersection() method to determine who knows both python and R.\n\n\nCode\n# knows_both = \n\n\n\nDifference\nAn difference of two sets contains all the items that are in A but not in B.\nThe difference (or relative complement) of set A and B is defined as $ A B $.\n\n\n\nimage\n\n\n\nsquares = {4, 9, 16, 25, 36, 49}\nmultiples_of_nine = {9, 18, 27, 36, 45}\n\nsquares_not_divisible_by_nine = squares.difference(multiples_of_nine)\nprint(squares_not_divisible_by_nine)\n\n&gt;&gt;&gt; {16, 49, 4, 25}\n\nüêç &lt;b&gt;Note.&lt;/b&gt; Because a &lt;b&gt;set&lt;/b&gt; is an &lt;i&gt;unordered&lt;/i&gt; collection, the result of a set function will return elements in an unpredictable order. In the example above, the intersection returned `{16, 49, 4, 25}` rather than `{4, 16, 25, 49}`, which you may have expected."
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html#try-it.-8",
    "href": "interactive_sessions/2-2_structured_data.html#try-it.-8",
    "title": "Session 2-2: Structured Data in Python",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nUse the difference() method to determine who knows R, but does not know Python.\n\nSymmetric Difference\nThis method returns all the items that are unique to each set.\nThe symmetric difference (or disjunctive union) of sets A and B is A \\triangle B (also sometimes written as A \\oplus B)\n\n\n\nimage\n\n\n\nsquares = {4, 9, 16, 25, 36, 49}\nmultiples_of_nine = {9, 18, 27, 36, 45}\n\nsquares_not_divisible_by_nine = squares.symmetric_difference(multiples_of_nine)\nprint(squares_not_divisible_by_nine)\n\n&gt;&gt;&gt; {16, 49, 18, 4, 25, 27, 45}"
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html#try-it.-9",
    "href": "interactive_sessions/2-2_structured_data.html#try-it.-9",
    "title": "Session 2-2: Structured Data in Python",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nUse the symmetric_difference() method to determine who only knows either R or Python.\n\nAdditional Set Methods: isdisjoint(), issubset(), issuperset()\nThere are three additional set functions that allow you to determine the relationships between two sets. Each of these functions returns either True or False, which means they are Boolean operators.\nisdisjoint() determines if two sets are disjoint. It returns True if the contents of two sets are completely distinct, and False if they have any overlap\nodds.isdisjoint(evens)\n&gt;&gt;&gt; True\n\nüêç &lt;b&gt;Note.&lt;/b&gt; Set &lt;i&gt;A&lt;/i&gt; is &lt;b&gt;disjoint&lt;/b&gt; from set &lt;i&gt;B&lt;/i&gt; if, and only if, the &lt;b&gt;intersection&lt;/b&gt; of &lt;i&gt;A&lt;/i&gt; and &lt;i&gt;B&lt;/i&gt; is &lt;code&gt;None&lt;/code&gt;.\n\nissubset() returns True if the content of set A is a subset of set B, and False if it is not a subset.\n\nprimes = {1, 3, 5, 7, 11}\nprimes.issubset(odds)\n&gt;&gt;&gt; True\n\nüêç &lt;b&gt;Note.&lt;/b&gt; Set &lt;i&gt;A&lt;/i&gt; is a &lt;b&gt;subset&lt;/b&gt; of set &lt;i&gt;B&lt;/i&gt; if, and only if, the &lt;b&gt;intersection&lt;/b&gt; of &lt;i&gt;A&lt;/i&gt; and &lt;i&gt;B&lt;/i&gt; is &lt;i&gt;A&lt;/i&gt;.\n\nissupserset() returns True if the content of set A is a superset of set B, and False if it is not a superset.\n\nodds.issuperset(primes)\n&gt;&gt;&gt; True\n\nüêç &lt;b&gt;Note.&lt;/b&gt; Set &lt;i&gt;A&lt;/i&gt; is a &lt;b&gt;superset&lt;/b&gt; of set &lt;i&gt;B&lt;/i&gt; if, and only if, set &lt;i&gt;B&lt;/i&gt; is a subset of &lt;i&gt;A&lt;/i&gt;.\n\nThere is a lot more to learn about dictionaries, including methods for deleting elements, merging dictionaries, and learning about additional collection types like OrderedDict that allow you to preserve the arrangement of dictionary elements (essentially making them sequences). We will keep coming back to them throughout the class. If you want to learn more, check out the great material in our reading: Dictionaries"
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html#tuples",
    "href": "interactive_sessions/2-2_structured_data.html#tuples",
    "title": "Session 2-2: Structured Data in Python",
    "section": "3. Tuples ",
    "text": "3. Tuples \n\nTLDR: Tuples are a kind of list that can‚Äôt be altered. They are not very common in data science applications, but you might run across them from time to time. ‚ÄúNamed Tuples‚Äù allow for the creation of simple structured data ‚Äúobjects‚Äù that don‚Äôt require much coding overhead.\n\nTuples are a type of sequence, similar to list, which you‚Äôve already seen. They primary difference between a tuple and a list is that a tuple is immutable, which means that it‚Äôs value cannot be changed once it is defined. A tuple is implemented using ():\nnum_tuple = (4, 23, 654, 2, 0, -12, 4391)\nstr_tuple = ('energy', 'water', 'carbon')\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Define tuple variables\nnum_tuple = (4, 23, 654, 2, 0, -12, 4391)\nstr_tuple = ('energy', 'water', 'carbon')\n\n\nAs with a list, a tuple may contain mixed data types, but this is not usually recommended.\nBecause they are both sequences, tuples and lists share many of the same methods. For example, just like lists, the len() command returns the length of the tuple.\nlen(str_tuple)\n&gt;&gt;&gt; 3\nThe min() and max() commands can also be used to find the minimum and maximum values in a tuple. For a tuple of strings, this corresponds to the alphabetically first and last elements.\nmin(str_tuple)\n&gt;&gt;&gt; 'carbon'\n\nmax(str_tuple)\n&gt;&gt;&gt; 'water'"
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html#try-it.-10",
    "href": "interactive_sessions/2-2_structured_data.html#try-it.-10",
    "title": "Session 2-2: Structured Data in Python",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nUse the len(), min(), and max() commands to find the length, minimum, and maximum of num_tuple.\n\n\nCode\n# Find the length of num_tuple\n\n# Minimum value of num_tuple\n\n# Maximum value of num_tuple\n\n\n\n\n\nOther ways to create tuples\nTuples can also be constructed by:\n\n\nUsing a pair of parentheses to indicate an empty tuple: ()\n\nUsing a trailing comma for a tuple with a single element: a, or (a,)\n\nSeparating items with commas: a, b, c or (a, b, c)\n\nusing the tuple() built-in function: tuple(iterable).\n\n\nüêç &lt;b&gt;Note.&lt;/b&gt; An &lt;i&gt;iterable&lt;/i&gt; is any object that is capable of returning its contents one at a time. Strings are iterable objects, so &lt;code&gt;tuple('abc')&lt;/code&gt; returns &lt;code&gt;('a', 'b', 'c')&lt;/code&gt;.\n\ntuple('earth')\n&gt;&gt;&gt; ('e', 'a', 'r', 't', 'h')"
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html#try-it.-11",
    "href": "interactive_sessions/2-2_structured_data.html#try-it.-11",
    "title": "Session 2-2: Structured Data in Python",
    "section": "‚úèÔ∏è Try it.",
    "text": "‚úèÔ∏è Try it.\nCreate three separate tuples containing the latitude and longitudes of the following cities:\n\nLos Angeles, CA (34.05, -118.25)\nJohannesburg, South Africa (-26.20, 28.05)\nCairo, Egypt (30.03, 31.23)\nCreate a fourth tuple that is made up of the three tuples (i.e.¬†a ‚Äútuple of tuples‚Äù).\n\n\n\nCode\n# Define a three new tuples, one for each city.\n\n# los_angeles = \n\n# johannesburg = \n\n# singapore = \n\n# Create a new tuple that is a tuple made up of the three city location tuples:\n# tuple_of_tuples = \n\n\n\nIndexing \nAs you learned with lists, any element of a sequence data type can be referenced by its position within the sequence. To access an element in a sequence by its index, use square brackets [].\nIndividual elements of tuples are accessed in the exact same manner as lists:\nnum_tuple[0]\n&gt;&gt;&gt; 4\nnum_tuple[-2]\n&gt;&gt;&gt; -12\nword_tuple = tuple('antidisestablishmentarianism')\n\nword_tuple[14]\n&gt;&gt;&gt; 's'\n\nword_tuple[::3]\n&gt;&gt;&gt; 'aistlhnrnm'"
  },
  {
    "objectID": "interactive_sessions/2-2_structured_data.html#try-it.-12",
    "href": "interactive_sessions/2-2_structured_data.html#try-it.-12",
    "title": "Session 2-2: Structured Data in Python",
    "section": "‚úèÔ∏è  Try it. ",
    "text": "‚úèÔ∏è  Try it. \nUse indexing to create a new tuple from the 2nd element in str_tuple. Find the 3rd element of this new tuple.\n\n\nCode\n# new_tuple = \n\n# 3rd element of new_tuple:\n\n\n\n\n\nImmutability \nAll objects in python are either mutable or immutable. A mutable object is one whose value can change. In contrast, an immutable object has a fixed value. You‚Äôve already been introduced to a few immutable objects including numbers, strings and now, tuples. These objects cannot be altered once created.\n\nüêç &lt;b&gt;Note.&lt;/b&gt;  If you attempt to modify the value of an existing tuple, you will get a &lt;code&gt;TypeError&lt;/code&gt; exception from the Python interpreter.\n\nnum_tuple[0] = 3\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input&gt; in &lt;module&gt;\n----&gt; 1 num_tuple[0] = 1\n\nTypeError: 'tuple' object does not support item assignment\n    \n\n\n\n\nTuple Operations \nBecause they are immutable, tuples do not have the same robust set of functions that lists have. Attempting to change a tuple (for example, by trying to append elements) will raise an AttributeError, because the append method isn‚Äôt available to tuple objects.\n\ntuple_of_colors = ('red', 'blue', 'green', 'black', 'white')\ntuple_of_colors.append('pink') # &lt;- UH-OH!\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-8-857308f688f6&gt; in &lt;module&gt;\n----&gt; 1 tuple_of_colors.append('pink')\n\nAttributeError: 'tuple' object has no attribute 'append'\nInstead of appending data to an existing tuple, when you want to change the contents of a tuple, you need to either create a new one, or modify the variable by re-defining it.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\ntuple_of_colors = ('red', 'blue', 'green', 'black', 'white')\ntuple_of_colors = tuple_of_colors + ('pink',)\nprint(tuple_of_colors)\n\n\n('red', 'blue', 'green', 'black', 'white', 'pink')\n\n\n\n\n\n\nDIVING DEEPER: Named Tuples \nTuples are convenient for storing information that you do not want to change. However, remembering which index should be used for each value can lead to errors, especially if the tuple has a lot of fields and is constructed far from where it is used.\nAs an example, we created the coordinate location of Cairo, Egypt as:\ncairo_location = (30.03, 31.23)\nBut wait‚Ä¶ Are those coordinates stored (latitude, longitude) or (longitude, latitude)? You might think it is easy to sort this out for most cities, but for Cairo it‚Äôs really difficult!\nPython has an additional immutable collection data type called a namedtuple which assigns names, as well as the numerical index, to each member. The namedtuple is part of the standard python library but it is not immediately available. In order to use the namedtuple data type, you first need to import it to your working environment. We will be using the import command quite a bit in order to extend what python can do and take advantage of all the tools that people have developed for environmental data science. For now, we need to import namedtuple from the collections library within python. The code for that looks like this:\nfrom collections import namedtuple\nOnce we import the namedtuple, we can create a new kind of custom data type that we can use to store our locations:\nLocation = namedtuple('Location', ['latitude', 'longitude'])\nIn the code above, the first argument to the namedtuple function is the name of the new tuple object type you want to create. We called this new object type a Location. The second argument is a list of the field names that the Location objects will have. In general, Location objects on Earth are defined by two pieces of information: the latitude and the longitude.\nNow that we‚Äôve defined this new Location object type, we can create a new Location object using this code:\ncairo_location = Location(latitude=30.03, longitude=31.23)\nNote that this code isn‚Äôt that different than the code we used to make a tuple:\ncairo_location = tuple(30.03, 31.23)\nThe difference is that we are using our custom namedtuple type called Location, and we are able to specify exactly which values correspond to the latitude and longitude fields. We can retrieve any field in our Location tuple by specifying the field:\ncairo_location.latitude\n&gt;&gt;&gt; 30.03\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nfrom collections import namedtuple\n\nLocation = namedtuple('Location', ['latitude', 'longitude'])\ncairo_location = Location(latitude=30.03, longitude=31.23)\ncairo_location.latitude, cairo_location.longitude\n\n\n(30.03, 31.23)"
  },
  {
    "objectID": "interactive_sessions/5-1_matplotlib.html",
    "href": "interactive_sessions/5-1_matplotlib.html",
    "title": "Session 5-1: Matplotlib üìà",
    "section": "",
    "text": "‚¨ÖÔ∏è Previous Session | üè† Course Home\nThere are extensive options for plotting in Python ‚Äì some favorites include statistical visualizations in Seaborn and interactive plots for web applications in Bokeh. The original and fundamental library for visualizations in Python, however, is matplotlib.\nMatplotlib was the first plotting library developed for Python and remains the most widely used library for data visualization in the Python community. Designed to resemble graphics in MATLAB, matplotlib is reminiscent of MATLAB in both appearance and functionality. As a result, it is not the easiest library to work with, and deviates from the object-oriented syntax we are familiar with in Python.\nThis session will serve as an introduction to plotting in Python using matplotlib. The nature of matplotlib ‚Äì and figure-making in general ‚Äì¬†is such that the easiest way to learn is by following examples. As such, this exercise is structured a bit differently than the others, so be sure to look carefully at the coded examples. Finally, the best way to learn advanced functions and find help with matplotlib is by exploring the examples in the gallery."
  },
  {
    "objectID": "interactive_sessions/5-1_matplotlib.html#introduction-to-matplotlib",
    "href": "interactive_sessions/5-1_matplotlib.html#introduction-to-matplotlib",
    "title": "Session 5-1: Matplotlib üìà",
    "section": "Introduction to matplotlib",
    "text": "Introduction to matplotlib\n\nAs always, we will begin by importing the required libraries and packages. For plotting, itself, we will use a module of the matplotlib library called  pyplot. The  pyplot module consists of a collection of functions to display and edit figures. As you advance with Python and with data analysis, you may want to explore additional features of  matplotlib, but  pyplot will suit the vast majority of your plotting needs at this stage.\nThe standard import statement for  matplotlib.pyplot  is:\nimport matplotlib.pyplot as plt\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n\nAnatomy of a matplotlib plot\nThe core components of a matplotlib plot are the Figure and the Axes. The Figure is the overall window upon which all components are drawn. It serves as the blank container for plots, as well as other things, such as a legend, color bar, etc. You can (and will) create multiple independent figures, and each figure can hold multiple Axes. To the figure you will add Axes, the area where the data are actually plotted and where associated ticks, labels, etc. live.\nWhen working with a single plot, we will mostly deal with the Figure object and its routines, but we will see the Axes become important as we increase the complexity of our plots.\n\n\n\nBasic plotting\n\n\nWe will start with the most basic plotting routines: plt.plot() and plt.scatter(). The first, plt.plot(), is used to generate a connected line plot (with optional markers for individual data points). plt.scatter(), as the name suggests, is used to generate a scatter plot.\nEach time you want to create a new figure, it is wise to first initialize a new instance of the matplotlib.figure.Figure class on which to plot our data. While this is not required to display the plot, if you subsequently plot additional data without a new Figure instance, all data will be plotted on the same figure. For example, let‚Äôs generate a few functions, y_{\\sin} = \\sin{(x)} and y_{\\cos} = \\cos{(x)}:\n# Generate a 1D array with 300 points between -5 and 5\nx = np.linspace(-5,5,300)\n# Generate sine wave\nysin = np.sin(x)\n# Generate cosine wave\nycos = np.cos(x)\nWe can plot these on the same figure without instancing plt.figure() as follows:\n# Plot sine wave\nplt.plot(x,ysin)\n# Plot cosine wave\nplt.plot(x,ycos)\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Generate a 1D array with 100 points between -5 and 5\nx = np.linspace(-5,5,100)\n# Generate sine wave\nysin = np.sin(x)\n# Generate cosine wave\nycos = np.cos(x)\n\n# Plot sine wave\nplt.plot(x,ysin)\n# Plot cosine wave\nplt.plot(x,ycos)\n\n\n\n\n\nTo create multiple graphs in separate figure windows, however, you need to create new Figure instances as follows:\nfig = plt.figure()\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Plot sine wave\nfig1 = plt.figure()\nplt.plot(x,ysin)\n\n# Plot cosine wave\nfig2 = plt.figure()\nplt.plot(x,ycos)\n\n\n\n\n\n\n\n\nThis also allows you to access the Figure object later by refering to the variable fig. Thus, even when you want to plot all data on a single plot, it is best to always start by initializing a new Figure.\nTo generate a scatter plot instead of a line, we can use plt.scatter():\n# Generate new x and y with fewer points for legibility\nxscat = np.linspace(-5,5,25)\nyscat = np.sin(xscat)\n\n# Plot sine function as scatter plot\nplt.scatter(xscat,yscat)\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Generate new x and y with fewer points for legibility\nxscat = np.linspace(-5,5,25)\nyscat = np.sin(xscat)\n\n# Plot sine function as scatter plot\nplt.scatter(xscat,yscat)\n\n\n&lt;matplotlib.collections.PathCollection at 0x17a67dbd0&gt;\n\n\n\n\n\nYou can also create a scatter plot using plt.plot() with keyword arguments, which allow you to change things like the color, style, and size of the lines and markers. We will explore some of these keyword arguments in the next section.\n\n\nKeyword arguments\n\n\nIn addition to the required x and y parameters, there are a number of optional keyword arguments that can be passed to the matplotlib plotting functions. Here, we will consider a few of the most useful: color, marker, and linestyle.\n\nColors\nThe first thing you might wish to control is the color of your plot. Matplotlib accepts several different color definitions to the color keyword argument, which is a feature of most plotting functions.\nFirst, colors can be passed as strings according to their HTML/CSS names. For example:\nplt.plot(x, y, 'green')\nIn total, there are 140 colors allowed in HTML; their names are shown below.\n\n\n\ncolors\n\n\nAs you can see in the image above, the basic colors can also be defined by a single-letter shortcut. These are shown in the table below.\n\n\n\n\n\n\n\nLetter code\nColor name\n\n\n\n\n‚Äòr‚Äô\nred\n\n\n‚Äòg‚Äô\ngreen\n\n\n‚Äòb‚Äô\nblue\n\n\n‚Äòc‚Äô\ncyan\n\n\n‚Äòm‚Äô\nmagenta\n\n\n‚Äòy‚Äô\nyellow\n\n\n‚Äòk‚Äô\nblack\n\n\n‚Äòw‚Äô\nwhite\n\n\n\nAnother way of specifying colors is to use an RGB(A) tuple, where the brightness of each channel (R, G, or B, which correspond to red, green, and blue) is given as a float between 0 and 1. An optional fourth value, A or alpha, value can be passed to specify the opacity of the line or marker.\nplt.plot(x, y, color=(0.2,0.7,1.0))\nA grayscale value can be used by passing a number between 0 and 1 as a string. In this representation, '0.0' corresponds to black and '1.0' corresponds to white.\nplt.plot(x, y, color='0.25')\nMy personal favorite way to define colors is to use  color hex codes, which represent colors as hexadecimals ranging from 0 to FF. Color hex codes consist of a hash character # followed by six hex values (e.g.¬†#AFD645). Hex codes must be passed as strings (e.g.¬†'#AFD645') in matplotlib and are perhaps the most flexible way to select colors.\nplt.plot(x, y, color='#C6E2FF')\nIn the cell below, five functions are plotted in different colors, each specified by a different definition.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave with different colors\nplt.plot(x, np.sin(x - 0), color='darkblue')     # HTML name\nplt.plot(x, np.sin(x - 1), color='m')            # Short letter code\nplt.plot(x, np.sin(x - 2), color=(0.0,0.8,0.81)) # RGB tuple\nplt.plot(x, np.sin(x - 3), color='0.65')         # Grayscale between 0 and 1\nplt.plot(x, np.sin(x - 4), color='#B8D62E')      # Hex code\n\n\n\n\n\n\n\nLinestyles\nUsing the linestyle keyword argument, you can change the style of the line plotted using plt.plot(). These can be specified either by their name or a shortcut. A few of the style options (and their matplotlib shortcuts) are shown in the table below. To see a full list of linestyle options, see the docs.\n\n\n\n\n\n\n\nShort code\nLine style\n\n\n\n\n‚Äò-‚Äô\nsolid\n\n\n‚Äò‚Äì‚Äô\ndashed\n\n\n‚Äò:‚Äô\ndotted\n\n\n‚Äò-.‚Äô\ndashdot\n\n\n\nAs we‚Äôve already seen, the default linestyle is solid. The syntax for changing a line‚Äôs style is:\nplt.plot(x, y, linestyle='dashed')\nor, more commonly:\nplt.plot(x, y, linestyle='--')\nLet‚Äôs adjust the style of our waveform plot using the linestyle keyword argument.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave with different colors + linestyles\nplt.plot(x, np.sin(x - 0), color='darkblue', linestyle='-')\nplt.plot(x, np.sin(x - 1), color='m', linestyle='dashed')\nplt.plot(x, np.sin(x - 2), color=(0.0,0.8,0.81), linestyle=':') \nplt.plot(x, np.sin(x - 3), color='0.65', linestyle='solid')\nplt.plot(x, np.sin(x - 4), color='#B8D62E', linestyle='-.')\n\n\n\n\n\n\n\nMarkers\nMarkers can be used in plt.plot() and plt.scatter(). There are several available markers in matplotlib, and you can also define your own. A few of the most useful are shown in the table below.\n\n\n\nMarker code\nSymbol\nDescription\n\n\n\n\n‚Äòo‚Äô\n‚óè\ncircle\n\n\n‚Äò.‚Äô\n‚ãÖ\npoint\n\n\n**‚Äô*‚Äô**\n‚òÖ\nstar\n\n\n‚Äò+‚Äô\n+\nplus\n\n\n‚Äòx‚Äô\n\\times\nx\n\n\n‚Äò^‚Äô\n‚ñ≤\ntriangle\n\n\n‚Äòs‚Äô\n‚óº\nsquare\n\n\n\nNote that unlike color and linestyle, the marker keyword argument only accepts a code to specify the marker style.\nplt.scatter(x, y, marker='+')\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave as scatter plot with different colors + markers\nplt.scatter(xscat, yscat-0, color='darkblue', marker='o')\nplt.scatter(xscat, yscat-1, color='m', marker='.')\nplt.scatter(xscat, yscat-2, color=(0.0,0.8,0.81), marker='+')\nplt.scatter(xscat, yscat-3, color='0.65', marker='*')\nplt.scatter(xscat, yscat-4, color='#B8D62E', marker='s')\n\n\n&lt;matplotlib.collections.PathCollection at 0x17a827fd0&gt;\n\n\n\n\n\nUsing the marker keyword argument with the plt.plot() function creates a connected line plot, where the data points are designated by markers and connected by lines.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave with different colors + markers\nplt.plot(xscat, np.sin(xscat - 0), color='darkblue', marker='o')\nplt.plot(xscat, np.sin(xscat - 1), color='m', marker='.')\nplt.plot(xscat, np.sin(xscat - 2), color=(0.0,0.8,0.81), marker='+')\nplt.plot(xscat, np.sin(xscat - 3), color='0.65', marker='*')\nplt.plot(xscat, np.sin(xscat - 4), color='#B8D62E', marker='s')\n\n\n\n\n\n\n\nExplicit definitions vs.¬†shortcuts\nUp to now, we have used explicit definitions to specify keyword arguments. While this is generally preferable, matplotlib does allow color, linestyle, and marker codes to be combined into a single, non-keyword argument. For example:\n# Plot a dashed red line\nplt.plot(x, y, 'r--')\nSeveral examples are presented in the cell below.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave with different colors + markers\nplt.plot(xscat, yscat-0, 'b-o')    # Solid blue line with circle markers\nplt.plot(xscat, yscat-1, 'm--*')   # Dashed magenta line with star markers\nplt.plot(xscat, yscat-2, 'c+')     # Cyan plus markers\nplt.plot(xscat, yscat-3, 'k')      # Solid black line\nplt.plot(xscat, yscat-4, 'y-s')    # Solid yellow line with square markers\n\n\n\n\n\nAs you can see, the downside of this method is that you are limited to the eight colors that have a single-letter code. To use other colors, you must use explicitly defined keyword arguments.\nIn addition to those we explored in this section, other useful keyword arguments include linewidth and markersize, which do exactly what you‚Äôd expect them to do. For a full list of keyword arguments (you should know what‚Äôs coming by now), see the docs.\n\n\n\nAxes settings\n\n\nNext, we will explore how to scale and annotate a plot using axes routines that control what goes on around the edges of the plot.\n\nLimits, labels, + ticks\nBy default, matplotlib will attempt to determine x- and y-axis limits, which usually work pretty well. Sometimes, however, it is useful to have finer control. The simplest way to adjust the display limits is to use the plt.xlim() and plt.ylim() methods:\n# Set axis limits\nplt.xlim(xmin, xmax)\nplt.ylim(ymin, ymax)\nIn the example below, adjust the numbers (these can be int or float values) to see how the plot changes.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\n\n# Set axis limits\nplt.xlim(-5,5)\nplt.ylim(-2,2)\n\n\n(-2.0, 2.0)\n\n\n\n\n\nYou may also find it useful to adjust the ticks and/or tick labels that matplotlib  displays by default. The plt.xticks() and plt.yticks() methods allow you to control the locations of both the ticks and the labels on the x- and y-axes, respectively. Both methods accept two list or array-like arguments, as well as optional keyword arguments. The first corresponds to the ticks, while the second controls the tick labels.\n# Set x-axis ticks at 0, 0.25, 0.5, 0.75, 1.0 with all labeled\nplt.xticks([0,0.25,0.5,0.75,1.0])\n# Set y-axis ticks from 0 to 100 with ticks on 10s and labels on 20s\nplt.yticks(np.arange(0,101,10),['0','','20','','40','','60','','80','','100'])\nIf the labels are not specified, all ticks will be labeled accordingly. To only label certain ticks, you must pass a list with empty strings in the location of the ticks you wish to leave unlabeled (or the ticks will be labeled in order).\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n\n([&lt;matplotlib.axis.YTick at 0x17aa243a0&gt;,\n  &lt;matplotlib.axis.YTick at 0x17aa0fd00&gt;,\n  &lt;matplotlib.axis.YTick at 0x17a9f0ca0&gt;,\n  &lt;matplotlib.axis.YTick at 0x17a9cf6d0&gt;,\n  &lt;matplotlib.axis.YTick at 0x17a9f0400&gt;],\n [Text(0, -1.0, '‚àí1.0'),\n  Text(0, -0.5, '‚àí0.5'),\n  Text(0, 0.0, '0.0'),\n  Text(0, 0.5, '0.5'),\n  Text(0, 1.0, '1.0')])\n\n\n\n\n\nAs with any plot, it is imperative to include x- and y-axis labels. This can be done by passing strings to the plt.xlabel() and plt.ylabel() methods:\n# Set axis labels\nplt.xlabel('x-axis')\nplt.ylabel('y-axis')\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n# Set axis labels\nplt.xlabel('x-axis')\nplt.ylabel('y-axis')\n\n\nText(0, 0.5, 'y-axis')\n\n\n\n\n\nA nice feature about matplotlib is that it supports TeX formatting for mathematical expressions. This is quite useful for displaying equations, exponents, units, and other mathematical operators. The syntax for TeX expressions is 'r$TeX expression here$'. For example, we can display the axis labels as x and \\sin{(x)} as follows:\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n# Set axis labels\nplt.xlabel(r'$x$')\nplt.ylabel(r'$\\sin{(x)}$')\n\n\nText(0, 0.5, '$\\\\sin{(x)}$')\n\n\n\n\n\n\n\nLegends + titles\nAdding a title to your plot is analogous to labeling the x- and y-axes. The plt.title() method allows you to set the title of your plot by passing a string:\nplt.title('Title')\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\nplt.plot(x, ycos, color='#B8D62E')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n# Set axis labels\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\n\n# Set title\nplt.title('Sinusoidal functions')\n\n\nText(0.5, 1.0, 'Sinusoidal functions')\n\n\n\n\n\nWhen multiple datasets are plotted on the same axes it is often useful to include a legend that labels each line or set of points. Matplotlib has a quick way of displaying a legend using the plt.legend() method. There are multiple ways of specifying the label for each dataset; I prefer to pass a list of strings to plt.legend():\n# Plot data\nplt.plot(x1, y1)\nplt.plot(x2, y2)\n\n# Legend\nplt.legend(labels=['Data1', 'Data2'])\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, color='darkblue')\nplt.plot(x, ycos, color='#B8D62E')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n# Set axis labels\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\n\n# Set title\nplt.title('Sinusoidal functions')\n\n# Legend\nplt.legend(labels=['sin(x)','cos(x)'])\n\n\n&lt;matplotlib.legend.Legend at 0x17a7c0e50&gt;\n\n\n\n\n\nAnother way of setting the data labels is to use the label keyword argument in the plt.plot() (or plt.scatter()) function:\n# Plot data\nplt.plot(x1, y1, label='Data1')\nplt.plot(x2, y2, label='Data2')\n\n# Legend\nplt.legend()\nNote that you must still run plt.legend() to display the legend.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure\nfig1 = plt.figure()\n# Plot sine wave \nplt.plot(x, ysin, label='sin(x)', color='darkblue')\nplt.plot(x, ycos, label='cos(x)', color='#B8D62E')\n\n# Set x-axis limits\nplt.xlim(-5,5)\n\n# Set axis ticks\nplt.xticks([-4,-3,-2,-1,0,1,2,3,4],['-4','','-2','','0','','2','','4'])\nplt.yticks([-1,-0.5,0,0.5,1])\n\n# Set axis labels\nplt.xlabel(r'$x$')\nplt.ylabel(r'$y$')\n\n# Set title\nplt.title('Sinusoidal functions')\n\n# Legend\nplt.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x17a5db520&gt;\n\n\n\n\n\n\n\n\nSubplots + multiple axes\n\n\nNow that we‚Äôve established the basics of plotting in matplotlib, let‚Äôs get a bit more complicated. Oftentimes, you may want to plot data on multiple axes within the same figure. The easiest way to do this in matplotlib is to use the plt.subplot() function, which takes three non-keyword arguments: nrows, ncols, and index. nrows and ncols correspond to the total number of rows and columns of the entire figure, while index refers to the index position of the current axes. Importantly (and annoyingly), the index for subplots starts in the upper left corner at 1 (not 0)!. The image below contains a few examples of how matplotlib arranges subplots.\n\nThe most explicit way of adding subplots is to use the fig.add_subplot() command to initialize new axes as variables:\n# Initialize empty figure\nfig = plt.figure()\n# Add four axes\nax1 = fig.add_subplot(2,2,1)\nax2 = fig.add_subplot(2,2,2)\nax3 = fig.add_subplot(2,2,3)\nax4 = fig.add_subplot(2,2,4)\nThis allows you to access each Axes object later to plot data and adjust the axes parameters.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure\nfig = plt.figure()\n# Add four axes\nax1 = fig.add_subplot(2,2,1)\nax2 = fig.add_subplot(2,2,2)\nax3 = fig.add_subplot(2,2,3)\nax4 = fig.add_subplot(2,2,4)\n\n\n\n\n\nTo plot data, we use ax.plot() or ax.scatter(). These methods are analogous to plt.plot() and plt.scatter() for acting on the Axes, rather than the Figure object.\n# Plot data\nax1.plot(x, y)\nax2.plot(x, y)\nax3.plot(x, y)\nax4.plot(x, y)\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure\nfig = plt.figure()\n# Add four axes\nax1 = fig.add_subplot(2,2,1)\nax2 = fig.add_subplot(2,2,2)\nax3 = fig.add_subplot(2,2,3)\nax4 = fig.add_subplot(2,2,4)\n\n# Plot data\n# Plot sine wave with different colors on different axes\nax1.plot(x, np.sin(x - 0), color='darkblue')\nax2.plot(x, np.sin(x - 1), color='m')\nax3.plot(x, np.sin(x - 2), color=(0.0,0.8,0.81))\nax4.plot(x, np.sin(x - 4), color='#B8D62E')\n\n\n\n\n\n\nFigure vs.¬†Axes methods\nPerhaps the trickiest part about subplots ‚Äì and Axes methods in general ‚Äì¬†is adjusting the axes settings. While most Figure functions translate directly Axes methods (e.g.¬†plt.plot() \\rightarrow ax.plot(), plt.legend() \\rightarrow ax.legend()), commands to set limits, ticks, labels, and titles are slightly modified. Some important Figure methods and their Axes counterparts are shown in the table below.\n\n\n\n\n\n\n\nFigure command\nAxes command\n\n\n\n\nplt.xlabel()\nax.set_xlabel()\n\n\nplt.ylabel()\nax.set_ylabel()\n\n\nplt.xlim()\nax.set_xlim()\n\n\nplt.ylim()\nax.set_ylim()\n\n\nplt.xticks()\nax.set_xticks()\n\n\nplt.yticks()\nax.set_yticks()\n\n\n\nThese are different primarily because the Figure functions are inherited from MATLAB, while the Axes functions are object-oriented. Generally, the arguments are similar ‚Äì if not identical ‚Äì¬†between the two.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure\nfig = plt.figure()\n# Add four axes\nax1 = fig.add_subplot(2,2,1)\nax2 = fig.add_subplot(2,2,2)\nax3 = fig.add_subplot(2,2,3)\nax4 = fig.add_subplot(2,2,4)\n\n# Plot data\n# Plot sine wave with different colors on different axes\nax1.plot(x, np.sin(x - 0), color='darkblue')\nax2.plot(x, np.sin(x - 1), color='m')\nax3.plot(x, np.sin(x - 2), color=(0.0,0.8,0.81))\nax4.plot(x, np.sin(x - 4), color='#B8D62E')\n\n# Set axes limits, labels, + ticks\nfor i,ax in enumerate([ax1,ax2,ax3,ax4]):\n    # Set x limits \n    ax.set_xlim(-5,5)\n    # Set title\n    ax.set_title(r'$\\sin{(x - %d)}$' % i)\n    # Only label x ticks and x-axis on bottom row\n    if i &lt; 2:\n        ax.set_xticklabels([])\n    else:\n        ax.set_xlabel('x')\n    # Only label y ticks and y-axis on left column\n    if i == 0 or i == 2:\n        ax.set_ylabel('y')\n    else:\n        ax.set_yticklabels([])\n\nplt.tight_layout()\n\n\n\n\n\nIn the last example, we included a command, plt.tight_layout(), which automatically formats the figure to fit the window. This is most useful when using an IDE with a separate plotting window, rather than with in-line plots like those in a notebook. To get a sense of what plt.tight_layout() does, try re-running the above cell with this command commented out.\nTo go beyond regularly gridded subplots and create subplots that span multiple rows and/or columns, check out GridSpec.\n\nüìö  &lt;b&gt; Practice 1. &lt;/b&gt; \nRecreate the plot below. You do not need to match the colors exactly, but do not rely on &lt;span class=\"codeb\"&gt;matplotlib&lt;/span&gt; defaults. Note: do not worry about the equation(s); these are included to indicate which functions to plot.\n\n\n\nüìö  &lt;b&gt; Practice 2. &lt;/b&gt; \nRecreate the plot below. You do not need to match the colors exactly, but do not rely on &lt;span class=\"codeb\"&gt;matplotlib&lt;/span&gt; defaults. Note: do not worry about the equation(s); these are included to indicate which functions to plot.\n\n\n\n\n\nWorking with real data\n\n\nAs we learned in the previous exercise, working with real-world data usually complicates things, and plotting is no exception. In particular, working with time series can get a bit messy. Let‚Äôs take a look at our BSRN data as an example.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Import data\nbsrn = pd.read_csv('../data/BSRN_GOB_2019-10.csv',index_col=0,parse_dates=True)\n\n\nNow that we‚Äôve imported our data, let‚Äôs make a quick plot of incoming shortwave radiation over time.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure\nfig = plt.figure()\n# Plot incoming SW radiation\nplt.plot(bsrn.index,bsrn.SWD_Wm2)\n# Label y-axis\nplt.ylabel(r'Incoming SW radiation (W m$^{-2}$)')\n\n\nText(0, 0.5, 'Incoming SW radiation (W m$^{-2}$)')\n\n\n\n\n\nThe x-axis looks rather messy because the tick labels are timestamps, which are, by nature, very long. Luckily, matplotlib has a module called dates for dealing with datetime objects.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nimport matplotlib.dates as mdates\n\n\nWithout going into too much detail, we can use some of the more advanced Axes settings to format and rotate the tick labels such that they no longer overlap, and we can use matplotlib.dates to format the timestamps. In short, we will use the mdates.DateFormatter() function to format the timestamps according to C formatting codes.\nThe following example demonstrates this, and includes a good code chunk for formatting timestamps to add to your repertoire. It is important to note that the formatting methods employed here are Axes methods, which means that we must operate on an Axes object, rather than the Figure.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure and axes\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\n# Plot incoming SW radiation\nax.plot(bsrn.index,bsrn.SWD_Wm2)\n# Label y-axis\nax.set_ylabel(r'Incoming SW radiation (W m$^{-2}$)')\n\n# Format timestamps\nax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%b-%y'))\n# Format and rotate ticks\nplt.setp(ax.get_xticklabels(), rotation=45, fontsize=10, ha='right')\nax.get_xticklabels()\n\n\n[Text(18170.0, 0, '01-Oct-19'),\n Text(18174.0, 0, '05-Oct-19'),\n Text(18178.0, 0, '09-Oct-19'),\n Text(18182.0, 0, '13-Oct-19'),\n Text(18186.0, 0, '17-Oct-19'),\n Text(18190.0, 0, '21-Oct-19'),\n Text(18194.0, 0, '25-Oct-19'),\n Text(18198.0, 0, '29-Oct-19'),\n Text(18201.0, 0, '01-Nov-19')]\n\n\n\n\n\n\nüìö  &lt;b&gt; Practice 3. &lt;/b&gt; \nPlot temperature and relative humidity (ideally using subplots) over the month of October 2019 at the BSRN station. Be sure to format the timestamps and include axis labels, a title, and a legend, if necessary.\n\n\nüìö  &lt;b&gt; Practice 4. &lt;/b&gt; \nSaturation vapor pressure, $e^*(T_a)$, is the maximum pressure of water vapor that can exist in equilibrium above a flat plane of water at a given temperature. It can be calculated from the Tetens equation:\n$$e^{*}(T_{a}) \\, = \\, a \\, e^{\\frac{b\\, \\cdot \\, T_{_a}}{T_{_a}\\, + \\, c}} $$\nwhere T_a is the air temperature in ¬∞C, a = 0.611 \\, \\text{kPa}, b = 17.502, and c = 240.97^{\\circ} \\text{C}. \n\n\nCalculate e^*(T_a) in kPa for all temperatures in bsrn.\n\n\nPlot temperature vs.¬†saturation vapor pressure for the BSRN station. Be sure to format your plot appropriately and include axis labels, a title, and a legend, if necessary.\n\n\nCompare your plot to Figure 3-1 in Campbell and Norman (1998). Do they look more or less the same?\n\n\n\n\nüìö  &lt;b&gt; Practice 5. &lt;/b&gt; \nThe difference between saturation vapor pressure and ambient air pressure is called vapor pressure deficit, $\\textit{VPD}$. $\\textit{VPD}$ can be calculated from saturation vapor pressure and relative humidity, $h_r$, as follows:\n$$ \\textit{VPD} \\, = \\, e^*(T_a) \\cdot (1 \\, - \\, h_r)$$\nwhere h_r is expressed as a fraction.\n\n\nCalculate the vapor pressure deficit for the BSRN data.\n\n\nCalculate the mean hourly e^*(T_a) and \\textit{VPD} over the entire month.\n\n\nPlot e^*(T_a) and \\textit{VPD} as a function of time of day. (Bonus: if you want to get fancy, plot both variables on one plot using  ax.twinx().)\n\n\nHow does relative humidity vary throughout the day? Why?\n\n\nCompare your plot(s) to Figure 3-3 in Campbell and Norman (1998). How do the values of e^*(T_a) and \\textit{VPD} for the BSRN station compare to those at constant vapor pressure in Figure 3-3?"
  },
  {
    "objectID": "interactive_sessions/interactive_sessions.html",
    "href": "interactive_sessions/interactive_sessions.html",
    "title": "EDS 217: Python for Environmental Data Science",
    "section": "",
    "text": "Return to Course Home Page\n\n\n\n\n\n\nCode\n# Interactive Sessions"
  },
  {
    "objectID": "interactive_sessions/99_seaborn.html",
    "href": "interactive_sessions/99_seaborn.html",
    "title": "Seaborn üìà",
    "section": "",
    "text": "Return to Course Home Page\n\n\n\n\n\n\n\nseaborn\n\n\nThis session provides a brief introduction to the Seaborn visualization library.\nSeaborn helps you explore and understand your data. Its plotting functions operate on dataframes and arrays containing whole datasets and internally perform the necessary semantic mapping and statistical aggregation to produce informative plots. Its dataset-oriented, declarative API lets you focus on what the different elements of your plots mean, rather than on the details of how to draw them.\nHere‚Äôs an example of seaborne‚Äôs capabilities.\n\n\nCode\n%matplotlib inline\n# Import seaborn\nimport seaborn as sns\n\n# Apply the default theme\nsns.set_theme()\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"sex\", style=\"smoker\", size=\"size\",\n)\n\ntips.head()\n\n\nBehind the scenes, seaborn uses matplotlib to draw its plots. The plot above shows the relationship between five variables in the built-in tips dataset using a single call to the seaborn function relplot().\nNotice that you only need to provide the names of the variables and their roles in the plot.\nThis interface is different from matplotlib, in that you do not need to specify attributes of the plot elements in terms of the color values or marker codes.\nBehind the scenes, seaborn handled the translation from values in the dataframe to arguments that matplotlib understands. This declarative approach lets you stay focused on the questions that you want to answer, rather than on the details of how to control matplotlib.\n\nSeaborn relplot()\nThe function relplot() is named that way because it is designed to visualize many different statistical relationships. While scatter plots are often effective, relationships where one variable represents a measure of time are better represented by a line. The relplot() function has a convenient kind parameter that lets you easily switch to this alternate representation:\n\n\nCode\ndots = sns.load_dataset(\"dots\")\nsns.relplot(\n    data=dots, kind=\"line\",\n    x=\"time\", y=\"firing_rate\", col=\"align\",\n    hue=\"choice\", size=\"coherence\", style=\"choice\",\n    facet_kws=dict(sharex=False),\n)\n\n\nIf you compare the two calls to relplot() in the two examples so far, you will see that the size and style parameters are used in both the scatter plots (first example) and line plots (second example). However, they affect the two visualizations differently.\nIn a scatter plot, the size and style arguments affect marker area and symbol representation.\nIn a line plot, the size and style arguments alter the line width and dashing.\nAllowing the same arguments (syntax) to change meaning (semantics) across different contexts is more characteristic of natural languages than formal ones. In this case, seaborn is attempting to allow you to write in a ‚Äúgrammar of graphics‚Äù, which is the same concept underlying ggplot created by Hadley Wickham.\nThe benefit of adopting this less formal specification is that you do not need to worry about as many syntax details and instead can focus more on the overall structure of the plot and the information you want it to convey.\n\n\nComparing matplotlib to seaborn\nA focus of today‚Äôs activities is translation, so let‚Äôs look at translating some of the examples from yesterday‚Äôs matplotlib exercise into seaborn.\nFirst, as always, let‚Äôs import our important packages:\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\n\nBasic line plots (sns.lineplot)\nYesterday we used a few functions, y_{\\sin} = \\sin{(x)} and y_{\\cos} = \\cos{(x)}:\n# Generate a 1D array with 300 points between -5 and 5\nx = np.linspace(-5,5,300)\n# Generate sine wave\nysin = np.sin(x)\n# Generate cosine wave\nycos = np.cos(x)\n\n# Now let's make a dataframe from these arrays:\ndf = pd.DataFrame({\n    'x': x,\n    'ysin': ysin,\n    'ycos': ycos\n    })\nWe can plot these on the same figure without instancing plt.figure() as follows:\n# Plot sine wave\nplt.plot(x,ysin)\n# Plot cosine wave\nplt.plot(x,ycos)\nSeaborn uses the lineplot command to plot line plots:\nsns.lineplot(data=df,x='x',y='ysin')\nsns.lineplot(data=df,x='x',y='ycos')\n\n\nCode\n# Generate a 1D array with 300 points between -5 and 5\nx = np.linspace(-5,5,300)\n# Generate sine wave\nysin = np.sin(x)\n# Generate cosine wave\nycos = np.cos(x)\n\n# Now let's make a dataframe from these arrays:\ndf = pd.DataFrame({\n    'x': x,\n    'ysin': ysin,\n    'ycos': ycos\n    })\n\nsns.lineplot(data=df,x='x',y='ysin')\nsns.lineplot(data=df,x='x',y='ycos')\nplt.ylabel('Add a title')\nplt.savefig('output.png')\n\n\nüìö Practice 2. 1. Load the flights dataset using the sns.load_dataset(\"flights\") command. 1. Explore the dataframe (it contains passenger data by month and year). 1. Use sns.lineplot() to plot the number of passengers throughout the data set. 1. Create a new dataset that contains data on a specific month (your choice) to see how monthly passengers have changed over time. 1. Create a plot that shows the average and range of passengers by year. (Hint: This is much easier than it sounds in seaborn!)\n\n\nCode\nflights = sns.load_dataset(\"flights\")\n\n\n\n\n\n\n\nWorking with real data\n\n\nAs we learned in the previous exercise, working with real-world data usually complicates things, and plotting is no exception. In particular, working with time series can get a bit messy. Let‚Äôs take a look at our BSRN data as an example.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Import data\nbsrn = pd.read_csv('../data/BSRN_GOB_2019-10.csv',index_col=0,parse_dates=True)\n\nbsrn.head()\n\n# Some species data \nmack_verts = pd.read_csv(\"../data/AS00601.csv\")\n\n\nNow that we‚Äôve imported our data, let‚Äôs make a quick plot of incoming shortwave radiation over time.\n\n‚ñ∂Ô∏è &lt;b&gt; Translate the cell below into seaborn. &lt;/b&gt;\n\n\n\nCode\n# # Initialize empty figure\n# fig = plt.figure()\n# # Plot incoming SW radiation\n# plt.plot(bsrn.index,bsrn.SWD_Wm2)\n# # Label y-axis\n# plt.ylabel(r'Incoming SW radiation (W m$^{-2}$)')\n\n\n\n\nThe x-axis looks rather messy because the tick labels are timestamps, which are, by nature, very long. Luckily, matplotlib has a module called dates for dealing with datetime objects.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\nimport matplotlib.dates as mdates\n\n\nWithout going into too much detail, we can use some of the more advanced Axes settings to format and rotate the tick labels such that they no longer overlap, and we can use matplotlib.dates to format the timestamps. In short, we will use the mdates.DateFormatter() function to format the timestamps according to C formatting codes.\nThe following example demonstrates this, and includes a good code chunk for formatting timestamps to add to your repertoire. It is important to note that the formatting methods employed here are Axes methods, which means that we must operate on an Axes object, rather than the Figure.\n\n‚ñ∂Ô∏è &lt;b&gt; Run the cell below. &lt;/b&gt;\n\n\n\nCode\n# Initialize empty figure and axes\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\n# Plot incoming SW radiation\nax.plot(bsrn.index,bsrn.SWD_Wm2)\n# Label y-axis\nax.set_ylabel(r'Incoming SW radiation (W m$^{-2}$)')\n\n# Format timestamps\nax.xaxis.set_major_formatter(mdates.DateFormatter('%d-%b-%y'))\n# Format and rotate ticks\nplt.setp(ax.get_xticklabels(), rotation=45, fontsize=10, ha='right')\nax.get_xticklabels()\n\n\n\nüìö  &lt;b&gt; Practice 3. &lt;/b&gt; \nPlot temperature and relative humidity (ideally using subplots) over the month of October 2019 at the BSRN station. Be sure to format the timestamps and include axis labels, a title, and a legend, if necessary."
  },
  {
    "objectID": "group_project/python_data_science_show_and_tell.html#python-library-show-and-tell",
    "href": "group_project/python_data_science_show_and_tell.html#python-library-show-and-tell",
    "title": "üì£ Data Science Library Show-and-Tell",
    "section": "Python Library Show-and-Tell",
    "text": "Python Library Show-and-Tell\n\nForm groups of 4-5 Students\nFind a Library\nWorking collaboratively use a github repo to create a notebook about the library\nPresent notebook to class (10-15 min)\n\n\nFind a Library\n\nPyPi pypi.org\nTowards Data Science towardsdatascience.com\nPodcasts: Talk Python to Me, Linear Digressions, Python Bytes\nDomain-specific searches ‚Äúpython oceanography‚Äù, ‚Äúpython climate science‚Äù, etc‚Ä¶\n\nGood candidates for a Show-and-Tell are libraries that are relevant and popular.\nPopularity can measured with GitHub stars, or PyPi downloads. Or if folks in the data science community are talking about it.\n\n\nCreate a github repo for your tutorial.\nThe repo should contain:\n\nan environment file including all necessary dependencies\na README.md file with a brief description of the library and a link to the library‚Äôs webpage or github repo.\nan example notebook (or notebooks) about the library.\nany other files necessary to run the example notebook(s)\n\n\n\nNotebook contents:\n\nBrief Description\n\nWhat is it?\nWhere is it? Link to project webpage, github\nWho developed?\nWhy was it created?\n\nPotential Use in Environmental Data Science\nQuick tutorial/example, using Env. Data Sci example if possible!\n\nOkay to incorporate examples found elsewhere (provide link and acknowledgement!)\n\n\n\n\nLibrary Show-and-Tell\n\nPresent your notebook to the class!\n10-15 minute presentation.\nQ&A.\nWe will have our presentations in the afternoon on the last day of class (Friday, Sept.¬†15)\nAll repos will be cloned to the environmentaldatascience github so everyone can learn from each other.\n\n\nBY LUNCH ON WEDNESDAY: SELECT YOUR LIBRARY, PLUS TWO ALTERNATES (IN CASE OF DUPLICATES)"
  }
]